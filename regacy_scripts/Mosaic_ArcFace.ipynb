{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b210ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9724931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bef26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ea4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a08571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/ielab/dataset/ms1m_dataset/train_mosaic/'\n",
    "test_path = '/home/ielab/dataset/ms1m_dataset/test_masked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4148c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = os.listdir(train_path)\n",
    "\n",
    "lb = {string : i for i,string in enumerate(col_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7903cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1977',\n",
       " '2782',\n",
       " '1236',\n",
       " '2546',\n",
       " '1867',\n",
       " '1647',\n",
       " '1458',\n",
       " '1686',\n",
       " '2647',\n",
       " '1504',\n",
       " '1148',\n",
       " '2373',\n",
       " '2246',\n",
       " '3262',\n",
       " '407',\n",
       " '1569',\n",
       " '226',\n",
       " '1663',\n",
       " '3590',\n",
       " '172',\n",
       " '1991',\n",
       " '192',\n",
       " '3183',\n",
       " '183',\n",
       " '1170',\n",
       " '1191',\n",
       " '256',\n",
       " '1618',\n",
       " '1731',\n",
       " '2076',\n",
       " '203',\n",
       " '3264',\n",
       " '1791',\n",
       " '2709',\n",
       " '2362',\n",
       " '2053',\n",
       " '2657',\n",
       " '3002',\n",
       " '1525',\n",
       " '2954',\n",
       " '2820',\n",
       " '2903',\n",
       " '3372',\n",
       " '2719',\n",
       " '1141',\n",
       " '2222',\n",
       " '2730',\n",
       " '265',\n",
       " '348',\n",
       " '1944',\n",
       " '3036',\n",
       " '1443',\n",
       " '1039',\n",
       " '3522',\n",
       " '1414',\n",
       " '1425',\n",
       " '1473',\n",
       " '1697',\n",
       " '2724',\n",
       " '1902',\n",
       " '1355',\n",
       " '2385',\n",
       " '466',\n",
       " '2276',\n",
       " '1015',\n",
       " '3338',\n",
       " '1481',\n",
       " '1433',\n",
       " '2144',\n",
       " '3578',\n",
       " '2529',\n",
       " '1486',\n",
       " '2157',\n",
       " '3501',\n",
       " '1259',\n",
       " '3095',\n",
       " '2240',\n",
       " '2883',\n",
       " '1915',\n",
       " '1004',\n",
       " '1107',\n",
       " '472',\n",
       " '2467',\n",
       " '1490',\n",
       " '2391',\n",
       " '148',\n",
       " '3345',\n",
       " '2387',\n",
       " '413',\n",
       " '2111',\n",
       " '2326',\n",
       " '2694',\n",
       " '2840',\n",
       " '1285',\n",
       " '2279',\n",
       " '1658',\n",
       " '3350',\n",
       " '3524',\n",
       " '1442',\n",
       " '1570',\n",
       " '2428',\n",
       " '2079',\n",
       " '2088',\n",
       " '1153',\n",
       " '2571',\n",
       " '3541',\n",
       " '1718',\n",
       " '2159',\n",
       " '2539',\n",
       " '2744',\n",
       " '2244',\n",
       " '1319',\n",
       " '3364',\n",
       " '3550',\n",
       " '2907',\n",
       " '1773',\n",
       " '1333',\n",
       " '112',\n",
       " '3407',\n",
       " '121',\n",
       " '272',\n",
       " '2045',\n",
       " '188',\n",
       " '2139',\n",
       " '171',\n",
       " '1403',\n",
       " '1101',\n",
       " '1277',\n",
       " '2656',\n",
       " '3567',\n",
       " '2106',\n",
       " '3294',\n",
       " '251',\n",
       " '1696',\n",
       " '2801',\n",
       " '1082',\n",
       " '2348',\n",
       " '1667',\n",
       " '229',\n",
       " '3486',\n",
       " '2143',\n",
       " '17',\n",
       " '1353',\n",
       " '1224',\n",
       " '2566',\n",
       " '454',\n",
       " '2655',\n",
       " '352',\n",
       " '1740',\n",
       " '2772',\n",
       " '3007',\n",
       " '1322',\n",
       " '3552',\n",
       " '1517',\n",
       " '1691',\n",
       " '279',\n",
       " '1633',\n",
       " '1776',\n",
       " '2218',\n",
       " '222',\n",
       " '1387',\n",
       " '1485',\n",
       " '3181',\n",
       " '3429',\n",
       " '2108',\n",
       " '1172',\n",
       " '1331',\n",
       " '1061',\n",
       " '206',\n",
       " '1713',\n",
       " '3343',\n",
       " '2004',\n",
       " '1519',\n",
       " '2522',\n",
       " '2948',\n",
       " '3109',\n",
       " '1662',\n",
       " '2006',\n",
       " '2417',\n",
       " '3101',\n",
       " '1565',\n",
       " '2177',\n",
       " '1019',\n",
       " '3049',\n",
       " '1755',\n",
       " '2845',\n",
       " '2650',\n",
       " '2731',\n",
       " '2747',\n",
       " '1802',\n",
       " '2551',\n",
       " '2749',\n",
       " '146',\n",
       " '1023',\n",
       " '264',\n",
       " '38',\n",
       " '2226',\n",
       " '1450',\n",
       " '1900',\n",
       " '2015',\n",
       " '27',\n",
       " '343',\n",
       " '1245',\n",
       " '1297',\n",
       " '1936',\n",
       " '299',\n",
       " '1942',\n",
       " '3158',\n",
       " '2171',\n",
       " '2860',\n",
       " '2190',\n",
       " '1782',\n",
       " '1901',\n",
       " '2921',\n",
       " '2750',\n",
       " '1027',\n",
       " '1806',\n",
       " '3091',\n",
       " '102',\n",
       " '3594',\n",
       " '3057',\n",
       " '2942',\n",
       " '3342',\n",
       " '1842',\n",
       " '3066',\n",
       " '3276',\n",
       " '2054',\n",
       " '335',\n",
       " '2602',\n",
       " '2034',\n",
       " '1121',\n",
       " '1117',\n",
       " '1218',\n",
       " '1578',\n",
       " '351',\n",
       " '2158',\n",
       " '2297',\n",
       " '2720',\n",
       " '2314',\n",
       " '2613',\n",
       " '3451',\n",
       " '2212',\n",
       " '1631',\n",
       " '2920',\n",
       " '1871',\n",
       " '2927',\n",
       " '1666',\n",
       " '3481',\n",
       " '3128',\n",
       " '3375',\n",
       " '1890',\n",
       " '3517',\n",
       " '140',\n",
       " '3558',\n",
       " '2738',\n",
       " '1612',\n",
       " '3527',\n",
       " '3255',\n",
       " '1952',\n",
       " '1737',\n",
       " '2619',\n",
       " '1855',\n",
       " '2824',\n",
       " '1924',\n",
       " '3497',\n",
       " '2209',\n",
       " '1254',\n",
       " '1492',\n",
       " '2187',\n",
       " '2147',\n",
       " '1126',\n",
       " '30',\n",
       " '1732',\n",
       " '2836',\n",
       " '3282',\n",
       " '2061',\n",
       " '1051',\n",
       " '1309',\n",
       " '3256',\n",
       " '1135',\n",
       " '2926',\n",
       " '3061',\n",
       " '1550',\n",
       " '1394',\n",
       " '2432',\n",
       " '1335',\n",
       " '2114',\n",
       " '2188',\n",
       " '2736',\n",
       " '2600',\n",
       " '1876',\n",
       " '1427',\n",
       " '2507',\n",
       " '3409',\n",
       " '2623',\n",
       " '3260',\n",
       " '2197',\n",
       " '48',\n",
       " '1847',\n",
       " '3365',\n",
       " '3521',\n",
       " '1833',\n",
       " '1212',\n",
       " '2880',\n",
       " '2375',\n",
       " '1883',\n",
       " '288',\n",
       " '2189',\n",
       " '1190',\n",
       " '1342',\n",
       " '304',\n",
       " '1242',\n",
       " '2024',\n",
       " '3004',\n",
       " '2032',\n",
       " '2050',\n",
       " '1874',\n",
       " '1553',\n",
       " '161',\n",
       " '3452',\n",
       " '1469',\n",
       " '1194',\n",
       " '2257',\n",
       " '1854',\n",
       " '3357',\n",
       " '1653',\n",
       " '1798',\n",
       " '2634',\n",
       " '2543',\n",
       " '2669',\n",
       " '2561',\n",
       " '3116',\n",
       " '1704',\n",
       " '2173',\n",
       " '2858',\n",
       " '1088',\n",
       " '2991',\n",
       " '1683',\n",
       " '3026',\n",
       " '3192',\n",
       " '3013',\n",
       " '2664',\n",
       " '33',\n",
       " '159',\n",
       " '3141',\n",
       " '3277',\n",
       " '2356',\n",
       " '1513',\n",
       " '3392',\n",
       " '2347',\n",
       " '3490',\n",
       " '3402',\n",
       " '1888',\n",
       " '1783',\n",
       " '1202',\n",
       " '2353',\n",
       " '1270',\n",
       " '241',\n",
       " '2848',\n",
       " '42',\n",
       " '270',\n",
       " '3431',\n",
       " '1307',\n",
       " '457',\n",
       " '310',\n",
       " '2446',\n",
       " '2063',\n",
       " '2291',\n",
       " '2967',\n",
       " '3198',\n",
       " '1086',\n",
       " '249',\n",
       " '487',\n",
       " '3153',\n",
       " '1047',\n",
       " '1334',\n",
       " '2122',\n",
       " '14',\n",
       " '2141',\n",
       " '2396',\n",
       " '3414',\n",
       " '1008',\n",
       " '1652',\n",
       " '3416',\n",
       " '1145',\n",
       " '1675',\n",
       " '291',\n",
       " '1775',\n",
       " '1324',\n",
       " '2398',\n",
       " '132',\n",
       " '2864',\n",
       " '1934',\n",
       " '3306',\n",
       " '1318',\n",
       " '2294',\n",
       " '2340',\n",
       " '3332',\n",
       " '2030',\n",
       " '152',\n",
       " '2628',\n",
       " '3540',\n",
       " '1698',\n",
       " '2900',\n",
       " '2586',\n",
       " '114',\n",
       " '1471',\n",
       " '1484',\n",
       " '373',\n",
       " '1456',\n",
       " '2180',\n",
       " '463',\n",
       " '3437',\n",
       " '2945',\n",
       " '1286',\n",
       " '1420',\n",
       " '3075',\n",
       " '2409',\n",
       " '1932',\n",
       " '1444',\n",
       " '1939',\n",
       " '1745',\n",
       " '1743',\n",
       " '2378',\n",
       " '2828',\n",
       " '2110',\n",
       " '2545',\n",
       " '2721',\n",
       " '139',\n",
       " '1695',\n",
       " '1476',\n",
       " '1724',\n",
       " '399',\n",
       " '374',\n",
       " '2581',\n",
       " '3010',\n",
       " '1830',\n",
       " '3275',\n",
       " '3156',\n",
       " '2072',\n",
       " '1156',\n",
       " '1817',\n",
       " '1734',\n",
       " '1099',\n",
       " '3460',\n",
       " '1009',\n",
       " '3510',\n",
       " '1114',\n",
       " '1719',\n",
       " '1796',\n",
       " '3063',\n",
       " '3143',\n",
       " '293',\n",
       " '484',\n",
       " '338',\n",
       " '2041',\n",
       " '1188',\n",
       " '2210',\n",
       " '1729',\n",
       " '3371',\n",
       " '2357',\n",
       " '2642',\n",
       " '2127',\n",
       " '3538',\n",
       " '2960',\n",
       " '2254',\n",
       " '2327',\n",
       " '3381',\n",
       " '427',\n",
       " '3467',\n",
       " '3241',\n",
       " '1628',\n",
       " '2207',\n",
       " '401',\n",
       " '3528',\n",
       " '2463',\n",
       " '3014',\n",
       " '1749',\n",
       " '1822',\n",
       " '2697',\n",
       " '2018',\n",
       " '2913',\n",
       " '2973',\n",
       " '2413',\n",
       " '1753',\n",
       " '2264',\n",
       " '1044',\n",
       " '1584',\n",
       " '2293',\n",
       " '1777',\n",
       " '3395',\n",
       " '1247',\n",
       " '2531',\n",
       " '3423',\n",
       " '1657',\n",
       " '247',\n",
       " '3575',\n",
       " '200',\n",
       " '3568',\n",
       " '1410',\n",
       " '3113',\n",
       " '3505',\n",
       " '2205',\n",
       " '2646',\n",
       " '1778',\n",
       " '2275',\n",
       " '3503',\n",
       " '1020',\n",
       " '3577',\n",
       " '2443',\n",
       " '465',\n",
       " '2372',\n",
       " '136',\n",
       " '167',\n",
       " '2448',\n",
       " '2194',\n",
       " '1163',\n",
       " '1349',\n",
       " '362',\n",
       " '1603',\n",
       " '1841',\n",
       " '2771',\n",
       " '3464',\n",
       " '1621',\n",
       " '3178',\n",
       " '3461',\n",
       " '1408',\n",
       " '3100',\n",
       " '1232',\n",
       " '2511',\n",
       " '1308',\n",
       " '2155',\n",
       " '3440',\n",
       " '3386',\n",
       " '1025',\n",
       " '3515',\n",
       " '2390',\n",
       " '1221',\n",
       " '2027',\n",
       " '2242',\n",
       " '3339',\n",
       " '398',\n",
       " '2306',\n",
       " '461',\n",
       " '1261',\n",
       " '375',\n",
       " '2596',\n",
       " '2492',\n",
       " '2007',\n",
       " '2313',\n",
       " '3270',\n",
       " '2249',\n",
       " '3278',\n",
       " '1041',\n",
       " '1500',\n",
       " '2295',\n",
       " '3518',\n",
       " '3133',\n",
       " '2454',\n",
       " '3265',\n",
       " '1445',\n",
       " '2224',\n",
       " '1970',\n",
       " '426',\n",
       " '467',\n",
       " '1368',\n",
       " '1937',\n",
       " '419',\n",
       " '2524',\n",
       " '2399',\n",
       " '1870',\n",
       " '3436',\n",
       " '1146',\n",
       " '388',\n",
       " '1042',\n",
       " '2608',\n",
       " '1098',\n",
       " '3219',\n",
       " '1611',\n",
       " '1311',\n",
       " '2852',\n",
       " '1877',\n",
       " '2648',\n",
       " '412',\n",
       " '1850',\n",
       " '1155',\n",
       " '1797',\n",
       " '3190',\n",
       " '2838',\n",
       " '1371',\n",
       " '2521',\n",
       " '1812',\n",
       " '2485',\n",
       " '2331',\n",
       " '2253',\n",
       " '3126',\n",
       " '2068',\n",
       " '1105',\n",
       " '322',\n",
       " '282',\n",
       " '2424',\n",
       " '3573',\n",
       " '2874',\n",
       " '3068',\n",
       " '3508',\n",
       " '1979',\n",
       " '1945',\n",
       " '424',\n",
       " '3096',\n",
       " '3235',\n",
       " '2423',\n",
       " '1196',\n",
       " '3076',\n",
       " '1639',\n",
       " '2762',\n",
       " '475',\n",
       " '1195',\n",
       " '2956',\n",
       " '3382',\n",
       " '186',\n",
       " '1586',\n",
       " '1175',\n",
       " '447',\n",
       " '1511',\n",
       " '324',\n",
       " '2859',\n",
       " '3152',\n",
       " '1917',\n",
       " '1493',\n",
       " '3245',\n",
       " '2270',\n",
       " '3543',\n",
       " '488',\n",
       " '3356',\n",
       " '2073',\n",
       " '3018',\n",
       " '1220',\n",
       " '2784',\n",
       " '1894',\n",
       " '1118',\n",
       " '1809',\n",
       " '3466',\n",
       " '3195',\n",
       " '3085',\n",
       " '3445',\n",
       " '1710',\n",
       " '2690',\n",
       " '1237',\n",
       " '157',\n",
       " '234',\n",
       " '1068',\n",
       " '201',\n",
       " '3355',\n",
       " '2542',\n",
       " '2156',\n",
       " '1637',\n",
       " '280',\n",
       " '1167',\n",
       " '3433',\n",
       " '1885',\n",
       " '2220',\n",
       " '3379',\n",
       " '2096',\n",
       " '2761',\n",
       " '1316',\n",
       " '2316',\n",
       " '1625',\n",
       " '3556',\n",
       " '1198',\n",
       " '471',\n",
       " '1060',\n",
       " '1954',\n",
       " '147',\n",
       " '29',\n",
       " '3058',\n",
       " '2058',\n",
       " '1112',\n",
       " '1363',\n",
       " '106',\n",
       " '2788',\n",
       " '1507',\n",
       " '2764',\n",
       " '2365',\n",
       " '2368',\n",
       " '422',\n",
       " '5',\n",
       " '2981',\n",
       " '1540',\n",
       " '1590',\n",
       " '300',\n",
       " '2002',\n",
       " '2822',\n",
       " '15',\n",
       " '276',\n",
       " '1472',\n",
       " '2483',\n",
       " '3123',\n",
       " '277',\n",
       " '2518',\n",
       " '1680',\n",
       " '2559',\n",
       " '1385',\n",
       " '2280',\n",
       " '2816',\n",
       " '19',\n",
       " '3363',\n",
       " '1572',\n",
       " '1248',\n",
       " '2704',\n",
       " '2812',\n",
       " '3498',\n",
       " '2089',\n",
       " '2493',\n",
       " '2292',\n",
       " '126',\n",
       " '2059',\n",
       " '1610',\n",
       " '3191',\n",
       " '41',\n",
       " '3246',\n",
       " '2425',\n",
       " '1110',\n",
       " '2376',\n",
       " '1238',\n",
       " '483',\n",
       " '1305',\n",
       " '298',\n",
       " '3584',\n",
       " '3525',\n",
       " '208',\n",
       " '2773',\n",
       " '1892',\n",
       " '3348',\n",
       " '2896',\n",
       " '3427',\n",
       " '2951',\n",
       " '11',\n",
       " '1341',\n",
       " '2843',\n",
       " '2049',\n",
       " '1654',\n",
       " '1836',\n",
       " '2161',\n",
       " '2632',\n",
       " '1256',\n",
       " '371',\n",
       " '2367',\n",
       " '2556',\n",
       " '2885',\n",
       " '2043',\n",
       " '1528',\n",
       " '1482',\n",
       " '329',\n",
       " '2841',\n",
       " '1116',\n",
       " '2671',\n",
       " '1974',\n",
       " '2733',\n",
       " '2401',\n",
       " '2434',\n",
       " '1765',\n",
       " '2093',\n",
       " '325',\n",
       " '2178',\n",
       " '237',\n",
       " '3471',\n",
       " '1839',\n",
       " '3012',\n",
       " '1687',\n",
       " '3316',\n",
       " '2169',\n",
       " '2935',\n",
       " '1693',\n",
       " '3531',\n",
       " '456',\n",
       " '1206',\n",
       " '1409',\n",
       " '1976',\n",
       " '1055',\n",
       " '1744',\n",
       " '2290',\n",
       " '2184',\n",
       " '2918',\n",
       " '23',\n",
       " '1067',\n",
       " '2804',\n",
       " '1391',\n",
       " '1903',\n",
       " '1661',\n",
       " '410',\n",
       " '3047',\n",
       " '1838',\n",
       " '2476',\n",
       " '2998',\n",
       " '3507',\n",
       " '3030',\n",
       " '1250',\n",
       " '2972',\n",
       " '2217',\n",
       " '3286',\n",
       " '2703',\n",
       " '2388',\n",
       " '1398',\n",
       " '3579',\n",
       " '2429',\n",
       " '1071',\n",
       " '2262',\n",
       " '168',\n",
       " '233',\n",
       " '1028',\n",
       " '2658',\n",
       " '2592',\n",
       " '170',\n",
       " '2394',\n",
       " '1134',\n",
       " '2455',\n",
       " '3103',\n",
       " '1968',\n",
       " '1130',\n",
       " '1480',\n",
       " '135',\n",
       " '1592',\n",
       " '3475',\n",
       " '376',\n",
       " '2131',\n",
       " '430',\n",
       " '1860',\n",
       " '340',\n",
       " '1676',\n",
       " '2377',\n",
       " '1136',\n",
       " '2405',\n",
       " '2082',\n",
       " '1235',\n",
       " '2986',\n",
       " '1563',\n",
       " '305',\n",
       " '3425',\n",
       " '1964',\n",
       " '1295',\n",
       " '1328',\n",
       " '2039',\n",
       " '1560',\n",
       " '355',\n",
       " '196',\n",
       " '2823',\n",
       " '2489',\n",
       " '357',\n",
       " '1024',\n",
       " '1378',\n",
       " '1787',\n",
       " '2639',\n",
       " '1477',\n",
       " '2995',\n",
       " '1717',\n",
       " '1655',\n",
       " '2201',\n",
       " '1595',\n",
       " '1125',\n",
       " '1432',\n",
       " '1026',\n",
       " '3117',\n",
       " '2839',\n",
       " '2115',\n",
       " '3244',\n",
       " '2803',\n",
       " '3297',\n",
       " '1545',\n",
       " '177',\n",
       " '2320',\n",
       " '1814',\n",
       " '3591',\n",
       " '2821',\n",
       " '2379',\n",
       " '1861',\n",
       " '2136',\n",
       " '2167',\n",
       " '1012',\n",
       " '1288',\n",
       " '3560',\n",
       " '2046',\n",
       " '327',\n",
       " '1547',\n",
       " '3146',\n",
       " '3127',\n",
       " '2478',\n",
       " '214',\n",
       " '3564',\n",
       " '1638',\n",
       " '320',\n",
       " '2247',\n",
       " '1029',\n",
       " '3378',\n",
       " '2758',\n",
       " '3582',\n",
       " '2358',\n",
       " '3051',\n",
       " '3323',\n",
       " '2255',\n",
       " '1402',\n",
       " '230',\n",
       " '502',\n",
       " '3019',\n",
       " '3037',\n",
       " '3060',\n",
       " '1258',\n",
       " '1113',\n",
       " '1384',\n",
       " '1035',\n",
       " '1640',\n",
       " '2876',\n",
       " '1824',\n",
       " '3086',\n",
       " '1826',\n",
       " '266',\n",
       " '3104',\n",
       " '3448',\n",
       " '2540',\n",
       " '2575',\n",
       " '2404',\n",
       " '1161',\n",
       " '2591',\n",
       " '2035',\n",
       " '1750',\n",
       " '2175',\n",
       " '3229',\n",
       " '3344',\n",
       " '1620',\n",
       " '2503',\n",
       " '2593',\n",
       " '3201',\n",
       " '1383',\n",
       " '2056',\n",
       " '1527',\n",
       " '1186',\n",
       " '2234',\n",
       " '2810',\n",
       " '1083',\n",
       " '120',\n",
       " '2325',\n",
       " '1793',\n",
       " '2746',\n",
       " '1865',\n",
       " '1827',\n",
       " '3196',\n",
       " '32',\n",
       " '2513',\n",
       " '2498',\n",
       " '3208',\n",
       " '2119',\n",
       " '3449',\n",
       " '1509',\n",
       " '1227',\n",
       " '2003',\n",
       " '1580',\n",
       " '2759',\n",
       " '3596',\n",
       " '3167',\n",
       " '2767',\n",
       " '365',\n",
       " '1255',\n",
       " '1891',\n",
       " '2113',\n",
       " '319',\n",
       " '1671',\n",
       " '209',\n",
       " '283',\n",
       " '2977',\n",
       " '145',\n",
       " '275',\n",
       " '2727',\n",
       " '1557',\n",
       " '2020',\n",
       " '1147',\n",
       " '3384',\n",
       " '1439',\n",
       " '3098',\n",
       " '1692',\n",
       " '2070',\n",
       " '2802',\n",
       " '3454',\n",
       " '3028',\n",
       " '2060',\n",
       " '415',\n",
       " '1338',\n",
       " '3478',\n",
       " '2902',\n",
       " '2055',\n",
       " '1304',\n",
       " '3418',\n",
       " '1152',\n",
       " '1034',\n",
       " '1303',\n",
       " '1097',\n",
       " '387',\n",
       " '1452',\n",
       " '2150',\n",
       " '3559',\n",
       " '478',\n",
       " '3069',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3aeb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1977': 0,\n",
       " '2782': 1,\n",
       " '1236': 2,\n",
       " '2546': 3,\n",
       " '1867': 4,\n",
       " '1647': 5,\n",
       " '1458': 6,\n",
       " '1686': 7,\n",
       " '2647': 8,\n",
       " '1504': 9,\n",
       " '1148': 10,\n",
       " '2373': 11,\n",
       " '2246': 12,\n",
       " '3262': 13,\n",
       " '407': 14,\n",
       " '1569': 15,\n",
       " '226': 16,\n",
       " '1663': 17,\n",
       " '3590': 18,\n",
       " '172': 19,\n",
       " '1991': 20,\n",
       " '192': 21,\n",
       " '3183': 22,\n",
       " '183': 23,\n",
       " '1170': 24,\n",
       " '1191': 25,\n",
       " '256': 26,\n",
       " '1618': 27,\n",
       " '1731': 28,\n",
       " '2076': 29,\n",
       " '203': 30,\n",
       " '3264': 31,\n",
       " '1791': 32,\n",
       " '2709': 33,\n",
       " '2362': 34,\n",
       " '2053': 35,\n",
       " '2657': 36,\n",
       " '3002': 37,\n",
       " '1525': 38,\n",
       " '2954': 39,\n",
       " '2820': 40,\n",
       " '2903': 41,\n",
       " '3372': 42,\n",
       " '2719': 43,\n",
       " '1141': 44,\n",
       " '2222': 45,\n",
       " '2730': 46,\n",
       " '265': 47,\n",
       " '348': 48,\n",
       " '1944': 49,\n",
       " '3036': 50,\n",
       " '1443': 51,\n",
       " '1039': 52,\n",
       " '3522': 53,\n",
       " '1414': 54,\n",
       " '1425': 55,\n",
       " '1473': 56,\n",
       " '1697': 57,\n",
       " '2724': 58,\n",
       " '1902': 59,\n",
       " '1355': 60,\n",
       " '2385': 61,\n",
       " '466': 62,\n",
       " '2276': 63,\n",
       " '1015': 64,\n",
       " '3338': 65,\n",
       " '1481': 66,\n",
       " '1433': 67,\n",
       " '2144': 68,\n",
       " '3578': 69,\n",
       " '2529': 70,\n",
       " '1486': 71,\n",
       " '2157': 72,\n",
       " '3501': 73,\n",
       " '1259': 74,\n",
       " '3095': 75,\n",
       " '2240': 76,\n",
       " '2883': 77,\n",
       " '1915': 78,\n",
       " '1004': 79,\n",
       " '1107': 80,\n",
       " '472': 81,\n",
       " '2467': 82,\n",
       " '1490': 83,\n",
       " '2391': 84,\n",
       " '148': 85,\n",
       " '3345': 86,\n",
       " '2387': 87,\n",
       " '413': 88,\n",
       " '2111': 89,\n",
       " '2326': 90,\n",
       " '2694': 91,\n",
       " '2840': 92,\n",
       " '1285': 93,\n",
       " '2279': 94,\n",
       " '1658': 95,\n",
       " '3350': 96,\n",
       " '3524': 97,\n",
       " '1442': 98,\n",
       " '1570': 99,\n",
       " '2428': 100,\n",
       " '2079': 101,\n",
       " '2088': 102,\n",
       " '1153': 103,\n",
       " '2571': 104,\n",
       " '3541': 105,\n",
       " '1718': 106,\n",
       " '2159': 107,\n",
       " '2539': 108,\n",
       " '2744': 109,\n",
       " '2244': 110,\n",
       " '1319': 111,\n",
       " '3364': 112,\n",
       " '3550': 113,\n",
       " '2907': 114,\n",
       " '1773': 115,\n",
       " '1333': 116,\n",
       " '112': 117,\n",
       " '3407': 118,\n",
       " '121': 119,\n",
       " '272': 120,\n",
       " '2045': 121,\n",
       " '188': 122,\n",
       " '2139': 123,\n",
       " '171': 124,\n",
       " '1403': 125,\n",
       " '1101': 126,\n",
       " '1277': 127,\n",
       " '2656': 128,\n",
       " '3567': 129,\n",
       " '2106': 130,\n",
       " '3294': 131,\n",
       " '251': 132,\n",
       " '1696': 133,\n",
       " '2801': 134,\n",
       " '1082': 135,\n",
       " '2348': 136,\n",
       " '1667': 137,\n",
       " '229': 138,\n",
       " '3486': 139,\n",
       " '2143': 140,\n",
       " '17': 141,\n",
       " '1353': 142,\n",
       " '1224': 143,\n",
       " '2566': 144,\n",
       " '454': 145,\n",
       " '2655': 146,\n",
       " '352': 147,\n",
       " '1740': 148,\n",
       " '2772': 149,\n",
       " '3007': 150,\n",
       " '1322': 151,\n",
       " '3552': 152,\n",
       " '1517': 153,\n",
       " '1691': 154,\n",
       " '279': 155,\n",
       " '1633': 156,\n",
       " '1776': 157,\n",
       " '2218': 158,\n",
       " '222': 159,\n",
       " '1387': 160,\n",
       " '1485': 161,\n",
       " '3181': 162,\n",
       " '3429': 163,\n",
       " '2108': 164,\n",
       " '1172': 165,\n",
       " '1331': 166,\n",
       " '1061': 167,\n",
       " '206': 168,\n",
       " '1713': 169,\n",
       " '3343': 170,\n",
       " '2004': 171,\n",
       " '1519': 172,\n",
       " '2522': 173,\n",
       " '2948': 174,\n",
       " '3109': 175,\n",
       " '1662': 176,\n",
       " '2006': 177,\n",
       " '2417': 178,\n",
       " '3101': 179,\n",
       " '1565': 180,\n",
       " '2177': 181,\n",
       " '1019': 182,\n",
       " '3049': 183,\n",
       " '1755': 184,\n",
       " '2845': 185,\n",
       " '2650': 186,\n",
       " '2731': 187,\n",
       " '2747': 188,\n",
       " '1802': 189,\n",
       " '2551': 190,\n",
       " '2749': 191,\n",
       " '146': 192,\n",
       " '1023': 193,\n",
       " '264': 194,\n",
       " '38': 195,\n",
       " '2226': 196,\n",
       " '1450': 197,\n",
       " '1900': 198,\n",
       " '2015': 199,\n",
       " '27': 200,\n",
       " '343': 201,\n",
       " '1245': 202,\n",
       " '1297': 203,\n",
       " '1936': 204,\n",
       " '299': 205,\n",
       " '1942': 206,\n",
       " '3158': 207,\n",
       " '2171': 208,\n",
       " '2860': 209,\n",
       " '2190': 210,\n",
       " '1782': 211,\n",
       " '1901': 212,\n",
       " '2921': 213,\n",
       " '2750': 214,\n",
       " '1027': 215,\n",
       " '1806': 216,\n",
       " '3091': 217,\n",
       " '102': 218,\n",
       " '3594': 219,\n",
       " '3057': 220,\n",
       " '2942': 221,\n",
       " '3342': 222,\n",
       " '1842': 223,\n",
       " '3066': 224,\n",
       " '3276': 225,\n",
       " '2054': 226,\n",
       " '335': 227,\n",
       " '2602': 228,\n",
       " '2034': 229,\n",
       " '1121': 230,\n",
       " '1117': 231,\n",
       " '1218': 232,\n",
       " '1578': 233,\n",
       " '351': 234,\n",
       " '2158': 235,\n",
       " '2297': 236,\n",
       " '2720': 237,\n",
       " '2314': 238,\n",
       " '2613': 239,\n",
       " '3451': 240,\n",
       " '2212': 241,\n",
       " '1631': 242,\n",
       " '2920': 243,\n",
       " '1871': 244,\n",
       " '2927': 245,\n",
       " '1666': 246,\n",
       " '3481': 247,\n",
       " '3128': 248,\n",
       " '3375': 249,\n",
       " '1890': 250,\n",
       " '3517': 251,\n",
       " '140': 252,\n",
       " '3558': 253,\n",
       " '2738': 254,\n",
       " '1612': 255,\n",
       " '3527': 256,\n",
       " '3255': 257,\n",
       " '1952': 258,\n",
       " '1737': 259,\n",
       " '2619': 260,\n",
       " '1855': 261,\n",
       " '2824': 262,\n",
       " '1924': 263,\n",
       " '3497': 264,\n",
       " '2209': 265,\n",
       " '1254': 266,\n",
       " '1492': 267,\n",
       " '2187': 268,\n",
       " '2147': 269,\n",
       " '1126': 270,\n",
       " '30': 271,\n",
       " '1732': 272,\n",
       " '2836': 273,\n",
       " '3282': 274,\n",
       " '2061': 275,\n",
       " '1051': 276,\n",
       " '1309': 277,\n",
       " '3256': 278,\n",
       " '1135': 279,\n",
       " '2926': 280,\n",
       " '3061': 281,\n",
       " '1550': 282,\n",
       " '1394': 283,\n",
       " '2432': 284,\n",
       " '1335': 285,\n",
       " '2114': 286,\n",
       " '2188': 287,\n",
       " '2736': 288,\n",
       " '2600': 289,\n",
       " '1876': 290,\n",
       " '1427': 291,\n",
       " '2507': 292,\n",
       " '3409': 293,\n",
       " '2623': 294,\n",
       " '3260': 295,\n",
       " '2197': 296,\n",
       " '48': 297,\n",
       " '1847': 298,\n",
       " '3365': 299,\n",
       " '3521': 300,\n",
       " '1833': 301,\n",
       " '1212': 302,\n",
       " '2880': 303,\n",
       " '2375': 304,\n",
       " '1883': 305,\n",
       " '288': 306,\n",
       " '2189': 307,\n",
       " '1190': 308,\n",
       " '1342': 309,\n",
       " '304': 310,\n",
       " '1242': 311,\n",
       " '2024': 312,\n",
       " '3004': 313,\n",
       " '2032': 314,\n",
       " '2050': 315,\n",
       " '1874': 316,\n",
       " '1553': 317,\n",
       " '161': 318,\n",
       " '3452': 319,\n",
       " '1469': 320,\n",
       " '1194': 321,\n",
       " '2257': 322,\n",
       " '1854': 323,\n",
       " '3357': 324,\n",
       " '1653': 325,\n",
       " '1798': 326,\n",
       " '2634': 327,\n",
       " '2543': 328,\n",
       " '2669': 329,\n",
       " '2561': 330,\n",
       " '3116': 331,\n",
       " '1704': 332,\n",
       " '2173': 333,\n",
       " '2858': 334,\n",
       " '1088': 335,\n",
       " '2991': 336,\n",
       " '1683': 337,\n",
       " '3026': 338,\n",
       " '3192': 339,\n",
       " '3013': 340,\n",
       " '2664': 341,\n",
       " '33': 342,\n",
       " '159': 343,\n",
       " '3141': 344,\n",
       " '3277': 345,\n",
       " '2356': 346,\n",
       " '1513': 347,\n",
       " '3392': 348,\n",
       " '2347': 349,\n",
       " '3490': 350,\n",
       " '3402': 351,\n",
       " '1888': 352,\n",
       " '1783': 353,\n",
       " '1202': 354,\n",
       " '2353': 355,\n",
       " '1270': 356,\n",
       " '241': 357,\n",
       " '2848': 358,\n",
       " '42': 359,\n",
       " '270': 360,\n",
       " '3431': 361,\n",
       " '1307': 362,\n",
       " '457': 363,\n",
       " '310': 364,\n",
       " '2446': 365,\n",
       " '2063': 366,\n",
       " '2291': 367,\n",
       " '2967': 368,\n",
       " '3198': 369,\n",
       " '1086': 370,\n",
       " '249': 371,\n",
       " '487': 372,\n",
       " '3153': 373,\n",
       " '1047': 374,\n",
       " '1334': 375,\n",
       " '2122': 376,\n",
       " '14': 377,\n",
       " '2141': 378,\n",
       " '2396': 379,\n",
       " '3414': 380,\n",
       " '1008': 381,\n",
       " '1652': 382,\n",
       " '3416': 383,\n",
       " '1145': 384,\n",
       " '1675': 385,\n",
       " '291': 386,\n",
       " '1775': 387,\n",
       " '1324': 388,\n",
       " '2398': 389,\n",
       " '132': 390,\n",
       " '2864': 391,\n",
       " '1934': 392,\n",
       " '3306': 393,\n",
       " '1318': 394,\n",
       " '2294': 395,\n",
       " '2340': 396,\n",
       " '3332': 397,\n",
       " '2030': 398,\n",
       " '152': 399,\n",
       " '2628': 400,\n",
       " '3540': 401,\n",
       " '1698': 402,\n",
       " '2900': 403,\n",
       " '2586': 404,\n",
       " '114': 405,\n",
       " '1471': 406,\n",
       " '1484': 407,\n",
       " '373': 408,\n",
       " '1456': 409,\n",
       " '2180': 410,\n",
       " '463': 411,\n",
       " '3437': 412,\n",
       " '2945': 413,\n",
       " '1286': 414,\n",
       " '1420': 415,\n",
       " '3075': 416,\n",
       " '2409': 417,\n",
       " '1932': 418,\n",
       " '1444': 419,\n",
       " '1939': 420,\n",
       " '1745': 421,\n",
       " '1743': 422,\n",
       " '2378': 423,\n",
       " '2828': 424,\n",
       " '2110': 425,\n",
       " '2545': 426,\n",
       " '2721': 427,\n",
       " '139': 428,\n",
       " '1695': 429,\n",
       " '1476': 430,\n",
       " '1724': 431,\n",
       " '399': 432,\n",
       " '374': 433,\n",
       " '2581': 434,\n",
       " '3010': 435,\n",
       " '1830': 436,\n",
       " '3275': 437,\n",
       " '3156': 438,\n",
       " '2072': 439,\n",
       " '1156': 440,\n",
       " '1817': 441,\n",
       " '1734': 442,\n",
       " '1099': 443,\n",
       " '3460': 444,\n",
       " '1009': 445,\n",
       " '3510': 446,\n",
       " '1114': 447,\n",
       " '1719': 448,\n",
       " '1796': 449,\n",
       " '3063': 450,\n",
       " '3143': 451,\n",
       " '293': 452,\n",
       " '484': 453,\n",
       " '338': 454,\n",
       " '2041': 455,\n",
       " '1188': 456,\n",
       " '2210': 457,\n",
       " '1729': 458,\n",
       " '3371': 459,\n",
       " '2357': 460,\n",
       " '2642': 461,\n",
       " '2127': 462,\n",
       " '3538': 463,\n",
       " '2960': 464,\n",
       " '2254': 465,\n",
       " '2327': 466,\n",
       " '3381': 467,\n",
       " '427': 468,\n",
       " '3467': 469,\n",
       " '3241': 470,\n",
       " '1628': 471,\n",
       " '2207': 472,\n",
       " '401': 473,\n",
       " '3528': 474,\n",
       " '2463': 475,\n",
       " '3014': 476,\n",
       " '1749': 477,\n",
       " '1822': 478,\n",
       " '2697': 479,\n",
       " '2018': 480,\n",
       " '2913': 481,\n",
       " '2973': 482,\n",
       " '2413': 483,\n",
       " '1753': 484,\n",
       " '2264': 485,\n",
       " '1044': 486,\n",
       " '1584': 487,\n",
       " '2293': 488,\n",
       " '1777': 489,\n",
       " '3395': 490,\n",
       " '1247': 491,\n",
       " '2531': 492,\n",
       " '3423': 493,\n",
       " '1657': 494,\n",
       " '247': 495,\n",
       " '3575': 496,\n",
       " '200': 497,\n",
       " '3568': 498,\n",
       " '1410': 499,\n",
       " '3113': 500,\n",
       " '3505': 501,\n",
       " '2205': 502,\n",
       " '2646': 503,\n",
       " '1778': 504,\n",
       " '2275': 505,\n",
       " '3503': 506,\n",
       " '1020': 507,\n",
       " '3577': 508,\n",
       " '2443': 509,\n",
       " '465': 510,\n",
       " '2372': 511,\n",
       " '136': 512,\n",
       " '167': 513,\n",
       " '2448': 514,\n",
       " '2194': 515,\n",
       " '1163': 516,\n",
       " '1349': 517,\n",
       " '362': 518,\n",
       " '1603': 519,\n",
       " '1841': 520,\n",
       " '2771': 521,\n",
       " '3464': 522,\n",
       " '1621': 523,\n",
       " '3178': 524,\n",
       " '3461': 525,\n",
       " '1408': 526,\n",
       " '3100': 527,\n",
       " '1232': 528,\n",
       " '2511': 529,\n",
       " '1308': 530,\n",
       " '2155': 531,\n",
       " '3440': 532,\n",
       " '3386': 533,\n",
       " '1025': 534,\n",
       " '3515': 535,\n",
       " '2390': 536,\n",
       " '1221': 537,\n",
       " '2027': 538,\n",
       " '2242': 539,\n",
       " '3339': 540,\n",
       " '398': 541,\n",
       " '2306': 542,\n",
       " '461': 543,\n",
       " '1261': 544,\n",
       " '375': 545,\n",
       " '2596': 546,\n",
       " '2492': 547,\n",
       " '2007': 548,\n",
       " '2313': 549,\n",
       " '3270': 550,\n",
       " '2249': 551,\n",
       " '3278': 552,\n",
       " '1041': 553,\n",
       " '1500': 554,\n",
       " '2295': 555,\n",
       " '3518': 556,\n",
       " '3133': 557,\n",
       " '2454': 558,\n",
       " '3265': 559,\n",
       " '1445': 560,\n",
       " '2224': 561,\n",
       " '1970': 562,\n",
       " '426': 563,\n",
       " '467': 564,\n",
       " '1368': 565,\n",
       " '1937': 566,\n",
       " '419': 567,\n",
       " '2524': 568,\n",
       " '2399': 569,\n",
       " '1870': 570,\n",
       " '3436': 571,\n",
       " '1146': 572,\n",
       " '388': 573,\n",
       " '1042': 574,\n",
       " '2608': 575,\n",
       " '1098': 576,\n",
       " '3219': 577,\n",
       " '1611': 578,\n",
       " '1311': 579,\n",
       " '2852': 580,\n",
       " '1877': 581,\n",
       " '2648': 582,\n",
       " '412': 583,\n",
       " '1850': 584,\n",
       " '1155': 585,\n",
       " '1797': 586,\n",
       " '3190': 587,\n",
       " '2838': 588,\n",
       " '1371': 589,\n",
       " '2521': 590,\n",
       " '1812': 591,\n",
       " '2485': 592,\n",
       " '2331': 593,\n",
       " '2253': 594,\n",
       " '3126': 595,\n",
       " '2068': 596,\n",
       " '1105': 597,\n",
       " '322': 598,\n",
       " '282': 599,\n",
       " '2424': 600,\n",
       " '3573': 601,\n",
       " '2874': 602,\n",
       " '3068': 603,\n",
       " '3508': 604,\n",
       " '1979': 605,\n",
       " '1945': 606,\n",
       " '424': 607,\n",
       " '3096': 608,\n",
       " '3235': 609,\n",
       " '2423': 610,\n",
       " '1196': 611,\n",
       " '3076': 612,\n",
       " '1639': 613,\n",
       " '2762': 614,\n",
       " '475': 615,\n",
       " '1195': 616,\n",
       " '2956': 617,\n",
       " '3382': 618,\n",
       " '186': 619,\n",
       " '1586': 620,\n",
       " '1175': 621,\n",
       " '447': 622,\n",
       " '1511': 623,\n",
       " '324': 624,\n",
       " '2859': 625,\n",
       " '3152': 626,\n",
       " '1917': 627,\n",
       " '1493': 628,\n",
       " '3245': 629,\n",
       " '2270': 630,\n",
       " '3543': 631,\n",
       " '488': 632,\n",
       " '3356': 633,\n",
       " '2073': 634,\n",
       " '3018': 635,\n",
       " '1220': 636,\n",
       " '2784': 637,\n",
       " '1894': 638,\n",
       " '1118': 639,\n",
       " '1809': 640,\n",
       " '3466': 641,\n",
       " '3195': 642,\n",
       " '3085': 643,\n",
       " '3445': 644,\n",
       " '1710': 645,\n",
       " '2690': 646,\n",
       " '1237': 647,\n",
       " '157': 648,\n",
       " '234': 649,\n",
       " '1068': 650,\n",
       " '201': 651,\n",
       " '3355': 652,\n",
       " '2542': 653,\n",
       " '2156': 654,\n",
       " '1637': 655,\n",
       " '280': 656,\n",
       " '1167': 657,\n",
       " '3433': 658,\n",
       " '1885': 659,\n",
       " '2220': 660,\n",
       " '3379': 661,\n",
       " '2096': 662,\n",
       " '2761': 663,\n",
       " '1316': 664,\n",
       " '2316': 665,\n",
       " '1625': 666,\n",
       " '3556': 667,\n",
       " '1198': 668,\n",
       " '471': 669,\n",
       " '1060': 670,\n",
       " '1954': 671,\n",
       " '147': 672,\n",
       " '29': 673,\n",
       " '3058': 674,\n",
       " '2058': 675,\n",
       " '1112': 676,\n",
       " '1363': 677,\n",
       " '106': 678,\n",
       " '2788': 679,\n",
       " '1507': 680,\n",
       " '2764': 681,\n",
       " '2365': 682,\n",
       " '2368': 683,\n",
       " '422': 684,\n",
       " '5': 685,\n",
       " '2981': 686,\n",
       " '1540': 687,\n",
       " '1590': 688,\n",
       " '300': 689,\n",
       " '2002': 690,\n",
       " '2822': 691,\n",
       " '15': 692,\n",
       " '276': 693,\n",
       " '1472': 694,\n",
       " '2483': 695,\n",
       " '3123': 696,\n",
       " '277': 697,\n",
       " '2518': 698,\n",
       " '1680': 699,\n",
       " '2559': 700,\n",
       " '1385': 701,\n",
       " '2280': 702,\n",
       " '2816': 703,\n",
       " '19': 704,\n",
       " '3363': 705,\n",
       " '1572': 706,\n",
       " '1248': 707,\n",
       " '2704': 708,\n",
       " '2812': 709,\n",
       " '3498': 710,\n",
       " '2089': 711,\n",
       " '2493': 712,\n",
       " '2292': 713,\n",
       " '126': 714,\n",
       " '2059': 715,\n",
       " '1610': 716,\n",
       " '3191': 717,\n",
       " '41': 718,\n",
       " '3246': 719,\n",
       " '2425': 720,\n",
       " '1110': 721,\n",
       " '2376': 722,\n",
       " '1238': 723,\n",
       " '483': 724,\n",
       " '1305': 725,\n",
       " '298': 726,\n",
       " '3584': 727,\n",
       " '3525': 728,\n",
       " '208': 729,\n",
       " '2773': 730,\n",
       " '1892': 731,\n",
       " '3348': 732,\n",
       " '2896': 733,\n",
       " '3427': 734,\n",
       " '2951': 735,\n",
       " '11': 736,\n",
       " '1341': 737,\n",
       " '2843': 738,\n",
       " '2049': 739,\n",
       " '1654': 740,\n",
       " '1836': 741,\n",
       " '2161': 742,\n",
       " '2632': 743,\n",
       " '1256': 744,\n",
       " '371': 745,\n",
       " '2367': 746,\n",
       " '2556': 747,\n",
       " '2885': 748,\n",
       " '2043': 749,\n",
       " '1528': 750,\n",
       " '1482': 751,\n",
       " '329': 752,\n",
       " '2841': 753,\n",
       " '1116': 754,\n",
       " '2671': 755,\n",
       " '1974': 756,\n",
       " '2733': 757,\n",
       " '2401': 758,\n",
       " '2434': 759,\n",
       " '1765': 760,\n",
       " '2093': 761,\n",
       " '325': 762,\n",
       " '2178': 763,\n",
       " '237': 764,\n",
       " '3471': 765,\n",
       " '1839': 766,\n",
       " '3012': 767,\n",
       " '1687': 768,\n",
       " '3316': 769,\n",
       " '2169': 770,\n",
       " '2935': 771,\n",
       " '1693': 772,\n",
       " '3531': 773,\n",
       " '456': 774,\n",
       " '1206': 775,\n",
       " '1409': 776,\n",
       " '1976': 777,\n",
       " '1055': 778,\n",
       " '1744': 779,\n",
       " '2290': 780,\n",
       " '2184': 781,\n",
       " '2918': 782,\n",
       " '23': 783,\n",
       " '1067': 784,\n",
       " '2804': 785,\n",
       " '1391': 786,\n",
       " '1903': 787,\n",
       " '1661': 788,\n",
       " '410': 789,\n",
       " '3047': 790,\n",
       " '1838': 791,\n",
       " '2476': 792,\n",
       " '2998': 793,\n",
       " '3507': 794,\n",
       " '3030': 795,\n",
       " '1250': 796,\n",
       " '2972': 797,\n",
       " '2217': 798,\n",
       " '3286': 799,\n",
       " '2703': 800,\n",
       " '2388': 801,\n",
       " '1398': 802,\n",
       " '3579': 803,\n",
       " '2429': 804,\n",
       " '1071': 805,\n",
       " '2262': 806,\n",
       " '168': 807,\n",
       " '233': 808,\n",
       " '1028': 809,\n",
       " '2658': 810,\n",
       " '2592': 811,\n",
       " '170': 812,\n",
       " '2394': 813,\n",
       " '1134': 814,\n",
       " '2455': 815,\n",
       " '3103': 816,\n",
       " '1968': 817,\n",
       " '1130': 818,\n",
       " '1480': 819,\n",
       " '135': 820,\n",
       " '1592': 821,\n",
       " '3475': 822,\n",
       " '376': 823,\n",
       " '2131': 824,\n",
       " '430': 825,\n",
       " '1860': 826,\n",
       " '340': 827,\n",
       " '1676': 828,\n",
       " '2377': 829,\n",
       " '1136': 830,\n",
       " '2405': 831,\n",
       " '2082': 832,\n",
       " '1235': 833,\n",
       " '2986': 834,\n",
       " '1563': 835,\n",
       " '305': 836,\n",
       " '3425': 837,\n",
       " '1964': 838,\n",
       " '1295': 839,\n",
       " '1328': 840,\n",
       " '2039': 841,\n",
       " '1560': 842,\n",
       " '355': 843,\n",
       " '196': 844,\n",
       " '2823': 845,\n",
       " '2489': 846,\n",
       " '357': 847,\n",
       " '1024': 848,\n",
       " '1378': 849,\n",
       " '1787': 850,\n",
       " '2639': 851,\n",
       " '1477': 852,\n",
       " '2995': 853,\n",
       " '1717': 854,\n",
       " '1655': 855,\n",
       " '2201': 856,\n",
       " '1595': 857,\n",
       " '1125': 858,\n",
       " '1432': 859,\n",
       " '1026': 860,\n",
       " '3117': 861,\n",
       " '2839': 862,\n",
       " '2115': 863,\n",
       " '3244': 864,\n",
       " '2803': 865,\n",
       " '3297': 866,\n",
       " '1545': 867,\n",
       " '177': 868,\n",
       " '2320': 869,\n",
       " '1814': 870,\n",
       " '3591': 871,\n",
       " '2821': 872,\n",
       " '2379': 873,\n",
       " '1861': 874,\n",
       " '2136': 875,\n",
       " '2167': 876,\n",
       " '1012': 877,\n",
       " '1288': 878,\n",
       " '3560': 879,\n",
       " '2046': 880,\n",
       " '327': 881,\n",
       " '1547': 882,\n",
       " '3146': 883,\n",
       " '3127': 884,\n",
       " '2478': 885,\n",
       " '214': 886,\n",
       " '3564': 887,\n",
       " '1638': 888,\n",
       " '320': 889,\n",
       " '2247': 890,\n",
       " '1029': 891,\n",
       " '3378': 892,\n",
       " '2758': 893,\n",
       " '3582': 894,\n",
       " '2358': 895,\n",
       " '3051': 896,\n",
       " '3323': 897,\n",
       " '2255': 898,\n",
       " '1402': 899,\n",
       " '230': 900,\n",
       " '502': 901,\n",
       " '3019': 902,\n",
       " '3037': 903,\n",
       " '3060': 904,\n",
       " '1258': 905,\n",
       " '1113': 906,\n",
       " '1384': 907,\n",
       " '1035': 908,\n",
       " '1640': 909,\n",
       " '2876': 910,\n",
       " '1824': 911,\n",
       " '3086': 912,\n",
       " '1826': 913,\n",
       " '266': 914,\n",
       " '3104': 915,\n",
       " '3448': 916,\n",
       " '2540': 917,\n",
       " '2575': 918,\n",
       " '2404': 919,\n",
       " '1161': 920,\n",
       " '2591': 921,\n",
       " '2035': 922,\n",
       " '1750': 923,\n",
       " '2175': 924,\n",
       " '3229': 925,\n",
       " '3344': 926,\n",
       " '1620': 927,\n",
       " '2503': 928,\n",
       " '2593': 929,\n",
       " '3201': 930,\n",
       " '1383': 931,\n",
       " '2056': 932,\n",
       " '1527': 933,\n",
       " '1186': 934,\n",
       " '2234': 935,\n",
       " '2810': 936,\n",
       " '1083': 937,\n",
       " '120': 938,\n",
       " '2325': 939,\n",
       " '1793': 940,\n",
       " '2746': 941,\n",
       " '1865': 942,\n",
       " '1827': 943,\n",
       " '3196': 944,\n",
       " '32': 945,\n",
       " '2513': 946,\n",
       " '2498': 947,\n",
       " '3208': 948,\n",
       " '2119': 949,\n",
       " '3449': 950,\n",
       " '1509': 951,\n",
       " '1227': 952,\n",
       " '2003': 953,\n",
       " '1580': 954,\n",
       " '2759': 955,\n",
       " '3596': 956,\n",
       " '3167': 957,\n",
       " '2767': 958,\n",
       " '365': 959,\n",
       " '1255': 960,\n",
       " '1891': 961,\n",
       " '2113': 962,\n",
       " '319': 963,\n",
       " '1671': 964,\n",
       " '209': 965,\n",
       " '283': 966,\n",
       " '2977': 967,\n",
       " '145': 968,\n",
       " '275': 969,\n",
       " '2727': 970,\n",
       " '1557': 971,\n",
       " '2020': 972,\n",
       " '1147': 973,\n",
       " '3384': 974,\n",
       " '1439': 975,\n",
       " '3098': 976,\n",
       " '1692': 977,\n",
       " '2070': 978,\n",
       " '2802': 979,\n",
       " '3454': 980,\n",
       " '3028': 981,\n",
       " '2060': 982,\n",
       " '415': 983,\n",
       " '1338': 984,\n",
       " '3478': 985,\n",
       " '2902': 986,\n",
       " '2055': 987,\n",
       " '1304': 988,\n",
       " '3418': 989,\n",
       " '1152': 990,\n",
       " '1034': 991,\n",
       " '1303': 992,\n",
       " '1097': 993,\n",
       " '387': 994,\n",
       " '1452': 995,\n",
       " '2150': 996,\n",
       " '3559': 997,\n",
       " '478': 998,\n",
       " '3069': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d132ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e34e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classe = len(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9b07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba37ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.dir_list = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.files = []\n",
    "        for folder in self.dir_list:\n",
    "          folder_list = os.listdir(os.path.join(img_dir, folder))\n",
    "          for file in folder_list:\n",
    "            self.files.append([\n",
    "                os.path.join(self.img_dir, folder+'/'+file),\n",
    "                folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.files[idx][0])\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        label = self.files[idx][1]\n",
    "        label = lb[label]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb1b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainset = CustomDataset(train_path, trans)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = CustomDataset(test_path, trans)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ad169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ef55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc6eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size, margin, scale):\n",
    "        \"\"\"\n",
    "        ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "        (https://arxiv.org/pdf/1801.07698.pdf)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "            margin: m in the paper, the angular margin penalty in radians\n",
    "            scale: s in the paper, feature scale\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        cosine = self.get_cosine(embeddings) # (None, n_classes)\n",
    "        mask = self.get_target_mask(labels) # (None, n_classes)\n",
    "        cosine_of_target_classes = cosine[mask == 1] # (None, )\n",
    "        modified_cosine_of_target_classes = self.modify_cosine_of_target_classes(\n",
    "            cosine_of_target_classes\n",
    "        ) # (None, )\n",
    "        diff = (modified_cosine_of_target_classes - cosine_of_target_classes).unsqueeze(1) # (None,1)\n",
    "        logits = cosine + (mask * diff) # (None, n_classes)\n",
    "        logits = self.scale_logits(logits) # (None, n_classes)\n",
    "        return logits\n",
    "        \n",
    "    def get_cosine(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "        Returns:\n",
    "            cosine: (None, n_classes)\n",
    "        \"\"\"\n",
    "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.W))\n",
    "        return cosine\n",
    "    \n",
    "    def get_target_mask(self, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            mask: (None, n_classes)\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        onehot = torch.zeros(batch_size, self.num_classes, device=labels.device)\n",
    "        onehot.scatter_(1, labels.unsqueeze(-1), 1)\n",
    "        return onehot\n",
    "        \n",
    "    def modify_cosine_of_target_classes(self, cosine_of_target_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cosine_of_target_classes: (None,)\n",
    "        Returns:\n",
    "            modified_cosine_of_target_classes: (None,)\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        # theta in the paper\n",
    "        angles = torch.acos(torch.clamp(cosine_of_target_classes, -1 + eps, 1 - eps))\n",
    "        return torch.cos(angles + self.margin)\n",
    "    \n",
    "    def scale_logits(self, logits):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (None, n_classes)\n",
    "        Returns:\n",
    "            scaled_logits: (None, n_classes)\n",
    "        \"\"\"\n",
    "        return logits * self.scale\n",
    "    \n",
    "class SoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size):\n",
    "        \"\"\"\n",
    "        Regular softmax loss (1 fc layer without bias + CrossEntropyLoss)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        logits = F.linear(embeddings, self.W)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f3c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet50(pretrained = False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Linear(1000, embedding_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.model(images)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.classifier(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3128edcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Batch: 4032 - Loss: 17.163076 - Time:12.20831823348999\n",
      "Epoch: 0 - Batch: 8128 - Loss: 17.195965 - Time:21.61734890937805\n",
      "Epoch: 0 - Batch: 12224 - Loss: 17.280937 - Time:31.032713174819946\n",
      "Epoch: 0 - Batch: 16320 - Loss: 17.035328 - Time:40.45094132423401\n",
      "Epoch: 0 - Batch: 20416 - Loss: 17.044983 - Time:49.87278079986572\n",
      "Epoch: 0 - Batch: 24512 - Loss: 16.852249 - Time:59.326844453811646\n",
      "Epoch: 0 - Batch: 28608 - Loss: 17.301140 - Time:68.75272631645203\n",
      "Epoch: 0 - Batch: 32704 - Loss: 17.026781 - Time:78.17005562782288\n",
      "Epoch: 0 - Batch: 36800 - Loss: 17.130539 - Time:87.58853101730347\n",
      "Epoch: 0 - Batch: 40896 - Loss: 17.096899 - Time:97.02509665489197\n",
      "Epoch: 0 - Batch: 44992 - Loss: 17.061865 - Time:106.44508123397827\n",
      "Epoch: 0 - Batch: 49088 - Loss: 17.392115 - Time:115.86147689819336\n",
      "Epoch: 0 - Batch: 53184 - Loss: 17.149094 - Time:125.28547835350037\n",
      "Epoch: 0 - Batch: 57280 - Loss: 17.105104 - Time:134.69430208206177\n",
      "Epoch: 0 - Batch: 61376 - Loss: 17.180023 - Time:144.10358428955078\n",
      "Epoch: 0 - Batch: 65472 - Loss: 17.313602 - Time:153.5144374370575\n",
      "Epoch: 0 - Batch: 69568 - Loss: 16.881760 - Time:162.92505526542664\n",
      "Epoch: 0 - Batch: 73664 - Loss: 17.074352 - Time:172.33532738685608\n",
      "Epoch: 0 - Batch: 77760 - Loss: 16.858677 - Time:181.74391412734985\n",
      "Epoch: 0 - Batch: 81856 - Loss: 17.025537 - Time:191.15256237983704\n",
      "Epoch: 0 - Batch: 85952 - Loss: 17.208979 - Time:200.57780933380127\n",
      "Epoch: 0 - Batch: 90048 - Loss: 17.070618 - Time:209.9883770942688\n",
      "Epoch: 0 - Batch: 94144 - Loss: 17.043610 - Time:219.39702224731445\n",
      "Epoch: 0 - Batch: 98240 - Loss: 17.299076 - Time:228.8075737953186\n",
      "Epoch: 0 - Batch: 102336 - Loss: 17.151255 - Time:238.23350286483765\n",
      "Epoch: 0 - Batch: 106432 - Loss: 17.093763 - Time:247.64231276512146\n",
      "Epoch: 0 - Batch: 110528 - Loss: 17.091347 - Time:257.0512571334839\n",
      "Epoch: 0 - Batch: 114624 - Loss: 16.946234 - Time:266.4759793281555\n",
      "Epoch: 0 - Batch: 118720 - Loss: 16.870966 - Time:275.886677980423\n",
      "Epoch: 0 - Batch: 122816 - Loss: 17.057825 - Time:285.2970395088196\n",
      "Epoch: 0 - Batch: 126912 - Loss: 17.032688 - Time:294.7053802013397\n",
      "Epoch: 0 - Batch: 131008 - Loss: 16.998621 - Time:304.1147150993347\n",
      "Epoch: 0 - Batch: 135104 - Loss: 16.713623 - Time:313.52495765686035\n",
      "Epoch: 0 - Batch: 139200 - Loss: 17.038822 - Time:322.93417263031006\n",
      "Epoch: 0 - Batch: 143296 - Loss: 17.255566 - Time:332.3453154563904\n",
      "Epoch: 0 - Batch: 147392 - Loss: 16.977764 - Time:341.7712240219116\n",
      "Epoch: 0 - Batch: 151488 - Loss: 16.968472 - Time:351.1814296245575\n",
      "Epoch: 1 - Batch: 4032 - Loss: 16.942520 - Time:9.550329446792603\n",
      "Epoch: 1 - Batch: 8128 - Loss: 16.847258 - Time:18.962496757507324\n",
      "Epoch: 1 - Batch: 12224 - Loss: 16.811399 - Time:28.372134923934937\n",
      "Epoch: 1 - Batch: 16320 - Loss: 16.926977 - Time:37.78419589996338\n",
      "Epoch: 1 - Batch: 20416 - Loss: 17.036997 - Time:47.21945786476135\n",
      "Epoch: 1 - Batch: 24512 - Loss: 16.840166 - Time:56.62793016433716\n",
      "Epoch: 1 - Batch: 28608 - Loss: 16.752562 - Time:66.03631472587585\n",
      "Epoch: 1 - Batch: 32704 - Loss: 17.059883 - Time:75.44755697250366\n",
      "Epoch: 1 - Batch: 36800 - Loss: 16.892656 - Time:84.8730821609497\n",
      "Epoch: 1 - Batch: 40896 - Loss: 16.728952 - Time:94.28243231773376\n",
      "Epoch: 1 - Batch: 44992 - Loss: 16.874840 - Time:103.69457054138184\n",
      "Epoch: 1 - Batch: 49088 - Loss: 17.023489 - Time:113.12008857727051\n",
      "Epoch: 1 - Batch: 53184 - Loss: 16.900648 - Time:122.52880001068115\n",
      "Epoch: 1 - Batch: 57280 - Loss: 16.846191 - Time:131.9392192363739\n",
      "Epoch: 1 - Batch: 61376 - Loss: 17.031124 - Time:141.34826278686523\n",
      "Epoch: 1 - Batch: 65472 - Loss: 16.862988 - Time:150.75909066200256\n",
      "Epoch: 1 - Batch: 69568 - Loss: 16.999659 - Time:160.17045831680298\n",
      "Epoch: 1 - Batch: 73664 - Loss: 16.774321 - Time:169.58192372322083\n",
      "Epoch: 1 - Batch: 77760 - Loss: 16.959639 - Time:178.9923734664917\n",
      "Epoch: 1 - Batch: 81856 - Loss: 16.757875 - Time:188.41917300224304\n",
      "Epoch: 1 - Batch: 85952 - Loss: 16.790033 - Time:197.82997965812683\n",
      "Epoch: 1 - Batch: 90048 - Loss: 16.776657 - Time:207.2400188446045\n",
      "Epoch: 1 - Batch: 94144 - Loss: 16.750359 - Time:216.65193128585815\n",
      "Epoch: 1 - Batch: 98240 - Loss: 16.853065 - Time:226.0891890525818\n",
      "Epoch: 1 - Batch: 102336 - Loss: 16.633507 - Time:235.50085139274597\n",
      "Epoch: 1 - Batch: 106432 - Loss: 16.738415 - Time:244.91284561157227\n",
      "Epoch: 1 - Batch: 110528 - Loss: 16.821526 - Time:254.33934020996094\n",
      "Epoch: 1 - Batch: 114624 - Loss: 16.791918 - Time:263.75001525878906\n",
      "Epoch: 1 - Batch: 118720 - Loss: 16.905678 - Time:273.1615889072418\n",
      "Epoch: 1 - Batch: 122816 - Loss: 16.634811 - Time:282.5730209350586\n",
      "Epoch: 1 - Batch: 126912 - Loss: 16.820127 - Time:291.9835169315338\n",
      "Epoch: 1 - Batch: 131008 - Loss: 16.807549 - Time:301.3947012424469\n",
      "Epoch: 1 - Batch: 135104 - Loss: 16.689756 - Time:310.805917263031\n",
      "Epoch: 1 - Batch: 139200 - Loss: 16.750109 - Time:320.2204911708832\n",
      "Epoch: 1 - Batch: 143296 - Loss: 16.591681 - Time:329.65038084983826\n",
      "Epoch: 1 - Batch: 147392 - Loss: 16.518400 - Time:339.0614857673645\n",
      "Epoch: 1 - Batch: 151488 - Loss: 16.698671 - Time:348.4722135066986\n",
      "Epoch: 2 - Batch: 4032 - Loss: 16.521015 - Time:9.549445867538452\n",
      "Epoch: 2 - Batch: 8128 - Loss: 16.622980 - Time:18.962687969207764\n",
      "Epoch: 2 - Batch: 12224 - Loss: 16.629631 - Time:28.399408102035522\n",
      "Epoch: 2 - Batch: 16320 - Loss: 16.601889 - Time:37.80959129333496\n",
      "Epoch: 2 - Batch: 20416 - Loss: 16.664482 - Time:47.22148299217224\n",
      "Epoch: 2 - Batch: 24512 - Loss: 16.700899 - Time:56.633116006851196\n",
      "Epoch: 2 - Batch: 28608 - Loss: 16.480865 - Time:66.04126858711243\n",
      "Epoch: 2 - Batch: 32704 - Loss: 16.803738 - Time:75.46633291244507\n",
      "Epoch: 2 - Batch: 36800 - Loss: 16.715315 - Time:84.87631702423096\n",
      "Epoch: 2 - Batch: 40896 - Loss: 16.789701 - Time:94.28676891326904\n",
      "Epoch: 2 - Batch: 44992 - Loss: 16.607584 - Time:103.71435809135437\n",
      "Epoch: 2 - Batch: 49088 - Loss: 16.805077 - Time:113.12521934509277\n",
      "Epoch: 2 - Batch: 53184 - Loss: 16.724735 - Time:122.53514194488525\n",
      "Epoch: 2 - Batch: 57280 - Loss: 16.596111 - Time:131.94565200805664\n",
      "Epoch: 2 - Batch: 61376 - Loss: 16.665127 - Time:141.35656785964966\n",
      "Epoch: 2 - Batch: 65472 - Loss: 16.776888 - Time:150.7702624797821\n",
      "Epoch: 2 - Batch: 69568 - Loss: 16.723627 - Time:160.18245196342468\n",
      "Epoch: 2 - Batch: 73664 - Loss: 16.606754 - Time:169.5903193950653\n",
      "Epoch: 2 - Batch: 77760 - Loss: 16.625198 - Time:179.0161154270172\n",
      "Epoch: 2 - Batch: 81856 - Loss: 16.469540 - Time:188.4247121810913\n",
      "Epoch: 2 - Batch: 85952 - Loss: 16.581829 - Time:197.83413243293762\n",
      "Epoch: 2 - Batch: 90048 - Loss: 16.631100 - Time:207.24353432655334\n",
      "Epoch: 2 - Batch: 94144 - Loss: 16.405853 - Time:216.66744661331177\n",
      "Epoch: 2 - Batch: 98240 - Loss: 16.738752 - Time:226.07682299613953\n",
      "Epoch: 2 - Batch: 102336 - Loss: 16.470102 - Time:235.4863839149475\n",
      "Epoch: 2 - Batch: 106432 - Loss: 16.531094 - Time:244.91318464279175\n",
      "Epoch: 2 - Batch: 110528 - Loss: 16.623079 - Time:254.3221254348755\n",
      "Epoch: 2 - Batch: 114624 - Loss: 16.690985 - Time:263.7317006587982\n",
      "Epoch: 2 - Batch: 118720 - Loss: 16.565216 - Time:273.14064598083496\n",
      "Epoch: 2 - Batch: 122816 - Loss: 16.405571 - Time:282.5521950721741\n",
      "Epoch: 2 - Batch: 126912 - Loss: 16.770332 - Time:291.9616584777832\n",
      "Epoch: 2 - Batch: 131008 - Loss: 16.502436 - Time:301.37010288238525\n",
      "Epoch: 2 - Batch: 135104 - Loss: 16.475880 - Time:310.7805087566376\n",
      "Epoch: 2 - Batch: 139200 - Loss: 16.620380 - Time:320.20841097831726\n",
      "Epoch: 2 - Batch: 143296 - Loss: 16.607754 - Time:329.61867594718933\n",
      "Epoch: 2 - Batch: 147392 - Loss: 16.292662 - Time:339.03086376190186\n",
      "Epoch: 2 - Batch: 151488 - Loss: 16.372322 - Time:348.4419333934784\n",
      "Epoch: 3 - Batch: 4032 - Loss: 16.268938 - Time:9.591011047363281\n",
      "Epoch: 3 - Batch: 8128 - Loss: 16.395870 - Time:18.9985408782959\n",
      "Epoch: 3 - Batch: 12224 - Loss: 16.507298 - Time:28.405637502670288\n",
      "Epoch: 3 - Batch: 16320 - Loss: 16.612534 - Time:37.81293702125549\n",
      "Epoch: 3 - Batch: 20416 - Loss: 16.255417 - Time:47.23685646057129\n",
      "Epoch: 3 - Batch: 24512 - Loss: 16.745564 - Time:56.64686369895935\n",
      "Epoch: 3 - Batch: 28608 - Loss: 16.387203 - Time:66.05579280853271\n",
      "Epoch: 3 - Batch: 32704 - Loss: 16.382511 - Time:75.48150873184204\n",
      "Epoch: 3 - Batch: 36800 - Loss: 16.446304 - Time:84.89106059074402\n",
      "Epoch: 3 - Batch: 40896 - Loss: 16.646675 - Time:94.30219626426697\n",
      "Epoch: 3 - Batch: 44992 - Loss: 16.295031 - Time:103.71309494972229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 - Batch: 49088 - Loss: 16.228437 - Time:113.12587404251099\n",
      "Epoch: 3 - Batch: 53184 - Loss: 16.443521 - Time:122.53741312026978\n",
      "Epoch: 3 - Batch: 57280 - Loss: 16.375132 - Time:131.95021271705627\n",
      "Epoch: 3 - Batch: 61376 - Loss: 16.511751 - Time:141.36043286323547\n",
      "Epoch: 3 - Batch: 65472 - Loss: 16.559866 - Time:150.7885901927948\n",
      "Epoch: 3 - Batch: 69568 - Loss: 16.570635 - Time:160.19714426994324\n",
      "Epoch: 3 - Batch: 73664 - Loss: 16.254227 - Time:169.60779118537903\n",
      "Epoch: 3 - Batch: 77760 - Loss: 16.324650 - Time:179.0159149169922\n",
      "Epoch: 3 - Batch: 81856 - Loss: 16.513859 - Time:188.43963408470154\n",
      "Epoch: 3 - Batch: 85952 - Loss: 16.278278 - Time:197.84835958480835\n",
      "Epoch: 3 - Batch: 90048 - Loss: 16.223141 - Time:207.25884413719177\n",
      "Epoch: 3 - Batch: 94144 - Loss: 16.265440 - Time:216.68336844444275\n",
      "Epoch: 3 - Batch: 98240 - Loss: 16.212202 - Time:226.09274911880493\n",
      "Epoch: 3 - Batch: 102336 - Loss: 16.285799 - Time:235.50418853759766\n",
      "Epoch: 3 - Batch: 106432 - Loss: 16.292656 - Time:244.9162518978119\n",
      "Epoch: 3 - Batch: 110528 - Loss: 16.305229 - Time:254.329115152359\n",
      "Epoch: 3 - Batch: 114624 - Loss: 16.414646 - Time:263.74095916748047\n",
      "Epoch: 3 - Batch: 118720 - Loss: 16.282084 - Time:273.1523926258087\n",
      "Epoch: 3 - Batch: 122816 - Loss: 16.167578 - Time:282.5649764537811\n",
      "Epoch: 3 - Batch: 126912 - Loss: 16.352287 - Time:291.99517011642456\n",
      "Epoch: 3 - Batch: 131008 - Loss: 16.377060 - Time:301.40522265434265\n",
      "Epoch: 3 - Batch: 135104 - Loss: 16.379927 - Time:310.8135437965393\n",
      "Epoch: 3 - Batch: 139200 - Loss: 16.429033 - Time:320.2240078449249\n",
      "Epoch: 3 - Batch: 143296 - Loss: 16.167105 - Time:329.65029072761536\n",
      "Epoch: 3 - Batch: 147392 - Loss: 15.976040 - Time:339.05942821502686\n",
      "Epoch: 3 - Batch: 151488 - Loss: 16.336561 - Time:348.4694821834564\n",
      "Epoch: 4 - Batch: 4032 - Loss: 16.308359 - Time:9.559293508529663\n",
      "Epoch: 4 - Batch: 8128 - Loss: 15.804334 - Time:18.96994709968567\n",
      "Epoch: 4 - Batch: 12224 - Loss: 16.324905 - Time:28.380260705947876\n",
      "Epoch: 4 - Batch: 16320 - Loss: 16.123486 - Time:37.78952169418335\n",
      "Epoch: 4 - Batch: 20416 - Loss: 16.126896 - Time:47.19933581352234\n",
      "Epoch: 4 - Batch: 24512 - Loss: 16.126791 - Time:56.609806537628174\n",
      "Epoch: 4 - Batch: 28608 - Loss: 16.033203 - Time:66.01990127563477\n",
      "Epoch: 4 - Batch: 32704 - Loss: 16.208971 - Time:75.42959761619568\n",
      "Epoch: 4 - Batch: 36800 - Loss: 16.203476 - Time:84.85756349563599\n",
      "Epoch: 4 - Batch: 40896 - Loss: 15.999803 - Time:94.26934146881104\n",
      "Epoch: 4 - Batch: 44992 - Loss: 16.017456 - Time:103.67856454849243\n",
      "Epoch: 4 - Batch: 49088 - Loss: 16.008633 - Time:113.08863592147827\n",
      "Epoch: 4 - Batch: 53184 - Loss: 16.065369 - Time:122.51586747169495\n",
      "Epoch: 4 - Batch: 57280 - Loss: 16.248924 - Time:131.92617774009705\n",
      "Epoch: 4 - Batch: 61376 - Loss: 16.334375 - Time:141.33687710762024\n",
      "Epoch: 4 - Batch: 65472 - Loss: 16.077957 - Time:150.76366782188416\n",
      "Epoch: 4 - Batch: 69568 - Loss: 16.162977 - Time:160.17343473434448\n",
      "Epoch: 4 - Batch: 73664 - Loss: 16.025311 - Time:169.58545184135437\n",
      "Epoch: 4 - Batch: 77760 - Loss: 16.227680 - Time:178.99660181999207\n",
      "Epoch: 4 - Batch: 81856 - Loss: 16.292820 - Time:188.40690994262695\n",
      "Epoch: 4 - Batch: 85952 - Loss: 15.834790 - Time:197.81774616241455\n",
      "Epoch: 4 - Batch: 90048 - Loss: 15.996877 - Time:207.22979283332825\n",
      "Epoch: 4 - Batch: 94144 - Loss: 15.887818 - Time:216.6423921585083\n",
      "Epoch: 4 - Batch: 98240 - Loss: 15.700039 - Time:226.06869459152222\n",
      "Epoch: 4 - Batch: 102336 - Loss: 15.979560 - Time:235.47910165786743\n",
      "Epoch: 4 - Batch: 106432 - Loss: 16.294920 - Time:244.88767623901367\n",
      "Epoch: 4 - Batch: 110528 - Loss: 15.818561 - Time:254.30207204818726\n",
      "Epoch: 4 - Batch: 114624 - Loss: 16.388609 - Time:263.7278542518616\n",
      "Epoch: 4 - Batch: 118720 - Loss: 15.544532 - Time:273.1374132633209\n",
      "Epoch: 4 - Batch: 122816 - Loss: 15.533503 - Time:282.5457034111023\n",
      "Epoch: 4 - Batch: 126912 - Loss: 15.679521 - Time:291.9737801551819\n",
      "Epoch: 4 - Batch: 131008 - Loss: 15.816845 - Time:301.38295888900757\n",
      "Epoch: 4 - Batch: 135104 - Loss: 15.697195 - Time:310.79398941993713\n",
      "Epoch: 4 - Batch: 139200 - Loss: 16.274195 - Time:320.20620560646057\n",
      "Epoch: 4 - Batch: 143296 - Loss: 16.103333 - Time:329.6218948364258\n",
      "Epoch: 4 - Batch: 147392 - Loss: 15.953706 - Time:339.0359570980072\n",
      "Epoch: 4 - Batch: 151488 - Loss: 15.728336 - Time:348.4484567642212\n",
      "Epoch: 5 - Batch: 4032 - Loss: 15.570523 - Time:9.57337999343872\n",
      "Epoch: 5 - Batch: 8128 - Loss: 15.807769 - Time:18.98447561264038\n",
      "Epoch: 5 - Batch: 12224 - Loss: 15.947597 - Time:28.39562678337097\n",
      "Epoch: 5 - Batch: 16320 - Loss: 15.452028 - Time:37.804890394210815\n",
      "Epoch: 5 - Batch: 20416 - Loss: 15.774717 - Time:47.21597242355347\n",
      "Epoch: 5 - Batch: 24512 - Loss: 15.447309 - Time:56.62824296951294\n",
      "Epoch: 5 - Batch: 28608 - Loss: 15.644396 - Time:66.04041576385498\n",
      "Epoch: 5 - Batch: 32704 - Loss: 15.874622 - Time:75.4517080783844\n",
      "Epoch: 5 - Batch: 36800 - Loss: 15.482792 - Time:84.87946224212646\n",
      "Epoch: 5 - Batch: 40896 - Loss: 15.802800 - Time:94.28985738754272\n",
      "Epoch: 5 - Batch: 44992 - Loss: 15.658578 - Time:103.7019670009613\n",
      "Epoch: 5 - Batch: 49088 - Loss: 15.549504 - Time:113.11309766769409\n",
      "Epoch: 5 - Batch: 53184 - Loss: 15.559984 - Time:122.54050779342651\n",
      "Epoch: 5 - Batch: 57280 - Loss: 15.360154 - Time:131.9513430595398\n",
      "Epoch: 5 - Batch: 61376 - Loss: 15.530333 - Time:141.3615744113922\n",
      "Epoch: 5 - Batch: 65472 - Loss: 15.683584 - Time:150.79084467887878\n",
      "Epoch: 5 - Batch: 69568 - Loss: 15.612948 - Time:160.20272016525269\n",
      "Epoch: 5 - Batch: 73664 - Loss: 15.535909 - Time:169.6153862476349\n",
      "Epoch: 5 - Batch: 77760 - Loss: 15.255049 - Time:179.02973079681396\n",
      "Epoch: 5 - Batch: 81856 - Loss: 15.303574 - Time:188.44238352775574\n",
      "Epoch: 5 - Batch: 85952 - Loss: 15.359809 - Time:197.8531937599182\n",
      "Epoch: 5 - Batch: 90048 - Loss: 15.498984 - Time:207.2641053199768\n",
      "Epoch: 5 - Batch: 94144 - Loss: 15.240075 - Time:216.67559671401978\n",
      "Epoch: 5 - Batch: 98240 - Loss: 15.729110 - Time:226.1063892841339\n",
      "Epoch: 5 - Batch: 102336 - Loss: 15.361473 - Time:235.51874256134033\n",
      "Epoch: 5 - Batch: 106432 - Loss: 15.311909 - Time:244.9315586090088\n",
      "Epoch: 5 - Batch: 110528 - Loss: 15.465273 - Time:254.34033799171448\n",
      "Epoch: 5 - Batch: 114624 - Loss: 15.139619 - Time:263.769469499588\n",
      "Epoch: 5 - Batch: 118720 - Loss: 15.186863 - Time:273.18035316467285\n",
      "Epoch: 5 - Batch: 122816 - Loss: 15.171698 - Time:282.59303855895996\n",
      "Epoch: 5 - Batch: 126912 - Loss: 15.164221 - Time:292.02226662635803\n",
      "Epoch: 5 - Batch: 131008 - Loss: 15.480888 - Time:301.4335722923279\n",
      "Epoch: 5 - Batch: 135104 - Loss: 15.366906 - Time:310.84410786628723\n",
      "Epoch: 5 - Batch: 139200 - Loss: 15.412243 - Time:320.2572298049927\n",
      "Epoch: 5 - Batch: 143296 - Loss: 15.234295 - Time:329.6686017513275\n",
      "Epoch: 5 - Batch: 147392 - Loss: 14.941333 - Time:339.0797679424286\n",
      "Epoch: 5 - Batch: 151488 - Loss: 15.453379 - Time:348.4911024570465\n",
      "Epoch: 6 - Batch: 4032 - Loss: 15.251360 - Time:9.567965269088745\n",
      "Epoch: 6 - Batch: 8128 - Loss: 14.978740 - Time:18.97792363166809\n",
      "Epoch: 6 - Batch: 12224 - Loss: 14.678406 - Time:28.387468099594116\n",
      "Epoch: 6 - Batch: 16320 - Loss: 15.192090 - Time:37.79479956626892\n",
      "Epoch: 6 - Batch: 20416 - Loss: 14.807419 - Time:47.20425248146057\n",
      "Epoch: 6 - Batch: 24512 - Loss: 14.901611 - Time:56.617412090301514\n",
      "Epoch: 6 - Batch: 28608 - Loss: 14.745744 - Time:66.03111338615417\n",
      "Epoch: 6 - Batch: 32704 - Loss: 15.302021 - Time:75.44371247291565\n",
      "Epoch: 6 - Batch: 36800 - Loss: 15.300970 - Time:84.86981987953186\n",
      "Epoch: 6 - Batch: 40896 - Loss: 14.706778 - Time:94.28191351890564\n",
      "Epoch: 6 - Batch: 44992 - Loss: 15.219746 - Time:103.69021034240723\n",
      "Epoch: 6 - Batch: 49088 - Loss: 14.864781 - Time:113.09920310974121\n",
      "Epoch: 6 - Batch: 53184 - Loss: 14.460058 - Time:122.52881264686584\n",
      "Epoch: 6 - Batch: 57280 - Loss: 15.121785 - Time:131.93816828727722\n",
      "Epoch: 6 - Batch: 61376 - Loss: 14.926640 - Time:141.3468496799469\n",
      "Epoch: 6 - Batch: 65472 - Loss: 14.823814 - Time:150.77434492111206\n",
      "Epoch: 6 - Batch: 69568 - Loss: 14.697511 - Time:160.1860854625702\n",
      "Epoch: 6 - Batch: 73664 - Loss: 14.919886 - Time:169.5970103740692\n",
      "Epoch: 6 - Batch: 77760 - Loss: 14.960526 - Time:179.0090057849884\n",
      "Epoch: 6 - Batch: 81856 - Loss: 14.507474 - Time:188.41922545433044\n",
      "Epoch: 6 - Batch: 85952 - Loss: 14.986257 - Time:197.83103132247925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 - Batch: 90048 - Loss: 14.249108 - Time:207.23904633522034\n",
      "Epoch: 6 - Batch: 94144 - Loss: 14.506726 - Time:216.65092945098877\n",
      "Epoch: 6 - Batch: 98240 - Loss: 14.450743 - Time:226.0792200565338\n",
      "Epoch: 6 - Batch: 102336 - Loss: 14.549614 - Time:235.49167466163635\n",
      "Epoch: 6 - Batch: 106432 - Loss: 14.457609 - Time:244.90367937088013\n",
      "Epoch: 6 - Batch: 110528 - Loss: 14.407684 - Time:254.31457328796387\n",
      "Epoch: 6 - Batch: 114624 - Loss: 14.694676 - Time:263.72549867630005\n",
      "Epoch: 6 - Batch: 118720 - Loss: 14.511046 - Time:273.15525579452515\n",
      "Epoch: 6 - Batch: 122816 - Loss: 14.277832 - Time:282.56654143333435\n",
      "Epoch: 6 - Batch: 126912 - Loss: 14.130533 - Time:291.99487042427063\n",
      "Epoch: 6 - Batch: 131008 - Loss: 15.130928 - Time:301.40721344947815\n",
      "Epoch: 6 - Batch: 135104 - Loss: 14.654873 - Time:310.81749272346497\n",
      "Epoch: 6 - Batch: 139200 - Loss: 14.351925 - Time:320.22706389427185\n",
      "Epoch: 6 - Batch: 143296 - Loss: 14.505547 - Time:329.63616728782654\n",
      "Epoch: 6 - Batch: 147392 - Loss: 13.910422 - Time:339.0453631877899\n",
      "Epoch: 6 - Batch: 151488 - Loss: 14.691704 - Time:348.4532811641693\n",
      "Epoch: 7 - Batch: 4032 - Loss: 13.497148 - Time:9.565186977386475\n",
      "Epoch: 7 - Batch: 8128 - Loss: 14.091841 - Time:18.974074602127075\n",
      "Epoch: 7 - Batch: 12224 - Loss: 13.999417 - Time:28.383352041244507\n",
      "Epoch: 7 - Batch: 16320 - Loss: 14.009552 - Time:37.79174518585205\n",
      "Epoch: 7 - Batch: 20416 - Loss: 13.993279 - Time:47.19926977157593\n",
      "Epoch: 7 - Batch: 24512 - Loss: 14.052478 - Time:56.60823106765747\n",
      "Epoch: 7 - Batch: 28608 - Loss: 14.169666 - Time:66.01845479011536\n",
      "Epoch: 7 - Batch: 32704 - Loss: 13.070241 - Time:75.42618060112\n",
      "Epoch: 7 - Batch: 36800 - Loss: 13.705900 - Time:84.85227298736572\n",
      "Epoch: 7 - Batch: 40896 - Loss: 14.067148 - Time:94.2637357711792\n",
      "Epoch: 7 - Batch: 44992 - Loss: 13.892571 - Time:103.67326998710632\n",
      "Epoch: 7 - Batch: 49088 - Loss: 13.996545 - Time:113.08256769180298\n",
      "Epoch: 7 - Batch: 53184 - Loss: 13.501321 - Time:122.51011395454407\n",
      "Epoch: 7 - Batch: 57280 - Loss: 12.662323 - Time:131.91903853416443\n",
      "Epoch: 7 - Batch: 61376 - Loss: 13.305064 - Time:141.32686591148376\n",
      "Epoch: 7 - Batch: 65472 - Loss: 13.113040 - Time:150.75206398963928\n",
      "Epoch: 7 - Batch: 69568 - Loss: 13.120183 - Time:160.15890789031982\n",
      "Epoch: 7 - Batch: 73664 - Loss: 13.706841 - Time:169.5681529045105\n",
      "Epoch: 7 - Batch: 77760 - Loss: 12.965201 - Time:178.9766228199005\n",
      "Epoch: 7 - Batch: 81856 - Loss: 13.424739 - Time:188.38889503479004\n",
      "Epoch: 7 - Batch: 85952 - Loss: 13.048415 - Time:197.79900407791138\n",
      "Epoch: 7 - Batch: 90048 - Loss: 13.176849 - Time:207.2083444595337\n",
      "Epoch: 7 - Batch: 94144 - Loss: 13.920689 - Time:216.61732721328735\n",
      "Epoch: 7 - Batch: 98240 - Loss: 13.723131 - Time:226.0423572063446\n",
      "Epoch: 7 - Batch: 102336 - Loss: 12.672585 - Time:235.4497947692871\n",
      "Epoch: 7 - Batch: 106432 - Loss: 12.713935 - Time:244.85976243019104\n",
      "Epoch: 7 - Batch: 110528 - Loss: 13.334351 - Time:254.26775336265564\n",
      "Epoch: 7 - Batch: 114624 - Loss: 13.095530 - Time:263.6776511669159\n",
      "Epoch: 7 - Batch: 118720 - Loss: 12.727484 - Time:273.10232496261597\n",
      "Epoch: 7 - Batch: 122816 - Loss: 13.423881 - Time:282.5124297142029\n",
      "Epoch: 7 - Batch: 126912 - Loss: 13.121320 - Time:291.9214174747467\n",
      "Epoch: 7 - Batch: 131008 - Loss: 12.882693 - Time:301.3456733226776\n",
      "Epoch: 7 - Batch: 135104 - Loss: 12.787836 - Time:310.7552318572998\n",
      "Epoch: 7 - Batch: 139200 - Loss: 13.653870 - Time:320.16513204574585\n",
      "Epoch: 7 - Batch: 143296 - Loss: 13.003379 - Time:329.57404685020447\n",
      "Epoch: 7 - Batch: 147392 - Loss: 12.536844 - Time:338.98291516304016\n",
      "Epoch: 7 - Batch: 151488 - Loss: 12.855079 - Time:348.3923213481903\n",
      "Epoch: 8 - Batch: 4032 - Loss: 11.612139 - Time:9.528148174285889\n",
      "Epoch: 8 - Batch: 8128 - Loss: 11.629565 - Time:18.96710515022278\n",
      "Epoch: 8 - Batch: 12224 - Loss: 12.244227 - Time:28.380439519882202\n",
      "Epoch: 8 - Batch: 16320 - Loss: 11.647395 - Time:37.791871070861816\n",
      "Epoch: 8 - Batch: 20416 - Loss: 12.060266 - Time:47.20407962799072\n",
      "Epoch: 8 - Batch: 24512 - Loss: 11.786281 - Time:56.63236117362976\n",
      "Epoch: 8 - Batch: 28608 - Loss: 10.462915 - Time:66.04287385940552\n",
      "Epoch: 8 - Batch: 32704 - Loss: 11.566676 - Time:75.45283246040344\n",
      "Epoch: 8 - Batch: 36800 - Loss: 12.703056 - Time:84.87919855117798\n",
      "Epoch: 8 - Batch: 40896 - Loss: 12.605148 - Time:94.29039645195007\n",
      "Epoch: 8 - Batch: 44992 - Loss: 11.008565 - Time:103.70016551017761\n",
      "Epoch: 8 - Batch: 49088 - Loss: 11.071405 - Time:113.10906052589417\n",
      "Epoch: 8 - Batch: 53184 - Loss: 11.902163 - Time:122.52121090888977\n",
      "Epoch: 8 - Batch: 57280 - Loss: 11.949730 - Time:131.93301510810852\n",
      "Epoch: 8 - Batch: 61376 - Loss: 11.958727 - Time:141.3462564945221\n",
      "Epoch: 8 - Batch: 65472 - Loss: 12.603421 - Time:150.75539302825928\n",
      "Epoch: 8 - Batch: 69568 - Loss: 12.090172 - Time:160.18424272537231\n",
      "Epoch: 8 - Batch: 73664 - Loss: 12.699478 - Time:169.59424138069153\n",
      "Epoch: 8 - Batch: 77760 - Loss: 11.318921 - Time:179.00316882133484\n",
      "Epoch: 8 - Batch: 81856 - Loss: 11.340037 - Time:188.4091351032257\n",
      "Epoch: 8 - Batch: 85952 - Loss: 11.103498 - Time:197.83637404441833\n",
      "Epoch: 8 - Batch: 90048 - Loss: 10.682912 - Time:207.24690413475037\n",
      "Epoch: 8 - Batch: 94144 - Loss: 11.888000 - Time:216.65809798240662\n",
      "Epoch: 8 - Batch: 98240 - Loss: 12.809903 - Time:226.08471083641052\n",
      "Epoch: 8 - Batch: 102336 - Loss: 10.508960 - Time:235.49463891983032\n",
      "Epoch: 8 - Batch: 106432 - Loss: 11.681972 - Time:244.9060046672821\n",
      "Epoch: 8 - Batch: 110528 - Loss: 11.159575 - Time:254.31686806678772\n",
      "Epoch: 8 - Batch: 114624 - Loss: 10.888249 - Time:263.7268786430359\n",
      "Epoch: 8 - Batch: 118720 - Loss: 11.318743 - Time:273.13747811317444\n",
      "Epoch: 8 - Batch: 122816 - Loss: 11.728102 - Time:282.5481073856354\n",
      "Epoch: 8 - Batch: 126912 - Loss: 11.301912 - Time:291.95776772499084\n",
      "Epoch: 8 - Batch: 131008 - Loss: 12.233004 - Time:301.3842535018921\n",
      "Epoch: 8 - Batch: 135104 - Loss: 10.949520 - Time:310.79483699798584\n",
      "Epoch: 8 - Batch: 139200 - Loss: 10.928080 - Time:320.2037127017975\n",
      "Epoch: 8 - Batch: 143296 - Loss: 11.785664 - Time:329.61168789863586\n",
      "Epoch: 8 - Batch: 147392 - Loss: 11.120612 - Time:339.01932430267334\n",
      "Epoch: 8 - Batch: 151488 - Loss: 10.642715 - Time:348.444326877594\n",
      "Epoch: 9 - Batch: 4032 - Loss: 11.454718 - Time:9.54381251335144\n",
      "Epoch: 9 - Batch: 8128 - Loss: 9.556245 - Time:18.95316505432129\n",
      "Epoch: 9 - Batch: 12224 - Loss: 9.560630 - Time:28.390604972839355\n",
      "Epoch: 9 - Batch: 16320 - Loss: 10.707474 - Time:37.799424171447754\n",
      "Epoch: 9 - Batch: 20416 - Loss: 10.657896 - Time:47.22638821601868\n",
      "Epoch: 9 - Batch: 24512 - Loss: 10.853752 - Time:56.63877058029175\n",
      "Epoch: 9 - Batch: 28608 - Loss: 9.035599 - Time:66.04717969894409\n",
      "Epoch: 9 - Batch: 32704 - Loss: 10.300935 - Time:75.45901465415955\n",
      "Epoch: 9 - Batch: 36800 - Loss: 10.833877 - Time:84.86996865272522\n",
      "Epoch: 9 - Batch: 40896 - Loss: 9.857444 - Time:94.28191351890564\n",
      "Epoch: 9 - Batch: 44992 - Loss: 9.101151 - Time:103.6938648223877\n",
      "Epoch: 9 - Batch: 49088 - Loss: 10.170341 - Time:113.10684847831726\n",
      "Epoch: 9 - Batch: 53184 - Loss: 10.837417 - Time:122.51783227920532\n",
      "Epoch: 9 - Batch: 57280 - Loss: 10.008067 - Time:131.94428253173828\n",
      "Epoch: 9 - Batch: 61376 - Loss: 9.313696 - Time:141.3545627593994\n",
      "Epoch: 9 - Batch: 65472 - Loss: 10.388659 - Time:150.76345014572144\n",
      "Epoch: 9 - Batch: 69568 - Loss: 11.093895 - Time:160.17239665985107\n",
      "Epoch: 9 - Batch: 73664 - Loss: 10.128627 - Time:169.59769988059998\n",
      "Epoch: 9 - Batch: 77760 - Loss: 9.056216 - Time:179.00576949119568\n",
      "Epoch: 9 - Batch: 81856 - Loss: 10.280422 - Time:188.4130825996399\n",
      "Epoch: 9 - Batch: 85952 - Loss: 10.117023 - Time:197.8413531780243\n",
      "Epoch: 9 - Batch: 90048 - Loss: 9.956248 - Time:207.25267243385315\n",
      "Epoch: 9 - Batch: 94144 - Loss: 9.509774 - Time:216.66312432289124\n",
      "Epoch: 9 - Batch: 98240 - Loss: 10.355311 - Time:226.07338047027588\n",
      "Epoch: 9 - Batch: 102336 - Loss: 10.033689 - Time:235.483229637146\n",
      "Epoch: 9 - Batch: 106432 - Loss: 9.425187 - Time:244.89542603492737\n",
      "Epoch: 9 - Batch: 110528 - Loss: 9.821938 - Time:254.30549907684326\n",
      "Epoch: 9 - Batch: 114624 - Loss: 10.613202 - Time:263.7157943248749\n",
      "Epoch: 9 - Batch: 118720 - Loss: 8.362343 - Time:273.1443901062012\n",
      "Epoch: 9 - Batch: 122816 - Loss: 9.343511 - Time:282.5559456348419\n",
      "Epoch: 9 - Batch: 126912 - Loss: 10.085979 - Time:291.96759009361267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 - Batch: 131008 - Loss: 9.877003 - Time:301.37659645080566\n",
      "Epoch: 9 - Batch: 135104 - Loss: 9.013651 - Time:310.8027002811432\n",
      "Epoch: 9 - Batch: 139200 - Loss: 9.868595 - Time:320.21410369873047\n",
      "Epoch: 9 - Batch: 143296 - Loss: 8.680666 - Time:329.6259038448334\n",
      "Epoch: 9 - Batch: 147392 - Loss: 8.013715 - Time:339.0517601966858\n",
      "Epoch: 9 - Batch: 151488 - Loss: 9.716344 - Time:348.4626638889313\n",
      "Epoch: 10 - Batch: 4032 - Loss: 7.584451 - Time:9.55242109298706\n",
      "Epoch: 10 - Batch: 8128 - Loss: 8.783316 - Time:18.96183133125305\n",
      "Epoch: 10 - Batch: 12224 - Loss: 8.085077 - Time:28.397619009017944\n",
      "Epoch: 10 - Batch: 16320 - Loss: 7.589932 - Time:37.80693554878235\n",
      "Epoch: 10 - Batch: 20416 - Loss: 8.757816 - Time:47.215375900268555\n",
      "Epoch: 10 - Batch: 24512 - Loss: 8.867952 - Time:56.62265992164612\n",
      "Epoch: 10 - Batch: 28608 - Loss: 7.653225 - Time:66.03135299682617\n",
      "Epoch: 10 - Batch: 32704 - Loss: 7.712182 - Time:75.4392340183258\n",
      "Epoch: 10 - Batch: 36800 - Loss: 7.784197 - Time:84.84921360015869\n",
      "Epoch: 10 - Batch: 40896 - Loss: 7.965765 - Time:94.25691199302673\n",
      "Epoch: 10 - Batch: 44992 - Loss: 8.003157 - Time:103.67932796478271\n",
      "Epoch: 10 - Batch: 49088 - Loss: 8.724472 - Time:113.08784317970276\n",
      "Epoch: 10 - Batch: 53184 - Loss: 6.571264 - Time:122.496999502182\n",
      "Epoch: 10 - Batch: 57280 - Loss: 8.580990 - Time:131.9063754081726\n",
      "Epoch: 10 - Batch: 61376 - Loss: 7.740213 - Time:141.3307032585144\n",
      "Epoch: 10 - Batch: 65472 - Loss: 7.679572 - Time:150.7377245426178\n",
      "Epoch: 10 - Batch: 69568 - Loss: 8.254532 - Time:160.14680814743042\n",
      "Epoch: 10 - Batch: 73664 - Loss: 7.841083 - Time:169.57124853134155\n",
      "Epoch: 10 - Batch: 77760 - Loss: 7.711831 - Time:178.97842121124268\n",
      "Epoch: 10 - Batch: 81856 - Loss: 6.865317 - Time:188.38638186454773\n",
      "Epoch: 10 - Batch: 85952 - Loss: 9.570259 - Time:197.7947986125946\n",
      "Epoch: 10 - Batch: 90048 - Loss: 8.437948 - Time:207.20331835746765\n",
      "Epoch: 10 - Batch: 94144 - Loss: 8.932540 - Time:216.61304783821106\n",
      "Epoch: 10 - Batch: 98240 - Loss: 7.810851 - Time:226.02215957641602\n",
      "Epoch: 10 - Batch: 102336 - Loss: 8.164234 - Time:235.4297332763672\n",
      "Epoch: 10 - Batch: 106432 - Loss: 8.403643 - Time:244.85226941108704\n",
      "Epoch: 10 - Batch: 110528 - Loss: 8.409191 - Time:254.2605423927307\n",
      "Epoch: 10 - Batch: 114624 - Loss: 7.658027 - Time:263.6679790019989\n",
      "Epoch: 10 - Batch: 118720 - Loss: 7.955044 - Time:273.0759778022766\n",
      "Epoch: 10 - Batch: 122816 - Loss: 7.974264 - Time:282.50106382369995\n",
      "Epoch: 10 - Batch: 126912 - Loss: 7.229003 - Time:291.9097046852112\n",
      "Epoch: 10 - Batch: 131008 - Loss: 7.668133 - Time:301.3178744316101\n",
      "Epoch: 10 - Batch: 135104 - Loss: 8.583389 - Time:310.7440857887268\n",
      "Epoch: 10 - Batch: 139200 - Loss: 7.345673 - Time:320.15514159202576\n",
      "Epoch: 10 - Batch: 143296 - Loss: 9.017450 - Time:329.5651960372925\n",
      "Epoch: 10 - Batch: 147392 - Loss: 9.141070 - Time:338.9749674797058\n",
      "Epoch: 10 - Batch: 151488 - Loss: 6.842155 - Time:348.3831079006195\n",
      "Epoch: 11 - Batch: 4032 - Loss: 7.323008 - Time:9.531261920928955\n",
      "Epoch: 11 - Batch: 8128 - Loss: 7.443671 - Time:18.93923544883728\n",
      "Epoch: 11 - Batch: 12224 - Loss: 6.806040 - Time:28.37456464767456\n",
      "Epoch: 11 - Batch: 16320 - Loss: 6.023457 - Time:37.78184366226196\n",
      "Epoch: 11 - Batch: 20416 - Loss: 5.742382 - Time:47.190601110458374\n",
      "Epoch: 11 - Batch: 24512 - Loss: 6.272914 - Time:56.59928488731384\n",
      "Epoch: 11 - Batch: 28608 - Loss: 7.283392 - Time:66.00647306442261\n",
      "Epoch: 11 - Batch: 32704 - Loss: 7.572504 - Time:75.4316394329071\n",
      "Epoch: 11 - Batch: 36800 - Loss: 6.158218 - Time:84.8419463634491\n",
      "Epoch: 11 - Batch: 40896 - Loss: 5.227797 - Time:94.2554862499237\n",
      "Epoch: 11 - Batch: 44992 - Loss: 6.769517 - Time:103.68408608436584\n",
      "Epoch: 11 - Batch: 49088 - Loss: 5.871123 - Time:113.0930507183075\n",
      "Epoch: 11 - Batch: 53184 - Loss: 6.413403 - Time:122.50245594978333\n",
      "Epoch: 11 - Batch: 57280 - Loss: 6.558392 - Time:131.91222500801086\n",
      "Epoch: 11 - Batch: 61376 - Loss: 5.807123 - Time:141.321763753891\n",
      "Epoch: 11 - Batch: 65472 - Loss: 5.843009 - Time:150.73132705688477\n",
      "Epoch: 11 - Batch: 69568 - Loss: 6.078711 - Time:160.14325952529907\n",
      "Epoch: 11 - Batch: 73664 - Loss: 7.254375 - Time:169.55301022529602\n",
      "Epoch: 11 - Batch: 77760 - Loss: 6.464431 - Time:178.98044395446777\n",
      "Epoch: 11 - Batch: 81856 - Loss: 5.985874 - Time:188.39026641845703\n",
      "Epoch: 11 - Batch: 85952 - Loss: 6.896096 - Time:197.80297183990479\n",
      "Epoch: 11 - Batch: 90048 - Loss: 6.757484 - Time:207.21401953697205\n",
      "Epoch: 11 - Batch: 94144 - Loss: 7.767648 - Time:216.639230966568\n",
      "Epoch: 11 - Batch: 98240 - Loss: 6.072080 - Time:226.04759168624878\n",
      "Epoch: 11 - Batch: 102336 - Loss: 6.036351 - Time:235.4551546573639\n",
      "Epoch: 11 - Batch: 106432 - Loss: 6.923758 - Time:244.880952835083\n",
      "Epoch: 11 - Batch: 110528 - Loss: 5.340895 - Time:254.2909927368164\n",
      "Epoch: 11 - Batch: 114624 - Loss: 5.311947 - Time:263.7021498680115\n",
      "Epoch: 11 - Batch: 118720 - Loss: 7.175501 - Time:273.1111478805542\n",
      "Epoch: 11 - Batch: 122816 - Loss: 6.162618 - Time:282.52226853370667\n",
      "Epoch: 11 - Batch: 126912 - Loss: 6.231845 - Time:291.9339301586151\n",
      "Epoch: 11 - Batch: 131008 - Loss: 6.281929 - Time:301.34895515441895\n",
      "Epoch: 11 - Batch: 135104 - Loss: 6.085104 - Time:310.7615237236023\n",
      "Epoch: 11 - Batch: 139200 - Loss: 5.978136 - Time:320.18686056137085\n",
      "Epoch: 11 - Batch: 143296 - Loss: 6.786495 - Time:329.5966396331787\n",
      "Epoch: 11 - Batch: 147392 - Loss: 6.878519 - Time:339.00617933273315\n",
      "Epoch: 11 - Batch: 151488 - Loss: 6.676981 - Time:348.41655707359314\n",
      "Epoch: 12 - Batch: 4032 - Loss: 4.615252 - Time:9.561003923416138\n",
      "Epoch: 12 - Batch: 8128 - Loss: 4.378784 - Time:18.971054792404175\n",
      "Epoch: 12 - Batch: 12224 - Loss: 4.329146 - Time:28.406723260879517\n",
      "Epoch: 12 - Batch: 16320 - Loss: 5.335182 - Time:37.81799030303955\n",
      "Epoch: 12 - Batch: 20416 - Loss: 4.924829 - Time:47.227877616882324\n",
      "Epoch: 12 - Batch: 24512 - Loss: 5.621530 - Time:56.63749933242798\n",
      "Epoch: 12 - Batch: 28608 - Loss: 4.616578 - Time:66.04787039756775\n",
      "Epoch: 12 - Batch: 32704 - Loss: 5.026531 - Time:75.4568943977356\n",
      "Epoch: 12 - Batch: 36800 - Loss: 5.063583 - Time:84.86738157272339\n",
      "Epoch: 12 - Batch: 40896 - Loss: 5.771748 - Time:94.27916288375854\n",
      "Epoch: 12 - Batch: 44992 - Loss: 4.734861 - Time:103.69025731086731\n",
      "Epoch: 12 - Batch: 49088 - Loss: 5.045357 - Time:113.11962413787842\n",
      "Epoch: 12 - Batch: 53184 - Loss: 6.287948 - Time:122.53080630302429\n",
      "Epoch: 12 - Batch: 57280 - Loss: 4.700173 - Time:131.94098091125488\n",
      "Epoch: 12 - Batch: 61376 - Loss: 5.936081 - Time:141.35057377815247\n",
      "Epoch: 12 - Batch: 65472 - Loss: 5.401939 - Time:150.77659845352173\n",
      "Epoch: 12 - Batch: 69568 - Loss: 5.474401 - Time:160.18582558631897\n",
      "Epoch: 12 - Batch: 73664 - Loss: 5.464643 - Time:169.59485149383545\n",
      "Epoch: 12 - Batch: 77760 - Loss: 5.010368 - Time:179.01853728294373\n",
      "Epoch: 12 - Batch: 81856 - Loss: 5.062527 - Time:188.43064665794373\n",
      "Epoch: 12 - Batch: 85952 - Loss: 4.602241 - Time:197.84100317955017\n",
      "Epoch: 12 - Batch: 90048 - Loss: 4.439723 - Time:207.25252389907837\n",
      "Epoch: 12 - Batch: 94144 - Loss: 5.321611 - Time:216.6648952960968\n",
      "Epoch: 12 - Batch: 98240 - Loss: 5.146957 - Time:226.07623624801636\n",
      "Epoch: 12 - Batch: 102336 - Loss: 4.565602 - Time:235.4863564968109\n",
      "Epoch: 12 - Batch: 106432 - Loss: 5.078637 - Time:244.8956277370453\n",
      "Epoch: 12 - Batch: 110528 - Loss: 3.894586 - Time:254.32291197776794\n",
      "Epoch: 12 - Batch: 114624 - Loss: 5.715322 - Time:263.73187041282654\n",
      "Epoch: 12 - Batch: 118720 - Loss: 4.333416 - Time:273.1429693698883\n",
      "Epoch: 12 - Batch: 122816 - Loss: 5.287671 - Time:282.5515282154083\n",
      "Epoch: 12 - Batch: 126912 - Loss: 3.980164 - Time:291.9765474796295\n",
      "Epoch: 12 - Batch: 131008 - Loss: 4.649378 - Time:301.3848226070404\n",
      "Epoch: 12 - Batch: 135104 - Loss: 4.841719 - Time:310.7959830760956\n",
      "Epoch: 12 - Batch: 139200 - Loss: 5.293213 - Time:320.22282791137695\n",
      "Epoch: 12 - Batch: 143296 - Loss: 4.541381 - Time:329.631986618042\n",
      "Epoch: 12 - Batch: 147392 - Loss: 4.986239 - Time:339.04287791252136\n",
      "Epoch: 12 - Batch: 151488 - Loss: 4.260457 - Time:348.45142340660095\n",
      "Epoch: 13 - Batch: 4032 - Loss: 3.428629 - Time:9.54482126235962\n",
      "Epoch: 13 - Batch: 8128 - Loss: 3.786395 - Time:18.983333587646484\n",
      "Epoch: 13 - Batch: 12224 - Loss: 3.913428 - Time:28.396119117736816\n",
      "Epoch: 13 - Batch: 16320 - Loss: 3.760986 - Time:37.806525468826294\n",
      "Epoch: 13 - Batch: 20416 - Loss: 4.938537 - Time:47.21635818481445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 - Batch: 24512 - Loss: 3.114677 - Time:56.626441955566406\n",
      "Epoch: 13 - Batch: 28608 - Loss: 4.232036 - Time:66.03540706634521\n",
      "Epoch: 13 - Batch: 32704 - Loss: 4.658972 - Time:75.44395041465759\n",
      "Epoch: 13 - Batch: 36800 - Loss: 3.092981 - Time:84.85134696960449\n",
      "Epoch: 13 - Batch: 40896 - Loss: 3.480134 - Time:94.27760601043701\n",
      "Epoch: 13 - Batch: 44992 - Loss: 3.727610 - Time:103.68783211708069\n",
      "Epoch: 13 - Batch: 49088 - Loss: 5.852755 - Time:113.09863591194153\n",
      "Epoch: 13 - Batch: 53184 - Loss: 4.714753 - Time:122.50715732574463\n",
      "Epoch: 13 - Batch: 57280 - Loss: 4.253403 - Time:131.9315824508667\n",
      "Epoch: 13 - Batch: 61376 - Loss: 3.412950 - Time:141.34016633033752\n",
      "Epoch: 13 - Batch: 65472 - Loss: 4.700018 - Time:150.75156211853027\n",
      "Epoch: 13 - Batch: 69568 - Loss: 3.980720 - Time:160.17922973632812\n",
      "Epoch: 13 - Batch: 73664 - Loss: 4.286685 - Time:169.59101796150208\n",
      "Epoch: 13 - Batch: 77760 - Loss: 4.097979 - Time:179.0011522769928\n",
      "Epoch: 13 - Batch: 81856 - Loss: 4.162910 - Time:188.41495156288147\n",
      "Epoch: 13 - Batch: 85952 - Loss: 3.245900 - Time:197.826477766037\n",
      "Epoch: 13 - Batch: 90048 - Loss: 3.823740 - Time:207.23816108703613\n",
      "Epoch: 13 - Batch: 94144 - Loss: 4.368057 - Time:216.65046072006226\n",
      "Epoch: 13 - Batch: 98240 - Loss: 3.107940 - Time:226.0613067150116\n",
      "Epoch: 13 - Batch: 102336 - Loss: 4.105445 - Time:235.48928785324097\n",
      "Epoch: 13 - Batch: 106432 - Loss: 4.476739 - Time:244.8984453678131\n",
      "Epoch: 13 - Batch: 110528 - Loss: 3.881909 - Time:254.30893754959106\n",
      "Epoch: 13 - Batch: 114624 - Loss: 5.089380 - Time:263.71956968307495\n",
      "Epoch: 13 - Batch: 118720 - Loss: 4.371452 - Time:273.14766454696655\n",
      "Epoch: 13 - Batch: 122816 - Loss: 4.032093 - Time:282.55682706832886\n",
      "Epoch: 13 - Batch: 126912 - Loss: 5.160186 - Time:291.96830773353577\n",
      "Epoch: 13 - Batch: 131008 - Loss: 3.959589 - Time:301.3936221599579\n",
      "Epoch: 13 - Batch: 135104 - Loss: 5.054620 - Time:310.80537009239197\n",
      "Epoch: 13 - Batch: 139200 - Loss: 4.288563 - Time:320.2149090766907\n",
      "Epoch: 13 - Batch: 143296 - Loss: 4.668918 - Time:329.62851095199585\n",
      "Epoch: 13 - Batch: 147392 - Loss: 4.340034 - Time:339.04065799713135\n",
      "Epoch: 13 - Batch: 151488 - Loss: 4.081263 - Time:348.45100235939026\n",
      "Epoch: 14 - Batch: 4032 - Loss: 2.891393 - Time:9.565999746322632\n",
      "Epoch: 14 - Batch: 8128 - Loss: 3.170825 - Time:18.97586488723755\n",
      "Epoch: 14 - Batch: 12224 - Loss: 2.866940 - Time:28.38437795639038\n",
      "Epoch: 14 - Batch: 16320 - Loss: 3.368012 - Time:37.79331946372986\n",
      "Epoch: 14 - Batch: 20416 - Loss: 3.309692 - Time:47.20295858383179\n",
      "Epoch: 14 - Batch: 24512 - Loss: 3.677120 - Time:56.61186337471008\n",
      "Epoch: 14 - Batch: 28608 - Loss: 4.132993 - Time:66.02037358283997\n",
      "Epoch: 14 - Batch: 32704 - Loss: 3.298745 - Time:75.42998266220093\n",
      "Epoch: 14 - Batch: 36800 - Loss: 3.719828 - Time:84.85600900650024\n",
      "Epoch: 14 - Batch: 40896 - Loss: 2.857357 - Time:94.2663185596466\n",
      "Epoch: 14 - Batch: 44992 - Loss: 3.330695 - Time:103.67733693122864\n",
      "Epoch: 14 - Batch: 49088 - Loss: 3.533193 - Time:113.08732223510742\n",
      "Epoch: 14 - Batch: 53184 - Loss: 2.494751 - Time:122.49662780761719\n",
      "Epoch: 14 - Batch: 57280 - Loss: 3.397029 - Time:131.9233202934265\n",
      "Epoch: 14 - Batch: 61376 - Loss: 2.869038 - Time:141.3333399295807\n",
      "Epoch: 14 - Batch: 65472 - Loss: 3.560504 - Time:150.74263787269592\n",
      "Epoch: 14 - Batch: 69568 - Loss: 3.467058 - Time:160.1684446334839\n",
      "Epoch: 14 - Batch: 73664 - Loss: 3.642579 - Time:169.5774745941162\n",
      "Epoch: 14 - Batch: 77760 - Loss: 3.663444 - Time:178.98818922042847\n",
      "Epoch: 14 - Batch: 81856 - Loss: 3.059537 - Time:188.3971962928772\n",
      "Epoch: 14 - Batch: 85952 - Loss: 4.147224 - Time:197.80830097198486\n",
      "Epoch: 14 - Batch: 90048 - Loss: 3.675057 - Time:207.21928477287292\n",
      "Epoch: 14 - Batch: 94144 - Loss: 3.335821 - Time:216.63167643547058\n",
      "Epoch: 14 - Batch: 98240 - Loss: 2.595591 - Time:226.04232096672058\n",
      "Epoch: 14 - Batch: 102336 - Loss: 3.780735 - Time:235.4681749343872\n",
      "Epoch: 14 - Batch: 106432 - Loss: 3.242272 - Time:244.87543559074402\n",
      "Epoch: 14 - Batch: 110528 - Loss: 4.049005 - Time:254.28419423103333\n",
      "Epoch: 14 - Batch: 114624 - Loss: 4.012683 - Time:263.694580078125\n",
      "Epoch: 14 - Batch: 118720 - Loss: 2.987844 - Time:273.12088227272034\n",
      "Epoch: 14 - Batch: 122816 - Loss: 3.720201 - Time:282.53021335601807\n",
      "Epoch: 14 - Batch: 126912 - Loss: 3.254878 - Time:291.93929529190063\n",
      "Epoch: 14 - Batch: 131008 - Loss: 4.258008 - Time:301.36608386039734\n",
      "Epoch: 14 - Batch: 135104 - Loss: 3.825068 - Time:310.77834606170654\n",
      "Epoch: 14 - Batch: 139200 - Loss: 3.520119 - Time:320.1883714199066\n",
      "Epoch: 14 - Batch: 143296 - Loss: 4.019253 - Time:329.5982012748718\n",
      "Epoch: 14 - Batch: 147392 - Loss: 3.810786 - Time:339.0072588920593\n",
      "Epoch: 14 - Batch: 151488 - Loss: 3.438125 - Time:348.41811180114746\n",
      "Epoch: 15 - Batch: 4032 - Loss: 2.357358 - Time:9.551692485809326\n",
      "Epoch: 15 - Batch: 8128 - Loss: 2.561393 - Time:18.96016025543213\n",
      "Epoch: 15 - Batch: 12224 - Loss: 1.734995 - Time:28.36829447746277\n",
      "Epoch: 15 - Batch: 16320 - Loss: 1.777727 - Time:37.77826642990112\n",
      "Epoch: 15 - Batch: 20416 - Loss: 1.827709 - Time:47.187695264816284\n",
      "Epoch: 15 - Batch: 24512 - Loss: 2.948652 - Time:56.5987286567688\n",
      "Epoch: 15 - Batch: 28608 - Loss: 2.799052 - Time:66.00676941871643\n",
      "Epoch: 15 - Batch: 32704 - Loss: 2.033230 - Time:75.41624927520752\n",
      "Epoch: 15 - Batch: 36800 - Loss: 2.260201 - Time:84.84280562400818\n",
      "Epoch: 15 - Batch: 40896 - Loss: 3.452317 - Time:94.25257992744446\n",
      "Epoch: 15 - Batch: 44992 - Loss: 1.994592 - Time:103.66124391555786\n",
      "Epoch: 15 - Batch: 49088 - Loss: 2.511178 - Time:113.06995749473572\n",
      "Epoch: 15 - Batch: 53184 - Loss: 3.035501 - Time:122.49364280700684\n",
      "Epoch: 15 - Batch: 57280 - Loss: 2.724715 - Time:131.90194511413574\n",
      "Epoch: 15 - Batch: 61376 - Loss: 2.607919 - Time:141.3122901916504\n",
      "Epoch: 15 - Batch: 65472 - Loss: 2.832181 - Time:150.73898792266846\n",
      "Epoch: 15 - Batch: 69568 - Loss: 4.035407 - Time:160.14833807945251\n",
      "Epoch: 15 - Batch: 73664 - Loss: 3.781128 - Time:169.55745029449463\n",
      "Epoch: 15 - Batch: 77760 - Loss: 1.569515 - Time:178.96868419647217\n",
      "Epoch: 15 - Batch: 81856 - Loss: 3.032065 - Time:188.37680196762085\n",
      "Epoch: 15 - Batch: 85952 - Loss: 2.825189 - Time:197.78394055366516\n",
      "Epoch: 15 - Batch: 90048 - Loss: 2.967769 - Time:207.19234943389893\n",
      "Epoch: 15 - Batch: 94144 - Loss: 2.843838 - Time:216.60231590270996\n",
      "Epoch: 15 - Batch: 98240 - Loss: 3.265404 - Time:226.02928113937378\n",
      "Epoch: 15 - Batch: 102336 - Loss: 2.545470 - Time:235.4401776790619\n",
      "Epoch: 15 - Batch: 106432 - Loss: 2.626874 - Time:244.84919500350952\n",
      "Epoch: 15 - Batch: 110528 - Loss: 2.554908 - Time:254.2576198577881\n",
      "Epoch: 15 - Batch: 114624 - Loss: 2.252560 - Time:263.66613602638245\n",
      "Epoch: 15 - Batch: 118720 - Loss: 2.867389 - Time:273.08939361572266\n",
      "Epoch: 15 - Batch: 122816 - Loss: 2.884022 - Time:282.4977903366089\n",
      "Epoch: 15 - Batch: 126912 - Loss: 2.084245 - Time:291.90680027008057\n",
      "Epoch: 15 - Batch: 131008 - Loss: 2.286199 - Time:301.33167147636414\n",
      "Epoch: 15 - Batch: 135104 - Loss: 3.331799 - Time:310.7404873371124\n",
      "Epoch: 15 - Batch: 139200 - Loss: 2.691167 - Time:320.15042328834534\n",
      "Epoch: 15 - Batch: 143296 - Loss: 3.779501 - Time:329.5608720779419\n",
      "Epoch: 15 - Batch: 147392 - Loss: 2.512194 - Time:338.96869468688965\n",
      "Epoch: 15 - Batch: 151488 - Loss: 2.469141 - Time:348.378458738327\n",
      "Epoch: 16 - Batch: 4032 - Loss: 1.115193 - Time:9.554304361343384\n",
      "Epoch: 16 - Batch: 8128 - Loss: 2.123580 - Time:18.99133038520813\n",
      "Epoch: 16 - Batch: 12224 - Loss: 2.188520 - Time:28.400397300720215\n",
      "Epoch: 16 - Batch: 16320 - Loss: 1.526862 - Time:37.81297039985657\n",
      "Epoch: 16 - Batch: 20416 - Loss: 2.574954 - Time:47.22368788719177\n",
      "Epoch: 16 - Batch: 24512 - Loss: 1.689318 - Time:56.649763345718384\n",
      "Epoch: 16 - Batch: 28608 - Loss: 2.055377 - Time:66.05878973007202\n",
      "Epoch: 16 - Batch: 32704 - Loss: 3.073022 - Time:75.47103261947632\n",
      "Epoch: 16 - Batch: 36800 - Loss: 2.287373 - Time:84.90074348449707\n",
      "Epoch: 16 - Batch: 40896 - Loss: 1.171443 - Time:94.3111674785614\n",
      "Epoch: 16 - Batch: 44992 - Loss: 2.262005 - Time:103.72218108177185\n",
      "Epoch: 16 - Batch: 49088 - Loss: 2.002619 - Time:113.13356947898865\n",
      "Epoch: 16 - Batch: 53184 - Loss: 2.558317 - Time:122.54384326934814\n",
      "Epoch: 16 - Batch: 57280 - Loss: 1.928933 - Time:131.95374488830566\n",
      "Epoch: 16 - Batch: 61376 - Loss: 1.816280 - Time:141.36514449119568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 - Batch: 65472 - Loss: 3.109900 - Time:150.7756016254425\n",
      "Epoch: 16 - Batch: 69568 - Loss: 2.388393 - Time:160.20396518707275\n",
      "Epoch: 16 - Batch: 73664 - Loss: 1.796962 - Time:169.61558604240417\n",
      "Epoch: 16 - Batch: 77760 - Loss: 3.375582 - Time:179.0263547897339\n",
      "Epoch: 16 - Batch: 81856 - Loss: 3.422773 - Time:188.43616342544556\n",
      "Epoch: 16 - Batch: 85952 - Loss: 2.814898 - Time:197.84815216064453\n",
      "Epoch: 16 - Batch: 90048 - Loss: 1.856678 - Time:207.27369451522827\n",
      "Epoch: 16 - Batch: 94144 - Loss: 3.084792 - Time:216.68379759788513\n",
      "Epoch: 16 - Batch: 98240 - Loss: 2.132100 - Time:226.111088514328\n",
      "Epoch: 16 - Batch: 102336 - Loss: 1.781449 - Time:235.52792072296143\n",
      "Epoch: 16 - Batch: 106432 - Loss: 2.816842 - Time:244.94263434410095\n",
      "Epoch: 16 - Batch: 110528 - Loss: 1.769979 - Time:254.35342741012573\n",
      "Epoch: 16 - Batch: 114624 - Loss: 1.848657 - Time:263.76597356796265\n",
      "Epoch: 16 - Batch: 118720 - Loss: 2.687830 - Time:273.1757564544678\n",
      "Epoch: 16 - Batch: 122816 - Loss: 1.622970 - Time:282.58652782440186\n",
      "Epoch: 16 - Batch: 126912 - Loss: 2.658281 - Time:291.995477437973\n",
      "Epoch: 16 - Batch: 131008 - Loss: 3.220870 - Time:301.404513835907\n",
      "Epoch: 16 - Batch: 135104 - Loss: 1.942089 - Time:310.8315818309784\n",
      "Epoch: 16 - Batch: 139200 - Loss: 2.846663 - Time:320.24123072624207\n",
      "Epoch: 16 - Batch: 143296 - Loss: 2.151091 - Time:329.65104246139526\n",
      "Epoch: 16 - Batch: 147392 - Loss: 2.470526 - Time:339.0608260631561\n",
      "Epoch: 16 - Batch: 151488 - Loss: 1.459628 - Time:348.48615169525146\n",
      "Epoch: 17 - Batch: 4032 - Loss: 1.817521 - Time:9.549695253372192\n",
      "Epoch: 17 - Batch: 8128 - Loss: 2.033449 - Time:18.960658073425293\n",
      "Epoch: 17 - Batch: 12224 - Loss: 1.964933 - Time:28.398497343063354\n",
      "Epoch: 17 - Batch: 16320 - Loss: 2.018338 - Time:37.80819272994995\n",
      "Epoch: 17 - Batch: 20416 - Loss: 1.366315 - Time:47.21542811393738\n",
      "Epoch: 17 - Batch: 24512 - Loss: 2.072494 - Time:56.64084076881409\n",
      "Epoch: 17 - Batch: 28608 - Loss: 2.134257 - Time:66.05204510688782\n",
      "Epoch: 17 - Batch: 32704 - Loss: 1.399875 - Time:75.45993638038635\n",
      "Epoch: 17 - Batch: 36800 - Loss: 1.548507 - Time:84.86887812614441\n",
      "Epoch: 17 - Batch: 40896 - Loss: 2.242324 - Time:94.27912044525146\n",
      "Epoch: 17 - Batch: 44992 - Loss: 1.879928 - Time:103.69014382362366\n",
      "Epoch: 17 - Batch: 49088 - Loss: 1.498298 - Time:113.1008722782135\n",
      "Epoch: 17 - Batch: 53184 - Loss: 1.709268 - Time:122.51054286956787\n",
      "Epoch: 17 - Batch: 57280 - Loss: 1.905234 - Time:131.9374327659607\n",
      "Epoch: 17 - Batch: 61376 - Loss: 1.521379 - Time:141.34919691085815\n",
      "Epoch: 17 - Batch: 65472 - Loss: 1.683256 - Time:150.7617220878601\n",
      "Epoch: 17 - Batch: 69568 - Loss: 1.956779 - Time:160.17370009422302\n",
      "Epoch: 17 - Batch: 73664 - Loss: 1.616708 - Time:169.6002209186554\n",
      "Epoch: 17 - Batch: 77760 - Loss: 1.836366 - Time:179.01269459724426\n",
      "Epoch: 17 - Batch: 81856 - Loss: 2.209178 - Time:188.42167234420776\n",
      "Epoch: 17 - Batch: 85952 - Loss: 2.144955 - Time:197.84621739387512\n",
      "Epoch: 17 - Batch: 90048 - Loss: 2.199003 - Time:207.25523400306702\n",
      "Epoch: 17 - Batch: 94144 - Loss: 1.302124 - Time:216.66407418251038\n",
      "Epoch: 17 - Batch: 98240 - Loss: 1.709375 - Time:226.0727345943451\n",
      "Epoch: 17 - Batch: 102336 - Loss: 2.265487 - Time:235.481689453125\n",
      "Epoch: 17 - Batch: 106432 - Loss: 2.440037 - Time:244.89370846748352\n",
      "Epoch: 17 - Batch: 110528 - Loss: 1.686943 - Time:254.30713772773743\n",
      "Epoch: 17 - Batch: 114624 - Loss: 1.387526 - Time:263.71544218063354\n",
      "Epoch: 17 - Batch: 118720 - Loss: 1.930171 - Time:273.1395902633667\n",
      "Epoch: 17 - Batch: 122816 - Loss: 1.836602 - Time:282.54659628868103\n",
      "Epoch: 17 - Batch: 126912 - Loss: 2.201315 - Time:291.955468416214\n",
      "Epoch: 17 - Batch: 131008 - Loss: 2.072391 - Time:301.36624336242676\n",
      "Epoch: 17 - Batch: 135104 - Loss: 1.261434 - Time:310.7918372154236\n",
      "Epoch: 17 - Batch: 139200 - Loss: 2.052162 - Time:320.2012405395508\n",
      "Epoch: 17 - Batch: 143296 - Loss: 2.140375 - Time:329.6090874671936\n",
      "Epoch: 17 - Batch: 147392 - Loss: 2.836728 - Time:339.0353765487671\n",
      "Epoch: 17 - Batch: 151488 - Loss: 1.650168 - Time:348.44345569610596\n",
      "Epoch: 18 - Batch: 4032 - Loss: 1.157543 - Time:9.57145380973816\n",
      "Epoch: 18 - Batch: 8128 - Loss: 1.403081 - Time:18.984105348587036\n",
      "Epoch: 18 - Batch: 12224 - Loss: 1.286357 - Time:28.421859979629517\n",
      "Epoch: 18 - Batch: 16320 - Loss: 1.640320 - Time:37.83344912528992\n",
      "Epoch: 18 - Batch: 20416 - Loss: 1.432307 - Time:47.24305081367493\n",
      "Epoch: 18 - Batch: 24512 - Loss: 1.371577 - Time:56.65239691734314\n",
      "Epoch: 18 - Batch: 28608 - Loss: 1.366843 - Time:66.06524276733398\n",
      "Epoch: 18 - Batch: 32704 - Loss: 1.613428 - Time:75.47432780265808\n",
      "Epoch: 18 - Batch: 36800 - Loss: 2.289197 - Time:84.88754439353943\n",
      "Epoch: 18 - Batch: 40896 - Loss: 1.060853 - Time:94.29973030090332\n",
      "Epoch: 18 - Batch: 44992 - Loss: 1.302807 - Time:103.72783088684082\n",
      "Epoch: 18 - Batch: 49088 - Loss: 2.033510 - Time:113.1358871459961\n",
      "Epoch: 18 - Batch: 53184 - Loss: 1.218590 - Time:122.54538941383362\n",
      "Epoch: 18 - Batch: 57280 - Loss: 1.490792 - Time:131.9579954147339\n",
      "Epoch: 18 - Batch: 61376 - Loss: 0.737619 - Time:141.38568496704102\n",
      "Epoch: 18 - Batch: 65472 - Loss: 1.412714 - Time:150.79740190505981\n",
      "Epoch: 18 - Batch: 69568 - Loss: 1.504555 - Time:160.2078275680542\n",
      "Epoch: 18 - Batch: 73664 - Loss: 1.700384 - Time:169.63662219047546\n",
      "Epoch: 18 - Batch: 77760 - Loss: 1.385277 - Time:179.04972219467163\n",
      "Epoch: 18 - Batch: 81856 - Loss: 1.543108 - Time:188.4605860710144\n",
      "Epoch: 18 - Batch: 85952 - Loss: 1.054729 - Time:197.87122344970703\n",
      "Epoch: 18 - Batch: 90048 - Loss: 1.018652 - Time:207.28009819984436\n",
      "Epoch: 18 - Batch: 94144 - Loss: 1.216833 - Time:216.69240140914917\n",
      "Epoch: 18 - Batch: 98240 - Loss: 1.851928 - Time:226.10263299942017\n",
      "Epoch: 18 - Batch: 102336 - Loss: 1.834293 - Time:235.5127773284912\n",
      "Epoch: 18 - Batch: 106432 - Loss: 1.302140 - Time:244.9402050971985\n",
      "Epoch: 18 - Batch: 110528 - Loss: 1.267362 - Time:254.34952425956726\n",
      "Epoch: 18 - Batch: 114624 - Loss: 0.884416 - Time:263.7596113681793\n",
      "Epoch: 18 - Batch: 118720 - Loss: 1.347985 - Time:273.17076683044434\n",
      "Epoch: 18 - Batch: 122816 - Loss: 1.723707 - Time:282.59843587875366\n",
      "Epoch: 18 - Batch: 126912 - Loss: 1.383101 - Time:292.00886154174805\n",
      "Epoch: 18 - Batch: 131008 - Loss: 1.081580 - Time:301.41940355300903\n",
      "Epoch: 18 - Batch: 135104 - Loss: 1.691117 - Time:310.8478088378906\n",
      "Epoch: 18 - Batch: 139200 - Loss: 1.061411 - Time:320.25797963142395\n",
      "Epoch: 18 - Batch: 143296 - Loss: 1.494753 - Time:329.6679401397705\n",
      "Epoch: 18 - Batch: 147392 - Loss: 1.670472 - Time:339.07875967025757\n",
      "Epoch: 18 - Batch: 151488 - Loss: 2.171018 - Time:348.4874391555786\n",
      "Epoch: 19 - Batch: 4032 - Loss: 1.407022 - Time:9.53151535987854\n",
      "Epoch: 19 - Batch: 8128 - Loss: 0.721156 - Time:18.9673433303833\n",
      "Epoch: 19 - Batch: 12224 - Loss: 1.239215 - Time:28.377158403396606\n",
      "Epoch: 19 - Batch: 16320 - Loss: 1.070837 - Time:37.78688955307007\n",
      "Epoch: 19 - Batch: 20416 - Loss: 0.989742 - Time:47.19629096984863\n",
      "Epoch: 19 - Batch: 24512 - Loss: 1.136277 - Time:56.605785608291626\n",
      "Epoch: 19 - Batch: 28608 - Loss: 1.332698 - Time:66.01539587974548\n",
      "Epoch: 19 - Batch: 32704 - Loss: 1.720163 - Time:75.4264988899231\n",
      "Epoch: 19 - Batch: 36800 - Loss: 2.040972 - Time:84.83821487426758\n",
      "Epoch: 19 - Batch: 40896 - Loss: 1.113338 - Time:94.26329851150513\n",
      "Epoch: 19 - Batch: 44992 - Loss: 1.394935 - Time:103.67077660560608\n",
      "Epoch: 19 - Batch: 49088 - Loss: 0.993971 - Time:113.07888627052307\n",
      "Epoch: 19 - Batch: 53184 - Loss: 1.284764 - Time:122.48749566078186\n",
      "Epoch: 19 - Batch: 57280 - Loss: 1.174165 - Time:131.91305470466614\n",
      "Epoch: 19 - Batch: 61376 - Loss: 1.034020 - Time:141.32060480117798\n",
      "Epoch: 19 - Batch: 65472 - Loss: 1.340454 - Time:150.72723960876465\n",
      "Epoch: 19 - Batch: 69568 - Loss: 1.652808 - Time:160.15198230743408\n",
      "Epoch: 19 - Batch: 73664 - Loss: 1.588482 - Time:169.55796360969543\n",
      "Epoch: 19 - Batch: 77760 - Loss: 1.306836 - Time:178.96515893936157\n",
      "Epoch: 19 - Batch: 81856 - Loss: 1.173927 - Time:188.3723795413971\n",
      "Epoch: 19 - Batch: 85952 - Loss: 0.931187 - Time:197.77833247184753\n",
      "Epoch: 19 - Batch: 90048 - Loss: 1.131449 - Time:207.18473982810974\n",
      "Epoch: 19 - Batch: 94144 - Loss: 1.511823 - Time:216.59327054023743\n",
      "Epoch: 19 - Batch: 98240 - Loss: 1.285572 - Time:226.00141763687134\n",
      "Epoch: 19 - Batch: 102336 - Loss: 1.201697 - Time:235.4253430366516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 - Batch: 106432 - Loss: 1.665824 - Time:244.83193802833557\n",
      "Epoch: 19 - Batch: 110528 - Loss: 1.524512 - Time:254.24020504951477\n",
      "Epoch: 19 - Batch: 114624 - Loss: 1.191915 - Time:263.64840483665466\n",
      "Epoch: 19 - Batch: 118720 - Loss: 1.084667 - Time:273.07401514053345\n",
      "Epoch: 19 - Batch: 122816 - Loss: 2.479450 - Time:282.483047246933\n",
      "Epoch: 19 - Batch: 126912 - Loss: 1.125271 - Time:291.8921756744385\n",
      "Epoch: 19 - Batch: 131008 - Loss: 1.538623 - Time:301.31569480895996\n",
      "Epoch: 19 - Batch: 135104 - Loss: 1.091715 - Time:310.7245469093323\n",
      "Epoch: 19 - Batch: 139200 - Loss: 1.857282 - Time:320.13194942474365\n",
      "Epoch: 19 - Batch: 143296 - Loss: 1.239630 - Time:329.54036593437195\n",
      "Epoch: 19 - Batch: 147392 - Loss: 1.695570 - Time:338.94917941093445\n",
      "Epoch: 19 - Batch: 151488 - Loss: 1.730883 - Time:348.35904908180237\n",
      "Epoch: 20 - Batch: 4032 - Loss: 0.794284 - Time:9.541469812393188\n",
      "Epoch: 20 - Batch: 8128 - Loss: 0.764281 - Time:18.954978942871094\n",
      "Epoch: 20 - Batch: 12224 - Loss: 1.148020 - Time:28.391499757766724\n",
      "Epoch: 20 - Batch: 16320 - Loss: 0.956112 - Time:37.799784421920776\n",
      "Epoch: 20 - Batch: 20416 - Loss: 0.935142 - Time:47.20619869232178\n",
      "Epoch: 20 - Batch: 24512 - Loss: 0.792721 - Time:56.61378216743469\n",
      "Epoch: 20 - Batch: 28608 - Loss: 1.618167 - Time:66.03787064552307\n",
      "Epoch: 20 - Batch: 32704 - Loss: 1.156273 - Time:75.4477219581604\n",
      "Epoch: 20 - Batch: 36800 - Loss: 0.751350 - Time:84.85870718955994\n",
      "Epoch: 20 - Batch: 40896 - Loss: 1.263094 - Time:94.28372955322266\n",
      "Epoch: 20 - Batch: 44992 - Loss: 1.545184 - Time:103.69156002998352\n",
      "Epoch: 20 - Batch: 49088 - Loss: 1.092386 - Time:113.10315251350403\n",
      "Epoch: 20 - Batch: 53184 - Loss: 0.560804 - Time:122.51470589637756\n",
      "Epoch: 20 - Batch: 57280 - Loss: 0.865179 - Time:131.92530584335327\n",
      "Epoch: 20 - Batch: 61376 - Loss: 0.816039 - Time:141.33663868904114\n",
      "Epoch: 20 - Batch: 65472 - Loss: 1.338135 - Time:150.74585008621216\n",
      "Epoch: 20 - Batch: 69568 - Loss: 1.493160 - Time:160.15647530555725\n",
      "Epoch: 20 - Batch: 73664 - Loss: 1.922586 - Time:169.58884930610657\n",
      "Epoch: 20 - Batch: 77760 - Loss: 1.147979 - Time:178.99993252754211\n",
      "Epoch: 20 - Batch: 81856 - Loss: 0.898943 - Time:188.41027069091797\n",
      "Epoch: 20 - Batch: 85952 - Loss: 1.013554 - Time:197.82249855995178\n",
      "Epoch: 20 - Batch: 90048 - Loss: 0.834429 - Time:207.24937415122986\n",
      "Epoch: 20 - Batch: 94144 - Loss: 0.424006 - Time:216.66055941581726\n",
      "Epoch: 20 - Batch: 98240 - Loss: 1.127292 - Time:226.07116150856018\n",
      "Epoch: 20 - Batch: 102336 - Loss: 1.398592 - Time:235.495845079422\n",
      "Epoch: 20 - Batch: 106432 - Loss: 1.511568 - Time:244.90453577041626\n",
      "Epoch: 20 - Batch: 110528 - Loss: 1.257646 - Time:254.31643056869507\n",
      "Epoch: 20 - Batch: 114624 - Loss: 1.949915 - Time:263.7293772697449\n",
      "Epoch: 20 - Batch: 118720 - Loss: 1.403476 - Time:273.14162516593933\n",
      "Epoch: 20 - Batch: 122816 - Loss: 0.793409 - Time:282.5527927875519\n",
      "Epoch: 20 - Batch: 126912 - Loss: 1.380992 - Time:291.96500611305237\n",
      "Epoch: 20 - Batch: 131008 - Loss: 1.079428 - Time:301.3770866394043\n",
      "Epoch: 20 - Batch: 135104 - Loss: 1.486514 - Time:310.80398178100586\n",
      "Epoch: 20 - Batch: 139200 - Loss: 1.456257 - Time:320.2166967391968\n",
      "Epoch: 20 - Batch: 143296 - Loss: 0.894369 - Time:329.62939047813416\n",
      "Epoch: 20 - Batch: 147392 - Loss: 1.193211 - Time:339.0400621891022\n",
      "Epoch: 20 - Batch: 151488 - Loss: 1.177799 - Time:348.46674489974976\n",
      "Epoch: 21 - Batch: 4032 - Loss: 0.823166 - Time:9.532477140426636\n",
      "Epoch: 21 - Batch: 8128 - Loss: 1.463342 - Time:18.946719646453857\n",
      "Epoch: 21 - Batch: 12224 - Loss: 1.025278 - Time:28.3848819732666\n",
      "Epoch: 21 - Batch: 16320 - Loss: 0.938016 - Time:37.79347610473633\n",
      "Epoch: 21 - Batch: 20416 - Loss: 0.603733 - Time:47.201905488967896\n",
      "Epoch: 21 - Batch: 24512 - Loss: 1.325068 - Time:56.62755799293518\n",
      "Epoch: 21 - Batch: 28608 - Loss: 0.913940 - Time:66.03692936897278\n",
      "Epoch: 21 - Batch: 32704 - Loss: 1.082917 - Time:75.44697451591492\n",
      "Epoch: 21 - Batch: 36800 - Loss: 1.002532 - Time:84.85572934150696\n",
      "Epoch: 21 - Batch: 40896 - Loss: 1.329952 - Time:94.2650535106659\n",
      "Epoch: 21 - Batch: 44992 - Loss: 1.130362 - Time:103.67540216445923\n",
      "Epoch: 21 - Batch: 49088 - Loss: 0.674096 - Time:113.08461380004883\n",
      "Epoch: 21 - Batch: 53184 - Loss: 1.150564 - Time:122.49462532997131\n",
      "Epoch: 21 - Batch: 57280 - Loss: 0.814811 - Time:131.91853880882263\n",
      "Epoch: 21 - Batch: 61376 - Loss: 0.648934 - Time:141.32722878456116\n",
      "Epoch: 21 - Batch: 65472 - Loss: 1.112182 - Time:150.73642992973328\n",
      "Epoch: 21 - Batch: 69568 - Loss: 0.998717 - Time:160.14497590065002\n",
      "Epoch: 21 - Batch: 73664 - Loss: 0.358496 - Time:169.55293679237366\n",
      "Epoch: 21 - Batch: 77760 - Loss: 0.582036 - Time:178.98033952713013\n",
      "Epoch: 21 - Batch: 81856 - Loss: 0.608851 - Time:188.39049530029297\n",
      "Epoch: 21 - Batch: 85952 - Loss: 1.019977 - Time:197.79956316947937\n",
      "Epoch: 21 - Batch: 90048 - Loss: 0.850934 - Time:207.2251124382019\n",
      "Epoch: 21 - Batch: 94144 - Loss: 0.601541 - Time:216.63476848602295\n",
      "Epoch: 21 - Batch: 98240 - Loss: 1.443162 - Time:226.04693245887756\n",
      "Epoch: 21 - Batch: 102336 - Loss: 1.114440 - Time:235.45927619934082\n",
      "Epoch: 21 - Batch: 106432 - Loss: 0.939000 - Time:244.8693151473999\n",
      "Epoch: 21 - Batch: 110528 - Loss: 1.346817 - Time:254.27932214736938\n",
      "Epoch: 21 - Batch: 114624 - Loss: 0.964789 - Time:263.6902356147766\n",
      "Epoch: 21 - Batch: 118720 - Loss: 0.828544 - Time:273.10364961624146\n",
      "Epoch: 21 - Batch: 122816 - Loss: 0.999849 - Time:282.5336437225342\n",
      "Epoch: 21 - Batch: 126912 - Loss: 1.222207 - Time:291.94687509536743\n",
      "Epoch: 21 - Batch: 131008 - Loss: 0.965488 - Time:301.35677218437195\n",
      "Epoch: 21 - Batch: 135104 - Loss: 0.933847 - Time:310.770254611969\n",
      "Epoch: 21 - Batch: 139200 - Loss: 1.290740 - Time:320.1988570690155\n",
      "Epoch: 21 - Batch: 143296 - Loss: 0.660489 - Time:329.6096875667572\n",
      "Epoch: 21 - Batch: 147392 - Loss: 1.801791 - Time:339.0211777687073\n",
      "Epoch: 21 - Batch: 151488 - Loss: 1.117068 - Time:348.4467213153839\n",
      "Epoch: 22 - Batch: 4032 - Loss: 0.633489 - Time:9.530715942382812\n",
      "Epoch: 22 - Batch: 8128 - Loss: 0.940354 - Time:18.94283676147461\n",
      "Epoch: 22 - Batch: 12224 - Loss: 1.124473 - Time:28.354171752929688\n",
      "Epoch: 22 - Batch: 16320 - Loss: 1.115129 - Time:37.76422309875488\n",
      "Epoch: 22 - Batch: 20416 - Loss: 0.848826 - Time:47.17391347885132\n",
      "Epoch: 22 - Batch: 24512 - Loss: 1.321213 - Time:56.5821807384491\n",
      "Epoch: 22 - Batch: 28608 - Loss: 0.455886 - Time:66.01597881317139\n",
      "Epoch: 22 - Batch: 32704 - Loss: 0.250581 - Time:75.4254891872406\n",
      "Epoch: 22 - Batch: 36800 - Loss: 0.741775 - Time:84.83386135101318\n",
      "Epoch: 22 - Batch: 40896 - Loss: 0.873434 - Time:94.24265193939209\n",
      "Epoch: 22 - Batch: 44992 - Loss: 0.565884 - Time:103.66725301742554\n",
      "Epoch: 22 - Batch: 49088 - Loss: 0.565158 - Time:113.0741355419159\n",
      "Epoch: 22 - Batch: 53184 - Loss: 0.611013 - Time:122.48167848587036\n",
      "Epoch: 22 - Batch: 57280 - Loss: 0.981674 - Time:131.90863394737244\n",
      "Epoch: 22 - Batch: 61376 - Loss: 1.128104 - Time:141.31607961654663\n",
      "Epoch: 22 - Batch: 65472 - Loss: 0.834199 - Time:150.72520399093628\n",
      "Epoch: 22 - Batch: 69568 - Loss: 0.466821 - Time:160.1347303390503\n",
      "Epoch: 22 - Batch: 73664 - Loss: 1.461730 - Time:169.54384970664978\n",
      "Epoch: 22 - Batch: 77760 - Loss: 1.736628 - Time:178.95311546325684\n",
      "Epoch: 22 - Batch: 81856 - Loss: 1.047296 - Time:188.36232948303223\n",
      "Epoch: 22 - Batch: 85952 - Loss: 1.014912 - Time:197.77467846870422\n",
      "Epoch: 22 - Batch: 90048 - Loss: 0.929795 - Time:207.20027589797974\n",
      "Epoch: 22 - Batch: 94144 - Loss: 0.883575 - Time:216.60744643211365\n",
      "Epoch: 22 - Batch: 98240 - Loss: 0.434010 - Time:226.01630854606628\n",
      "Epoch: 22 - Batch: 102336 - Loss: 0.848785 - Time:235.4243507385254\n",
      "Epoch: 22 - Batch: 106432 - Loss: 1.184338 - Time:244.8340654373169\n",
      "Epoch: 22 - Batch: 110528 - Loss: 0.271878 - Time:254.26066756248474\n",
      "Epoch: 22 - Batch: 114624 - Loss: 0.707528 - Time:263.66932821273804\n",
      "Epoch: 22 - Batch: 118720 - Loss: 0.508618 - Time:273.07818937301636\n",
      "Epoch: 22 - Batch: 122816 - Loss: 1.135560 - Time:282.504123210907\n",
      "Epoch: 22 - Batch: 126912 - Loss: 1.026516 - Time:291.9118461608887\n",
      "Epoch: 22 - Batch: 131008 - Loss: 1.363126 - Time:301.3203444480896\n",
      "Epoch: 22 - Batch: 135104 - Loss: 0.491438 - Time:310.73025608062744\n",
      "Epoch: 22 - Batch: 139200 - Loss: 1.110453 - Time:320.13887214660645\n",
      "Epoch: 22 - Batch: 143296 - Loss: 1.427601 - Time:329.5490653514862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 - Batch: 147392 - Loss: 1.277563 - Time:338.95978450775146\n",
      "Epoch: 22 - Batch: 151488 - Loss: 1.238671 - Time:348.37012553215027\n",
      "Epoch: 23 - Batch: 4032 - Loss: 0.812141 - Time:9.559623956680298\n",
      "Epoch: 23 - Batch: 8128 - Loss: 0.833426 - Time:18.966094255447388\n",
      "Epoch: 23 - Batch: 12224 - Loss: 0.591086 - Time:28.374496698379517\n",
      "Epoch: 23 - Batch: 16320 - Loss: 0.623208 - Time:37.81215310096741\n",
      "Epoch: 23 - Batch: 20416 - Loss: 0.621373 - Time:47.22276949882507\n",
      "Epoch: 23 - Batch: 24512 - Loss: 0.980448 - Time:56.632763385772705\n",
      "Epoch: 23 - Batch: 28608 - Loss: 0.173661 - Time:66.06045794487\n",
      "Epoch: 23 - Batch: 32704 - Loss: 0.934694 - Time:75.46682405471802\n",
      "Epoch: 23 - Batch: 36800 - Loss: 0.765271 - Time:84.87345480918884\n",
      "Epoch: 23 - Batch: 40896 - Loss: 0.549363 - Time:94.28285789489746\n",
      "Epoch: 23 - Batch: 44992 - Loss: 1.230381 - Time:103.69402170181274\n",
      "Epoch: 23 - Batch: 49088 - Loss: 0.697810 - Time:113.10347437858582\n",
      "Epoch: 23 - Batch: 53184 - Loss: 0.902894 - Time:122.51296663284302\n",
      "Epoch: 23 - Batch: 57280 - Loss: 0.475300 - Time:131.92064428329468\n",
      "Epoch: 23 - Batch: 61376 - Loss: 1.062440 - Time:141.34732818603516\n",
      "Epoch: 23 - Batch: 65472 - Loss: 1.062687 - Time:150.75699996948242\n",
      "Epoch: 23 - Batch: 69568 - Loss: 1.326259 - Time:160.16738843917847\n",
      "Epoch: 23 - Batch: 73664 - Loss: 0.370202 - Time:169.57836079597473\n",
      "Epoch: 23 - Batch: 77760 - Loss: 0.854483 - Time:178.98755407333374\n",
      "Epoch: 23 - Batch: 81856 - Loss: 1.225024 - Time:188.41041707992554\n",
      "Epoch: 23 - Batch: 85952 - Loss: 1.318546 - Time:197.81568789482117\n",
      "Epoch: 23 - Batch: 90048 - Loss: 0.670583 - Time:207.23865580558777\n",
      "Epoch: 23 - Batch: 94144 - Loss: 1.483497 - Time:216.64546489715576\n",
      "Epoch: 23 - Batch: 98240 - Loss: 0.634812 - Time:226.05243110656738\n",
      "Epoch: 23 - Batch: 102336 - Loss: 0.627610 - Time:235.46004104614258\n",
      "Epoch: 23 - Batch: 106432 - Loss: 1.539758 - Time:244.86714363098145\n",
      "Epoch: 23 - Batch: 110528 - Loss: 0.703465 - Time:254.27408480644226\n",
      "Epoch: 23 - Batch: 114624 - Loss: 0.854036 - Time:263.6831223964691\n",
      "Epoch: 23 - Batch: 118720 - Loss: 0.845016 - Time:273.09255504608154\n",
      "Epoch: 23 - Batch: 122816 - Loss: 0.893296 - Time:282.4986312389374\n",
      "Epoch: 23 - Batch: 126912 - Loss: 0.652405 - Time:291.9249997138977\n",
      "Epoch: 23 - Batch: 131008 - Loss: 1.627606 - Time:301.3327810764313\n",
      "Epoch: 23 - Batch: 135104 - Loss: 0.799709 - Time:310.74112462997437\n",
      "Epoch: 23 - Batch: 139200 - Loss: 1.111491 - Time:320.1470057964325\n",
      "Epoch: 23 - Batch: 143296 - Loss: 1.410543 - Time:329.5714793205261\n",
      "Epoch: 23 - Batch: 147392 - Loss: 0.405274 - Time:338.98043870925903\n",
      "Epoch: 23 - Batch: 151488 - Loss: 1.507459 - Time:348.3880543708801\n",
      "Epoch: 24 - Batch: 4032 - Loss: 1.101465 - Time:9.54266905784607\n",
      "Epoch: 24 - Batch: 8128 - Loss: 0.459143 - Time:18.951138973236084\n",
      "Epoch: 24 - Batch: 12224 - Loss: 0.874336 - Time:28.361351013183594\n",
      "Epoch: 24 - Batch: 16320 - Loss: 0.713978 - Time:37.773574113845825\n",
      "Epoch: 24 - Batch: 20416 - Loss: 0.878110 - Time:47.18409514427185\n",
      "Epoch: 24 - Batch: 24512 - Loss: 0.694252 - Time:56.59470009803772\n",
      "Epoch: 24 - Batch: 28608 - Loss: 0.367243 - Time:66.00553297996521\n",
      "Epoch: 24 - Batch: 32704 - Loss: 0.388229 - Time:75.44509649276733\n",
      "Epoch: 24 - Batch: 36800 - Loss: 0.622260 - Time:84.85730457305908\n",
      "Epoch: 24 - Batch: 40896 - Loss: 0.730535 - Time:94.26858186721802\n",
      "Epoch: 24 - Batch: 44992 - Loss: 0.513798 - Time:103.67883038520813\n",
      "Epoch: 24 - Batch: 49088 - Loss: 1.255332 - Time:113.10582876205444\n",
      "Epoch: 24 - Batch: 53184 - Loss: 0.686779 - Time:122.51771593093872\n",
      "Epoch: 24 - Batch: 57280 - Loss: 0.416572 - Time:131.9296863079071\n",
      "Epoch: 24 - Batch: 61376 - Loss: 0.704743 - Time:141.35735177993774\n",
      "Epoch: 24 - Batch: 65472 - Loss: 0.923826 - Time:150.7699158191681\n",
      "Epoch: 24 - Batch: 69568 - Loss: 1.090413 - Time:160.18080282211304\n",
      "Epoch: 24 - Batch: 73664 - Loss: 0.668471 - Time:169.5941822528839\n",
      "Epoch: 24 - Batch: 77760 - Loss: 0.886754 - Time:179.00681400299072\n",
      "Epoch: 24 - Batch: 81856 - Loss: 0.679617 - Time:188.41990733146667\n",
      "Epoch: 24 - Batch: 85952 - Loss: 0.912959 - Time:197.82970666885376\n",
      "Epoch: 24 - Batch: 90048 - Loss: 0.471975 - Time:207.23802089691162\n",
      "Epoch: 24 - Batch: 94144 - Loss: 0.537451 - Time:216.66392588615417\n",
      "Epoch: 24 - Batch: 98240 - Loss: 0.339752 - Time:226.07526326179504\n",
      "Epoch: 24 - Batch: 102336 - Loss: 1.308781 - Time:235.48593831062317\n",
      "Epoch: 24 - Batch: 106432 - Loss: 1.050273 - Time:244.892808675766\n",
      "Epoch: 24 - Batch: 110528 - Loss: 1.474466 - Time:254.30302715301514\n",
      "Epoch: 24 - Batch: 114624 - Loss: 0.644051 - Time:263.7285985946655\n",
      "Epoch: 24 - Batch: 118720 - Loss: 0.660480 - Time:273.13972640037537\n",
      "Epoch: 24 - Batch: 122816 - Loss: 0.929888 - Time:282.5679740905762\n",
      "Epoch: 24 - Batch: 126912 - Loss: 1.064803 - Time:291.9800627231598\n",
      "Epoch: 24 - Batch: 131008 - Loss: 0.745025 - Time:301.39260697364807\n",
      "Epoch: 24 - Batch: 135104 - Loss: 0.913931 - Time:310.8042619228363\n",
      "Epoch: 24 - Batch: 139200 - Loss: 0.932231 - Time:320.2159044742584\n",
      "Epoch: 24 - Batch: 143296 - Loss: 1.516407 - Time:329.624249458313\n",
      "Epoch: 24 - Batch: 147392 - Loss: 0.515607 - Time:339.0344617366791\n",
      "Epoch: 24 - Batch: 151488 - Loss: 0.468786 - Time:348.44244837760925\n",
      "Epoch: 25 - Batch: 4032 - Loss: 0.937145 - Time:9.570502042770386\n",
      "Epoch: 25 - Batch: 8128 - Loss: 0.916998 - Time:18.978603839874268\n",
      "Epoch: 25 - Batch: 12224 - Loss: 0.431634 - Time:28.388166904449463\n",
      "Epoch: 25 - Batch: 16320 - Loss: 0.429766 - Time:37.798088788986206\n",
      "Epoch: 25 - Batch: 20416 - Loss: 0.509198 - Time:47.20785617828369\n",
      "Epoch: 25 - Batch: 24512 - Loss: 0.519270 - Time:56.61456656455994\n",
      "Epoch: 25 - Batch: 28608 - Loss: 1.323164 - Time:66.0214900970459\n",
      "Epoch: 25 - Batch: 32704 - Loss: 0.206490 - Time:75.42779350280762\n",
      "Epoch: 25 - Batch: 36800 - Loss: 0.723392 - Time:84.85129976272583\n",
      "Epoch: 25 - Batch: 40896 - Loss: 0.249065 - Time:94.2571189403534\n",
      "Epoch: 25 - Batch: 44992 - Loss: 0.746531 - Time:103.66531872749329\n",
      "Epoch: 25 - Batch: 49088 - Loss: 0.528955 - Time:113.07633757591248\n",
      "Epoch: 25 - Batch: 53184 - Loss: 0.414579 - Time:122.50045537948608\n",
      "Epoch: 25 - Batch: 57280 - Loss: 0.894746 - Time:131.90578603744507\n",
      "Epoch: 25 - Batch: 61376 - Loss: 0.738500 - Time:141.31124544143677\n",
      "Epoch: 25 - Batch: 65472 - Loss: 0.650962 - Time:150.73588395118713\n",
      "Epoch: 25 - Batch: 69568 - Loss: 0.932239 - Time:160.143630027771\n",
      "Epoch: 25 - Batch: 73664 - Loss: 0.235935 - Time:169.5530297756195\n",
      "Epoch: 25 - Batch: 77760 - Loss: 0.458340 - Time:178.96172761917114\n",
      "Epoch: 25 - Batch: 81856 - Loss: 0.603824 - Time:188.37104964256287\n",
      "Epoch: 25 - Batch: 85952 - Loss: 0.238448 - Time:197.7793629169464\n",
      "Epoch: 25 - Batch: 90048 - Loss: 0.329450 - Time:207.1861011981964\n",
      "Epoch: 25 - Batch: 94144 - Loss: 0.655332 - Time:216.59324145317078\n",
      "Epoch: 25 - Batch: 98240 - Loss: 0.593055 - Time:226.01889038085938\n",
      "Epoch: 25 - Batch: 102336 - Loss: 0.828273 - Time:235.42667961120605\n",
      "Epoch: 25 - Batch: 106432 - Loss: 1.214202 - Time:244.83468985557556\n",
      "Epoch: 25 - Batch: 110528 - Loss: 0.609580 - Time:254.2422378063202\n",
      "Epoch: 25 - Batch: 114624 - Loss: 0.893061 - Time:263.6669797897339\n",
      "Epoch: 25 - Batch: 118720 - Loss: 0.497909 - Time:273.0747537612915\n",
      "Epoch: 25 - Batch: 122816 - Loss: 0.522428 - Time:282.48550271987915\n",
      "Epoch: 25 - Batch: 126912 - Loss: 0.960500 - Time:291.91179609298706\n",
      "Epoch: 25 - Batch: 131008 - Loss: 0.643110 - Time:301.3217508792877\n",
      "Epoch: 25 - Batch: 135104 - Loss: 0.581298 - Time:310.7329194545746\n",
      "Epoch: 25 - Batch: 139200 - Loss: 0.764295 - Time:320.1439759731293\n",
      "Epoch: 25 - Batch: 143296 - Loss: 0.932533 - Time:329.5530071258545\n",
      "Epoch: 25 - Batch: 147392 - Loss: 0.339490 - Time:338.9623339176178\n",
      "Epoch: 25 - Batch: 151488 - Loss: 0.862418 - Time:348.3732089996338\n",
      "Epoch: 26 - Batch: 4032 - Loss: 0.605143 - Time:9.587413311004639\n",
      "Epoch: 26 - Batch: 8128 - Loss: 0.384571 - Time:18.997435092926025\n",
      "Epoch: 26 - Batch: 12224 - Loss: 0.177055 - Time:28.40573811531067\n",
      "Epoch: 26 - Batch: 16320 - Loss: 0.303198 - Time:37.812477111816406\n",
      "Epoch: 26 - Batch: 20416 - Loss: 0.726196 - Time:47.22283744812012\n",
      "Epoch: 26 - Batch: 24512 - Loss: 0.661495 - Time:56.63328838348389\n",
      "Epoch: 26 - Batch: 28608 - Loss: 0.194641 - Time:66.04333233833313\n",
      "Epoch: 26 - Batch: 32704 - Loss: 0.714630 - Time:75.45276117324829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 - Batch: 36800 - Loss: 0.396955 - Time:84.87803912162781\n",
      "Epoch: 26 - Batch: 40896 - Loss: 0.398845 - Time:94.28365015983582\n",
      "Epoch: 26 - Batch: 44992 - Loss: 0.574021 - Time:103.69156765937805\n",
      "Epoch: 26 - Batch: 49088 - Loss: 0.699004 - Time:113.10084891319275\n",
      "Epoch: 26 - Batch: 53184 - Loss: 0.725133 - Time:122.52578735351562\n",
      "Epoch: 26 - Batch: 57280 - Loss: 0.454534 - Time:131.93439483642578\n",
      "Epoch: 26 - Batch: 61376 - Loss: 0.618462 - Time:141.34242391586304\n",
      "Epoch: 26 - Batch: 65472 - Loss: 0.690065 - Time:150.76840257644653\n",
      "Epoch: 26 - Batch: 69568 - Loss: 0.744315 - Time:160.17743182182312\n",
      "Epoch: 26 - Batch: 73664 - Loss: 0.571971 - Time:169.58660864830017\n",
      "Epoch: 26 - Batch: 77760 - Loss: 0.396330 - Time:178.99476766586304\n",
      "Epoch: 26 - Batch: 81856 - Loss: 1.362921 - Time:188.40167427062988\n",
      "Epoch: 26 - Batch: 85952 - Loss: 1.456776 - Time:197.80914211273193\n",
      "Epoch: 26 - Batch: 90048 - Loss: 0.435183 - Time:207.21721625328064\n",
      "Epoch: 26 - Batch: 94144 - Loss: 0.783342 - Time:216.623699426651\n",
      "Epoch: 26 - Batch: 98240 - Loss: 0.308142 - Time:226.0478856563568\n",
      "Epoch: 26 - Batch: 102336 - Loss: 0.895907 - Time:235.45551443099976\n",
      "Epoch: 26 - Batch: 106432 - Loss: 1.409871 - Time:244.86450147628784\n",
      "Epoch: 26 - Batch: 110528 - Loss: 0.431821 - Time:254.2730758190155\n",
      "Epoch: 26 - Batch: 114624 - Loss: 0.420673 - Time:263.696754693985\n",
      "Epoch: 26 - Batch: 118720 - Loss: 0.986094 - Time:273.10616421699524\n",
      "Epoch: 26 - Batch: 122816 - Loss: 0.706451 - Time:282.5139834880829\n",
      "Epoch: 26 - Batch: 126912 - Loss: 0.744465 - Time:291.9409234523773\n",
      "Epoch: 26 - Batch: 131008 - Loss: 0.889806 - Time:301.35077142715454\n",
      "Epoch: 26 - Batch: 135104 - Loss: 0.806144 - Time:310.76249170303345\n",
      "Epoch: 26 - Batch: 139200 - Loss: 0.633154 - Time:320.1718678474426\n",
      "Epoch: 26 - Batch: 143296 - Loss: 0.475058 - Time:329.58322167396545\n",
      "Epoch: 26 - Batch: 147392 - Loss: 0.584059 - Time:338.99292945861816\n",
      "Epoch: 26 - Batch: 151488 - Loss: 0.838532 - Time:348.40203380584717\n",
      "Epoch: 27 - Batch: 4032 - Loss: 0.537467 - Time:9.574432373046875\n",
      "Epoch: 27 - Batch: 8128 - Loss: 0.330484 - Time:18.98306679725647\n",
      "Epoch: 27 - Batch: 12224 - Loss: 0.602729 - Time:28.389976978302002\n",
      "Epoch: 27 - Batch: 16320 - Loss: 0.567594 - Time:37.79699087142944\n",
      "Epoch: 27 - Batch: 20416 - Loss: 0.152992 - Time:47.20416331291199\n",
      "Epoch: 27 - Batch: 24512 - Loss: 0.416240 - Time:56.61386847496033\n",
      "Epoch: 27 - Batch: 28608 - Loss: 0.592331 - Time:66.01963019371033\n",
      "Epoch: 27 - Batch: 32704 - Loss: 0.560325 - Time:75.42720127105713\n",
      "Epoch: 27 - Batch: 36800 - Loss: 0.391489 - Time:84.85012030601501\n",
      "Epoch: 27 - Batch: 40896 - Loss: 0.822254 - Time:94.25559210777283\n",
      "Epoch: 27 - Batch: 44992 - Loss: 0.912736 - Time:103.6607129573822\n",
      "Epoch: 27 - Batch: 49088 - Loss: 0.653398 - Time:113.06711935997009\n",
      "Epoch: 27 - Batch: 53184 - Loss: 0.634627 - Time:122.49167537689209\n",
      "Epoch: 27 - Batch: 57280 - Loss: 0.844479 - Time:131.89940309524536\n",
      "Epoch: 27 - Batch: 61376 - Loss: 0.350672 - Time:141.30906629562378\n",
      "Epoch: 27 - Batch: 65472 - Loss: 0.214432 - Time:150.73194766044617\n",
      "Epoch: 27 - Batch: 69568 - Loss: 0.310443 - Time:160.13949036598206\n",
      "Epoch: 27 - Batch: 73664 - Loss: 0.194792 - Time:169.5489501953125\n",
      "Epoch: 27 - Batch: 77760 - Loss: 0.686081 - Time:178.95669269561768\n",
      "Epoch: 27 - Batch: 81856 - Loss: 0.428435 - Time:188.36608695983887\n",
      "Epoch: 27 - Batch: 85952 - Loss: 0.683154 - Time:197.77305698394775\n",
      "Epoch: 27 - Batch: 90048 - Loss: 1.011939 - Time:207.1828978061676\n",
      "Epoch: 27 - Batch: 94144 - Loss: 0.471963 - Time:216.5896282196045\n",
      "Epoch: 27 - Batch: 98240 - Loss: 0.919138 - Time:226.0154938697815\n",
      "Epoch: 27 - Batch: 102336 - Loss: 0.910153 - Time:235.4241852760315\n",
      "Epoch: 27 - Batch: 106432 - Loss: 0.621454 - Time:244.83231902122498\n",
      "Epoch: 27 - Batch: 110528 - Loss: 0.606363 - Time:254.24066853523254\n",
      "Epoch: 27 - Batch: 114624 - Loss: 0.522915 - Time:263.647922039032\n",
      "Epoch: 27 - Batch: 118720 - Loss: 0.711127 - Time:273.0742723941803\n",
      "Epoch: 27 - Batch: 122816 - Loss: 0.920626 - Time:282.4828624725342\n",
      "Epoch: 27 - Batch: 126912 - Loss: 1.040855 - Time:291.90997433662415\n",
      "Epoch: 27 - Batch: 131008 - Loss: 0.333599 - Time:301.31724667549133\n",
      "Epoch: 27 - Batch: 135104 - Loss: 0.346257 - Time:310.72605991363525\n",
      "Epoch: 27 - Batch: 139200 - Loss: 0.626648 - Time:320.1341121196747\n",
      "Epoch: 27 - Batch: 143296 - Loss: 0.719273 - Time:329.5419442653656\n",
      "Epoch: 27 - Batch: 147392 - Loss: 0.629143 - Time:338.94892406463623\n",
      "Epoch: 27 - Batch: 151488 - Loss: 0.966218 - Time:348.3568241596222\n",
      "Epoch: 28 - Batch: 4032 - Loss: 0.750034 - Time:9.529218912124634\n",
      "Epoch: 28 - Batch: 8128 - Loss: 0.267531 - Time:18.963340282440186\n",
      "Epoch: 28 - Batch: 12224 - Loss: 0.606078 - Time:28.37268328666687\n",
      "Epoch: 28 - Batch: 16320 - Loss: 0.127759 - Time:37.782718658447266\n",
      "Epoch: 28 - Batch: 20416 - Loss: 0.442410 - Time:47.19281220436096\n",
      "Epoch: 28 - Batch: 24512 - Loss: 0.323655 - Time:56.61804461479187\n",
      "Epoch: 28 - Batch: 28608 - Loss: 0.274784 - Time:66.02916812896729\n",
      "Epoch: 28 - Batch: 32704 - Loss: 0.305832 - Time:75.43531584739685\n",
      "Epoch: 28 - Batch: 36800 - Loss: 0.821362 - Time:84.8610737323761\n",
      "Epoch: 28 - Batch: 40896 - Loss: 0.239652 - Time:94.27041792869568\n",
      "Epoch: 28 - Batch: 44992 - Loss: 0.500106 - Time:103.68024468421936\n",
      "Epoch: 28 - Batch: 49088 - Loss: 0.848503 - Time:113.09031224250793\n",
      "Epoch: 28 - Batch: 53184 - Loss: 0.365519 - Time:122.50042581558228\n",
      "Epoch: 28 - Batch: 57280 - Loss: 0.492662 - Time:131.9103455543518\n",
      "Epoch: 28 - Batch: 61376 - Loss: 0.676580 - Time:141.31833910942078\n",
      "Epoch: 28 - Batch: 65472 - Loss: 0.529207 - Time:150.73031449317932\n",
      "Epoch: 28 - Batch: 69568 - Loss: 0.428759 - Time:160.15573048591614\n",
      "Epoch: 28 - Batch: 73664 - Loss: 0.705344 - Time:169.56269240379333\n",
      "Epoch: 28 - Batch: 77760 - Loss: 0.707033 - Time:178.9718930721283\n",
      "Epoch: 28 - Batch: 81856 - Loss: 0.966721 - Time:188.3802330493927\n",
      "Epoch: 28 - Batch: 85952 - Loss: 0.974516 - Time:197.8060462474823\n",
      "Epoch: 28 - Batch: 90048 - Loss: 0.867670 - Time:207.2158968448639\n",
      "Epoch: 28 - Batch: 94144 - Loss: 0.616610 - Time:216.6257107257843\n",
      "Epoch: 28 - Batch: 98240 - Loss: 0.573033 - Time:226.05021381378174\n",
      "Epoch: 28 - Batch: 102336 - Loss: 0.580966 - Time:235.45894479751587\n",
      "Epoch: 28 - Batch: 106432 - Loss: 0.535635 - Time:244.8671247959137\n",
      "Epoch: 28 - Batch: 110528 - Loss: 0.405580 - Time:254.27559185028076\n",
      "Epoch: 28 - Batch: 114624 - Loss: 0.619047 - Time:263.6847071647644\n",
      "Epoch: 28 - Batch: 118720 - Loss: 1.021008 - Time:273.09335255622864\n",
      "Epoch: 28 - Batch: 122816 - Loss: 0.705806 - Time:282.5025951862335\n",
      "Epoch: 28 - Batch: 126912 - Loss: 0.205628 - Time:291.91397619247437\n",
      "Epoch: 28 - Batch: 131008 - Loss: 0.331171 - Time:301.34348368644714\n",
      "Epoch: 28 - Batch: 135104 - Loss: 0.674366 - Time:310.7540349960327\n",
      "Epoch: 28 - Batch: 139200 - Loss: 0.416027 - Time:320.16413736343384\n",
      "Epoch: 28 - Batch: 143296 - Loss: 0.553798 - Time:329.5752592086792\n",
      "Epoch: 28 - Batch: 147392 - Loss: 0.566800 - Time:338.9867753982544\n",
      "Epoch: 28 - Batch: 151488 - Loss: 0.584505 - Time:348.4124732017517\n",
      "Epoch: 29 - Batch: 4032 - Loss: 0.123418 - Time:9.555359840393066\n",
      "Epoch: 29 - Batch: 8128 - Loss: 0.247279 - Time:18.993407487869263\n",
      "Epoch: 29 - Batch: 12224 - Loss: 0.593331 - Time:28.40442991256714\n",
      "Epoch: 29 - Batch: 16320 - Loss: 0.470772 - Time:37.81644415855408\n",
      "Epoch: 29 - Batch: 20416 - Loss: 0.917923 - Time:47.245399475097656\n",
      "Epoch: 29 - Batch: 24512 - Loss: 0.125652 - Time:56.65581464767456\n",
      "Epoch: 29 - Batch: 28608 - Loss: 0.496423 - Time:66.06600141525269\n",
      "Epoch: 29 - Batch: 32704 - Loss: 0.869353 - Time:75.48185086250305\n",
      "Epoch: 29 - Batch: 36800 - Loss: 0.344382 - Time:84.89152884483337\n",
      "Epoch: 29 - Batch: 40896 - Loss: 0.582939 - Time:94.30212926864624\n",
      "Epoch: 29 - Batch: 44992 - Loss: 0.580323 - Time:103.71237373352051\n",
      "Epoch: 29 - Batch: 49088 - Loss: 0.475227 - Time:113.12209558486938\n",
      "Epoch: 29 - Batch: 53184 - Loss: 0.652181 - Time:122.55103969573975\n",
      "Epoch: 29 - Batch: 57280 - Loss: 0.747247 - Time:131.96063590049744\n",
      "Epoch: 29 - Batch: 61376 - Loss: 0.172303 - Time:141.36984872817993\n",
      "Epoch: 29 - Batch: 65472 - Loss: 0.321097 - Time:150.78100180625916\n",
      "Epoch: 29 - Batch: 69568 - Loss: 0.641983 - Time:160.1912486553192\n",
      "Epoch: 29 - Batch: 73664 - Loss: 0.702355 - Time:169.62010741233826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 - Batch: 77760 - Loss: 0.457621 - Time:179.03232526779175\n",
      "Epoch: 29 - Batch: 81856 - Loss: 0.303738 - Time:188.44513487815857\n",
      "Epoch: 29 - Batch: 85952 - Loss: 0.902313 - Time:197.87432050704956\n",
      "Epoch: 29 - Batch: 90048 - Loss: 0.777470 - Time:207.28577375411987\n",
      "Epoch: 29 - Batch: 94144 - Loss: 0.272000 - Time:216.69666409492493\n",
      "Epoch: 29 - Batch: 98240 - Loss: 0.589159 - Time:226.10684442520142\n",
      "Epoch: 29 - Batch: 102336 - Loss: 0.315895 - Time:235.5181245803833\n",
      "Epoch: 29 - Batch: 106432 - Loss: 0.749128 - Time:244.9297046661377\n",
      "Epoch: 29 - Batch: 110528 - Loss: 0.284601 - Time:254.3377194404602\n",
      "Epoch: 29 - Batch: 114624 - Loss: 0.796287 - Time:263.7469711303711\n",
      "Epoch: 29 - Batch: 118720 - Loss: 0.276740 - Time:273.170618057251\n",
      "Epoch: 29 - Batch: 122816 - Loss: 1.156081 - Time:282.58070397377014\n",
      "Epoch: 29 - Batch: 126912 - Loss: 1.395823 - Time:291.9887945652008\n",
      "Epoch: 29 - Batch: 131008 - Loss: 1.056548 - Time:301.3976047039032\n",
      "Epoch: 29 - Batch: 135104 - Loss: 0.478426 - Time:310.8252923488617\n",
      "Epoch: 29 - Batch: 139200 - Loss: 0.354302 - Time:320.23608112335205\n",
      "Epoch: 29 - Batch: 143296 - Loss: 0.538298 - Time:329.64536452293396\n",
      "Epoch: 29 - Batch: 147392 - Loss: 0.704508 - Time:339.0742390155792\n",
      "Epoch: 29 - Batch: 151488 - Loss: 0.893271 - Time:348.4841892719269\n",
      "Epoch: 30 - Batch: 4032 - Loss: 0.591762 - Time:9.54942536354065\n",
      "Epoch: 30 - Batch: 8128 - Loss: 0.254499 - Time:18.96041750907898\n",
      "Epoch: 30 - Batch: 12224 - Loss: 0.433567 - Time:28.397441387176514\n",
      "Epoch: 30 - Batch: 16320 - Loss: 0.472237 - Time:37.80726671218872\n",
      "Epoch: 30 - Batch: 20416 - Loss: 0.762754 - Time:47.21740674972534\n",
      "Epoch: 30 - Batch: 24512 - Loss: 0.279819 - Time:56.62368702888489\n",
      "Epoch: 30 - Batch: 28608 - Loss: 0.370716 - Time:66.03239011764526\n",
      "Epoch: 30 - Batch: 32704 - Loss: 0.313599 - Time:75.44270062446594\n",
      "Epoch: 30 - Batch: 36800 - Loss: 0.082071 - Time:84.85164666175842\n",
      "Epoch: 30 - Batch: 40896 - Loss: 0.473601 - Time:94.25873947143555\n",
      "Epoch: 30 - Batch: 44992 - Loss: 0.213436 - Time:103.68370604515076\n",
      "Epoch: 30 - Batch: 49088 - Loss: 0.253360 - Time:113.09135818481445\n",
      "Epoch: 30 - Batch: 53184 - Loss: 0.276504 - Time:122.4980103969574\n",
      "Epoch: 30 - Batch: 57280 - Loss: 0.484632 - Time:131.90884947776794\n",
      "Epoch: 30 - Batch: 61376 - Loss: 0.472066 - Time:141.3337688446045\n",
      "Epoch: 30 - Batch: 65472 - Loss: 0.160021 - Time:150.7461552619934\n",
      "Epoch: 30 - Batch: 69568 - Loss: 0.301723 - Time:160.15545868873596\n",
      "Epoch: 30 - Batch: 73664 - Loss: 0.500671 - Time:169.57965469360352\n",
      "Epoch: 30 - Batch: 77760 - Loss: 0.243428 - Time:178.98845887184143\n",
      "Epoch: 30 - Batch: 81856 - Loss: 0.623761 - Time:188.39609742164612\n",
      "Epoch: 30 - Batch: 85952 - Loss: 0.269018 - Time:197.80521178245544\n",
      "Epoch: 30 - Batch: 90048 - Loss: 0.273551 - Time:207.2129249572754\n",
      "Epoch: 30 - Batch: 94144 - Loss: 0.858375 - Time:216.61983132362366\n",
      "Epoch: 30 - Batch: 98240 - Loss: 1.514472 - Time:226.02848076820374\n",
      "Epoch: 30 - Batch: 102336 - Loss: 0.595031 - Time:235.43718361854553\n",
      "Epoch: 30 - Batch: 106432 - Loss: 0.554272 - Time:244.86407685279846\n",
      "Epoch: 30 - Batch: 110528 - Loss: 0.507416 - Time:254.27293848991394\n",
      "Epoch: 30 - Batch: 114624 - Loss: 0.612369 - Time:263.681960105896\n",
      "Epoch: 30 - Batch: 118720 - Loss: 0.824383 - Time:273.0927405357361\n",
      "Epoch: 30 - Batch: 122816 - Loss: 0.232839 - Time:282.52165818214417\n",
      "Epoch: 30 - Batch: 126912 - Loss: 0.673218 - Time:291.9315240383148\n",
      "Epoch: 30 - Batch: 131008 - Loss: 0.557236 - Time:301.34143567085266\n",
      "Epoch: 30 - Batch: 135104 - Loss: 0.483959 - Time:310.76926350593567\n",
      "Epoch: 30 - Batch: 139200 - Loss: 0.584869 - Time:320.17825984954834\n",
      "Epoch: 30 - Batch: 143296 - Loss: 0.441356 - Time:329.58622550964355\n",
      "Epoch: 30 - Batch: 147392 - Loss: 0.226851 - Time:338.99548721313477\n",
      "Epoch: 30 - Batch: 151488 - Loss: 0.599312 - Time:348.4044439792633\n",
      "Epoch: 31 - Batch: 4032 - Loss: 0.491843 - Time:9.541398525238037\n",
      "Epoch: 31 - Batch: 8128 - Loss: 0.574351 - Time:18.97755455970764\n",
      "Epoch: 31 - Batch: 12224 - Loss: 0.539244 - Time:28.387697219848633\n",
      "Epoch: 31 - Batch: 16320 - Loss: 0.124219 - Time:37.79615640640259\n",
      "Epoch: 31 - Batch: 20416 - Loss: 0.151059 - Time:47.20416212081909\n",
      "Epoch: 31 - Batch: 24512 - Loss: 0.583434 - Time:56.61130690574646\n",
      "Epoch: 31 - Batch: 28608 - Loss: 0.352070 - Time:66.01863527297974\n",
      "Epoch: 31 - Batch: 32704 - Loss: 0.705743 - Time:75.42930221557617\n",
      "Epoch: 31 - Batch: 36800 - Loss: 0.590012 - Time:84.83762097358704\n",
      "Epoch: 31 - Batch: 40896 - Loss: 0.442344 - Time:94.26185059547424\n",
      "Epoch: 31 - Batch: 44992 - Loss: 0.340812 - Time:103.67006349563599\n",
      "Epoch: 31 - Batch: 49088 - Loss: 0.433846 - Time:113.07732462882996\n",
      "Epoch: 31 - Batch: 53184 - Loss: 0.639730 - Time:122.48563766479492\n",
      "Epoch: 31 - Batch: 57280 - Loss: 0.354926 - Time:131.9093472957611\n",
      "Epoch: 31 - Batch: 61376 - Loss: 0.185586 - Time:141.31844353675842\n",
      "Epoch: 31 - Batch: 65472 - Loss: 0.138386 - Time:150.7257137298584\n",
      "Epoch: 31 - Batch: 69568 - Loss: 0.438518 - Time:160.1495325565338\n",
      "Epoch: 31 - Batch: 73664 - Loss: 0.927601 - Time:169.55902004241943\n",
      "Epoch: 31 - Batch: 77760 - Loss: 0.602205 - Time:178.96694231033325\n",
      "Epoch: 31 - Batch: 81856 - Loss: 0.856694 - Time:188.37582182884216\n",
      "Epoch: 31 - Batch: 85952 - Loss: 0.717471 - Time:197.78809785842896\n",
      "Epoch: 31 - Batch: 90048 - Loss: 0.409575 - Time:207.19659781455994\n",
      "Epoch: 31 - Batch: 94144 - Loss: 0.520222 - Time:216.60660648345947\n",
      "Epoch: 31 - Batch: 98240 - Loss: 0.245149 - Time:226.01561403274536\n",
      "Epoch: 31 - Batch: 102336 - Loss: 0.304023 - Time:235.44098949432373\n",
      "Epoch: 31 - Batch: 106432 - Loss: 1.013461 - Time:244.8503177165985\n",
      "Epoch: 31 - Batch: 110528 - Loss: 0.492985 - Time:254.2615611553192\n",
      "Epoch: 31 - Batch: 114624 - Loss: 0.226147 - Time:263.6707353591919\n",
      "Epoch: 31 - Batch: 118720 - Loss: 0.375666 - Time:273.0984790325165\n",
      "Epoch: 31 - Batch: 122816 - Loss: 0.493782 - Time:282.5084888935089\n",
      "Epoch: 31 - Batch: 126912 - Loss: 0.201426 - Time:291.919739484787\n",
      "Epoch: 31 - Batch: 131008 - Loss: 0.604966 - Time:301.34465050697327\n",
      "Epoch: 31 - Batch: 135104 - Loss: 0.179411 - Time:310.7552011013031\n",
      "Epoch: 31 - Batch: 139200 - Loss: 0.704118 - Time:320.1648778915405\n",
      "Epoch: 31 - Batch: 143296 - Loss: 0.440340 - Time:329.5749888420105\n",
      "Epoch: 31 - Batch: 147392 - Loss: 0.200946 - Time:338.98225259780884\n",
      "Epoch: 31 - Batch: 151488 - Loss: 0.280103 - Time:348.3895494937897\n",
      "Epoch: 32 - Batch: 4032 - Loss: 0.594045 - Time:9.553472995758057\n",
      "Epoch: 32 - Batch: 8128 - Loss: 0.162557 - Time:18.992969751358032\n",
      "Epoch: 32 - Batch: 12224 - Loss: 0.331978 - Time:28.403234004974365\n",
      "Epoch: 32 - Batch: 16320 - Loss: 0.275058 - Time:37.81140470504761\n",
      "Epoch: 32 - Batch: 20416 - Loss: 0.499856 - Time:47.219417095184326\n",
      "Epoch: 32 - Batch: 24512 - Loss: 0.256333 - Time:56.630512714385986\n",
      "Epoch: 32 - Batch: 28608 - Loss: 0.311196 - Time:66.05558037757874\n",
      "Epoch: 32 - Batch: 32704 - Loss: 0.331078 - Time:75.46441841125488\n",
      "Epoch: 32 - Batch: 36800 - Loss: 0.291115 - Time:84.87374472618103\n",
      "Epoch: 32 - Batch: 40896 - Loss: 0.271965 - Time:94.29892635345459\n",
      "Epoch: 32 - Batch: 44992 - Loss: 0.156050 - Time:103.70804715156555\n",
      "Epoch: 32 - Batch: 49088 - Loss: 0.525228 - Time:113.11897397041321\n",
      "Epoch: 32 - Batch: 53184 - Loss: 0.159000 - Time:122.52792978286743\n",
      "Epoch: 32 - Batch: 57280 - Loss: 0.300904 - Time:131.93785786628723\n",
      "Epoch: 32 - Batch: 61376 - Loss: 0.330466 - Time:141.34934902191162\n",
      "Epoch: 32 - Batch: 65472 - Loss: 0.236165 - Time:150.7614917755127\n",
      "Epoch: 32 - Batch: 69568 - Loss: 0.227376 - Time:160.17382717132568\n",
      "Epoch: 32 - Batch: 73664 - Loss: 0.439874 - Time:169.60058641433716\n",
      "Epoch: 32 - Batch: 77760 - Loss: 0.081318 - Time:179.01093101501465\n",
      "Epoch: 32 - Batch: 81856 - Loss: 0.357363 - Time:188.42205119132996\n",
      "Epoch: 32 - Batch: 85952 - Loss: 0.499785 - Time:197.83326172828674\n",
      "Epoch: 32 - Batch: 90048 - Loss: 0.193370 - Time:207.26248621940613\n",
      "Epoch: 32 - Batch: 94144 - Loss: 0.411534 - Time:216.67197108268738\n",
      "Epoch: 32 - Batch: 98240 - Loss: 0.583575 - Time:226.0836832523346\n",
      "Epoch: 32 - Batch: 102336 - Loss: 0.560631 - Time:235.50950169563293\n",
      "Epoch: 32 - Batch: 106432 - Loss: 0.228048 - Time:244.91880774497986\n",
      "Epoch: 32 - Batch: 110528 - Loss: 0.440447 - Time:254.3289875984192\n",
      "Epoch: 32 - Batch: 114624 - Loss: 0.810580 - Time:263.73972368240356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 - Batch: 118720 - Loss: 0.535342 - Time:273.1490774154663\n",
      "Epoch: 32 - Batch: 122816 - Loss: 0.675609 - Time:282.5588028430939\n",
      "Epoch: 32 - Batch: 126912 - Loss: 0.112132 - Time:291.9697206020355\n",
      "Epoch: 32 - Batch: 131008 - Loss: 0.558657 - Time:301.38345885276794\n",
      "Epoch: 32 - Batch: 135104 - Loss: 0.137166 - Time:310.8101963996887\n",
      "Epoch: 32 - Batch: 139200 - Loss: 1.160895 - Time:320.22128987312317\n",
      "Epoch: 32 - Batch: 143296 - Loss: 0.443254 - Time:329.6317946910858\n",
      "Epoch: 32 - Batch: 147392 - Loss: 0.389162 - Time:339.04003524780273\n",
      "Epoch: 32 - Batch: 151488 - Loss: 0.254893 - Time:348.4653513431549\n",
      "Epoch: 33 - Batch: 4032 - Loss: 0.344736 - Time:9.532726526260376\n",
      "Epoch: 33 - Batch: 8128 - Loss: 0.242976 - Time:18.944024324417114\n",
      "Epoch: 33 - Batch: 12224 - Loss: 0.595148 - Time:28.38262939453125\n",
      "Epoch: 33 - Batch: 16320 - Loss: 0.066919 - Time:37.79391026496887\n",
      "Epoch: 33 - Batch: 20416 - Loss: 0.110766 - Time:47.206270933151245\n",
      "Epoch: 33 - Batch: 24512 - Loss: 0.432509 - Time:56.6367290019989\n",
      "Epoch: 33 - Batch: 28608 - Loss: 0.085740 - Time:66.05112051963806\n",
      "Epoch: 33 - Batch: 32704 - Loss: 0.425737 - Time:75.46500015258789\n",
      "Epoch: 33 - Batch: 36800 - Loss: 0.264781 - Time:84.87870860099792\n",
      "Epoch: 33 - Batch: 40896 - Loss: 0.518617 - Time:94.29203414916992\n",
      "Epoch: 33 - Batch: 44992 - Loss: 0.279947 - Time:103.7051088809967\n",
      "Epoch: 33 - Batch: 49088 - Loss: 0.332217 - Time:113.11921668052673\n",
      "Epoch: 33 - Batch: 53184 - Loss: 0.490780 - Time:122.53423428535461\n",
      "Epoch: 33 - Batch: 57280 - Loss: 0.539405 - Time:131.96180415153503\n",
      "Epoch: 33 - Batch: 61376 - Loss: 0.329696 - Time:141.3730549812317\n",
      "Epoch: 33 - Batch: 65472 - Loss: 0.303909 - Time:150.78335165977478\n",
      "Epoch: 33 - Batch: 69568 - Loss: 0.610549 - Time:160.19370102882385\n",
      "Epoch: 33 - Batch: 73664 - Loss: 0.780494 - Time:169.60361671447754\n",
      "Epoch: 33 - Batch: 77760 - Loss: 0.221148 - Time:179.0330765247345\n",
      "Epoch: 33 - Batch: 81856 - Loss: 0.617394 - Time:188.44691252708435\n",
      "Epoch: 33 - Batch: 85952 - Loss: 0.694038 - Time:197.87721705436707\n",
      "Epoch: 33 - Batch: 90048 - Loss: 0.755752 - Time:207.2918245792389\n",
      "Epoch: 33 - Batch: 94144 - Loss: 0.246569 - Time:216.70363068580627\n",
      "Epoch: 33 - Batch: 98240 - Loss: 0.447922 - Time:226.1176722049713\n",
      "Epoch: 33 - Batch: 102336 - Loss: 0.407362 - Time:235.5321500301361\n",
      "Epoch: 33 - Batch: 106432 - Loss: 0.591033 - Time:244.947829246521\n",
      "Epoch: 33 - Batch: 110528 - Loss: 0.184653 - Time:254.3600172996521\n",
      "Epoch: 33 - Batch: 114624 - Loss: 0.756917 - Time:263.77616000175476\n",
      "Epoch: 33 - Batch: 118720 - Loss: 0.717802 - Time:273.18849325180054\n",
      "Epoch: 33 - Batch: 122816 - Loss: 0.495316 - Time:282.61791038513184\n",
      "Epoch: 33 - Batch: 126912 - Loss: 0.454119 - Time:292.0301921367645\n",
      "Epoch: 33 - Batch: 131008 - Loss: 0.435179 - Time:301.43971133232117\n",
      "Epoch: 33 - Batch: 135104 - Loss: 0.623565 - Time:310.8499448299408\n",
      "Epoch: 33 - Batch: 139200 - Loss: 0.593209 - Time:320.2751865386963\n",
      "Epoch: 33 - Batch: 143296 - Loss: 0.949999 - Time:329.68631768226624\n",
      "Epoch: 33 - Batch: 147392 - Loss: 0.525829 - Time:339.0972411632538\n",
      "Epoch: 33 - Batch: 151488 - Loss: 0.295459 - Time:348.5285909175873\n",
      "Epoch: 34 - Batch: 4032 - Loss: 0.206599 - Time:9.532168626785278\n",
      "Epoch: 34 - Batch: 8128 - Loss: 0.507966 - Time:18.94173240661621\n",
      "Epoch: 34 - Batch: 12224 - Loss: 0.330059 - Time:28.378727912902832\n",
      "Epoch: 34 - Batch: 16320 - Loss: 0.181836 - Time:37.791513204574585\n",
      "Epoch: 34 - Batch: 20416 - Loss: 0.775869 - Time:47.20273518562317\n",
      "Epoch: 34 - Batch: 24512 - Loss: 0.490174 - Time:56.614229679107666\n",
      "Epoch: 34 - Batch: 28608 - Loss: 0.648880 - Time:66.02342581748962\n",
      "Epoch: 34 - Batch: 32704 - Loss: 0.284034 - Time:75.43205976486206\n",
      "Epoch: 34 - Batch: 36800 - Loss: 0.138940 - Time:84.84319114685059\n",
      "Epoch: 34 - Batch: 40896 - Loss: 0.286849 - Time:94.25289344787598\n",
      "Epoch: 34 - Batch: 44992 - Loss: 0.214831 - Time:103.6794695854187\n",
      "Epoch: 34 - Batch: 49088 - Loss: 0.122119 - Time:113.09033203125\n",
      "Epoch: 34 - Batch: 53184 - Loss: 0.468013 - Time:122.50097012519836\n",
      "Epoch: 34 - Batch: 57280 - Loss: 0.410281 - Time:131.91219973564148\n",
      "Epoch: 34 - Batch: 61376 - Loss: 0.664528 - Time:141.3383433818817\n",
      "Epoch: 34 - Batch: 65472 - Loss: 0.142316 - Time:150.7489504814148\n",
      "Epoch: 34 - Batch: 69568 - Loss: 0.404436 - Time:160.16154193878174\n",
      "Epoch: 34 - Batch: 73664 - Loss: 0.476216 - Time:169.58670663833618\n",
      "Epoch: 34 - Batch: 77760 - Loss: 0.904651 - Time:178.99612498283386\n",
      "Epoch: 34 - Batch: 81856 - Loss: 0.317555 - Time:188.40584802627563\n",
      "Epoch: 34 - Batch: 85952 - Loss: 0.513036 - Time:197.8167655467987\n",
      "Epoch: 34 - Batch: 90048 - Loss: 0.640443 - Time:207.22810578346252\n",
      "Epoch: 34 - Batch: 94144 - Loss: 0.352216 - Time:216.6385052204132\n",
      "Epoch: 34 - Batch: 98240 - Loss: 0.113193 - Time:226.04935574531555\n",
      "Epoch: 34 - Batch: 102336 - Loss: 0.607203 - Time:235.45787596702576\n",
      "Epoch: 34 - Batch: 106432 - Loss: 0.607765 - Time:244.88347220420837\n",
      "Epoch: 34 - Batch: 110528 - Loss: 0.782124 - Time:254.29215478897095\n",
      "Epoch: 34 - Batch: 114624 - Loss: 0.791044 - Time:263.70281076431274\n",
      "Epoch: 34 - Batch: 118720 - Loss: 0.393845 - Time:273.1130311489105\n",
      "Epoch: 34 - Batch: 122816 - Loss: 0.446740 - Time:282.53927636146545\n",
      "Epoch: 34 - Batch: 126912 - Loss: 0.907767 - Time:291.94748854637146\n",
      "Epoch: 34 - Batch: 131008 - Loss: 0.362234 - Time:301.35721492767334\n",
      "Epoch: 34 - Batch: 135104 - Loss: 0.499606 - Time:310.7816026210785\n",
      "Epoch: 34 - Batch: 139200 - Loss: 0.457426 - Time:320.1922068595886\n",
      "Epoch: 34 - Batch: 143296 - Loss: 0.505907 - Time:329.6022093296051\n",
      "Epoch: 34 - Batch: 147392 - Loss: 0.585733 - Time:339.01018929481506\n",
      "Epoch: 34 - Batch: 151488 - Loss: 0.361765 - Time:348.418559551239\n",
      "Epoch: 35 - Batch: 4032 - Loss: 0.424231 - Time:9.567829847335815\n",
      "Epoch: 35 - Batch: 8128 - Loss: 0.269254 - Time:19.00734806060791\n",
      "Epoch: 35 - Batch: 12224 - Loss: 0.342701 - Time:28.41709542274475\n",
      "Epoch: 35 - Batch: 16320 - Loss: 0.092702 - Time:37.82728695869446\n",
      "Epoch: 35 - Batch: 20416 - Loss: 0.258358 - Time:47.235615253448486\n",
      "Epoch: 35 - Batch: 24512 - Loss: 0.639666 - Time:56.64532423019409\n",
      "Epoch: 35 - Batch: 28608 - Loss: 0.078199 - Time:66.05461978912354\n",
      "Epoch: 35 - Batch: 32704 - Loss: 0.134233 - Time:75.4628005027771\n",
      "Epoch: 35 - Batch: 36800 - Loss: 0.331695 - Time:84.86859369277954\n",
      "Epoch: 35 - Batch: 40896 - Loss: 0.224983 - Time:94.29467797279358\n",
      "Epoch: 35 - Batch: 44992 - Loss: 0.097950 - Time:103.70316672325134\n",
      "Epoch: 35 - Batch: 49088 - Loss: 0.332474 - Time:113.11181879043579\n",
      "Epoch: 35 - Batch: 53184 - Loss: 0.761257 - Time:122.52244591712952\n",
      "Epoch: 35 - Batch: 57280 - Loss: 0.413736 - Time:131.94984221458435\n",
      "Epoch: 35 - Batch: 61376 - Loss: 0.265323 - Time:141.35817408561707\n",
      "Epoch: 35 - Batch: 65472 - Loss: 0.147923 - Time:150.76718258857727\n",
      "Epoch: 35 - Batch: 69568 - Loss: 0.609725 - Time:160.19585943222046\n",
      "Epoch: 35 - Batch: 73664 - Loss: 0.485073 - Time:169.6039764881134\n",
      "Epoch: 35 - Batch: 77760 - Loss: 0.585450 - Time:179.01363039016724\n",
      "Epoch: 35 - Batch: 81856 - Loss: 0.175757 - Time:188.42581629753113\n",
      "Epoch: 35 - Batch: 85952 - Loss: 0.131420 - Time:197.83569049835205\n",
      "Epoch: 35 - Batch: 90048 - Loss: 0.602065 - Time:207.24600195884705\n",
      "Epoch: 35 - Batch: 94144 - Loss: 0.590746 - Time:216.65899753570557\n",
      "Epoch: 35 - Batch: 98240 - Loss: 0.242974 - Time:226.06874418258667\n",
      "Epoch: 35 - Batch: 102336 - Loss: 0.609247 - Time:235.49511289596558\n",
      "Epoch: 35 - Batch: 106432 - Loss: 0.705878 - Time:244.90775775909424\n",
      "Epoch: 35 - Batch: 110528 - Loss: 0.433898 - Time:254.31818556785583\n",
      "Epoch: 35 - Batch: 114624 - Loss: 0.192906 - Time:263.7277421951294\n",
      "Epoch: 35 - Batch: 118720 - Loss: 0.304952 - Time:273.1557049751282\n",
      "Epoch: 35 - Batch: 122816 - Loss: 0.725442 - Time:282.5686285495758\n",
      "Epoch: 35 - Batch: 126912 - Loss: 0.100410 - Time:291.97979736328125\n",
      "Epoch: 35 - Batch: 131008 - Loss: 0.615409 - Time:301.4065110683441\n",
      "Epoch: 35 - Batch: 135104 - Loss: 0.244613 - Time:310.8180959224701\n",
      "Epoch: 35 - Batch: 139200 - Loss: 0.524438 - Time:320.2282519340515\n",
      "Epoch: 35 - Batch: 143296 - Loss: 0.097457 - Time:329.6369173526764\n",
      "Epoch: 35 - Batch: 147392 - Loss: 0.471247 - Time:339.0472218990326\n",
      "Epoch: 35 - Batch: 151488 - Loss: 0.715452 - Time:348.4574749469757\n",
      "Epoch: 36 - Batch: 4032 - Loss: 0.407571 - Time:9.532692432403564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 - Batch: 8128 - Loss: 0.108801 - Time:18.945505380630493\n",
      "Epoch: 36 - Batch: 12224 - Loss: 0.298256 - Time:28.383621215820312\n",
      "Epoch: 36 - Batch: 16320 - Loss: 0.226277 - Time:37.79402184486389\n",
      "Epoch: 36 - Batch: 20416 - Loss: 0.419418 - Time:47.204577922821045\n",
      "Epoch: 36 - Batch: 24512 - Loss: 0.261406 - Time:56.61335802078247\n",
      "Epoch: 36 - Batch: 28608 - Loss: 0.189324 - Time:66.03924083709717\n",
      "Epoch: 36 - Batch: 32704 - Loss: 0.345564 - Time:75.4501645565033\n",
      "Epoch: 36 - Batch: 36800 - Loss: 0.361558 - Time:84.85900378227234\n",
      "Epoch: 36 - Batch: 40896 - Loss: 0.223262 - Time:94.28480219841003\n",
      "Epoch: 36 - Batch: 44992 - Loss: 0.161893 - Time:103.69429659843445\n",
      "Epoch: 36 - Batch: 49088 - Loss: 0.271316 - Time:113.10450291633606\n",
      "Epoch: 36 - Batch: 53184 - Loss: 0.081278 - Time:122.51450204849243\n",
      "Epoch: 36 - Batch: 57280 - Loss: 0.091831 - Time:131.92372751235962\n",
      "Epoch: 36 - Batch: 61376 - Loss: 0.122419 - Time:141.33388686180115\n",
      "Epoch: 36 - Batch: 65472 - Loss: 0.684980 - Time:150.74609684944153\n",
      "Epoch: 36 - Batch: 69568 - Loss: 0.601164 - Time:160.15570545196533\n",
      "Epoch: 36 - Batch: 73664 - Loss: 0.438598 - Time:169.57876682281494\n",
      "Epoch: 36 - Batch: 77760 - Loss: 0.541895 - Time:178.9872727394104\n",
      "Epoch: 36 - Batch: 81856 - Loss: 0.333607 - Time:188.395512342453\n",
      "Epoch: 36 - Batch: 85952 - Loss: 0.496281 - Time:197.80485463142395\n",
      "Epoch: 36 - Batch: 90048 - Loss: 0.192227 - Time:207.2318332195282\n",
      "Epoch: 36 - Batch: 94144 - Loss: 0.331494 - Time:216.63986945152283\n",
      "Epoch: 36 - Batch: 98240 - Loss: 0.826564 - Time:226.0492868423462\n",
      "Epoch: 36 - Batch: 102336 - Loss: 0.406948 - Time:235.47599077224731\n",
      "Epoch: 36 - Batch: 106432 - Loss: 0.411656 - Time:244.8850417137146\n",
      "Epoch: 36 - Batch: 110528 - Loss: 0.276158 - Time:254.29626607894897\n",
      "Epoch: 36 - Batch: 114624 - Loss: 0.421224 - Time:263.70593786239624\n",
      "Epoch: 36 - Batch: 118720 - Loss: 0.239851 - Time:273.1162474155426\n",
      "Epoch: 36 - Batch: 122816 - Loss: 0.228970 - Time:282.5248010158539\n",
      "Epoch: 36 - Batch: 126912 - Loss: 0.240626 - Time:291.93367552757263\n",
      "Epoch: 36 - Batch: 131008 - Loss: 0.572961 - Time:301.34262108802795\n",
      "Epoch: 36 - Batch: 135104 - Loss: 0.271525 - Time:310.76817536354065\n",
      "Epoch: 36 - Batch: 139200 - Loss: 0.947192 - Time:320.178542137146\n",
      "Epoch: 36 - Batch: 143296 - Loss: 0.315507 - Time:329.58923745155334\n",
      "Epoch: 36 - Batch: 147392 - Loss: 0.918188 - Time:338.9980342388153\n",
      "Epoch: 36 - Batch: 151488 - Loss: 0.158047 - Time:348.4247922897339\n",
      "Epoch: 37 - Batch: 4032 - Loss: 0.662274 - Time:9.533210039138794\n",
      "Epoch: 37 - Batch: 8128 - Loss: 0.496882 - Time:18.943856239318848\n",
      "Epoch: 37 - Batch: 12224 - Loss: 0.261692 - Time:28.357484579086304\n",
      "Epoch: 37 - Batch: 16320 - Loss: 0.075662 - Time:37.79649996757507\n",
      "Epoch: 37 - Batch: 20416 - Loss: 0.204486 - Time:47.20854043960571\n",
      "Epoch: 37 - Batch: 24512 - Loss: 0.244069 - Time:56.63690137863159\n",
      "Epoch: 37 - Batch: 28608 - Loss: 0.255836 - Time:66.04579257965088\n",
      "Epoch: 37 - Batch: 32704 - Loss: 0.093765 - Time:75.45610117912292\n",
      "Epoch: 37 - Batch: 36800 - Loss: 0.203023 - Time:84.86813449859619\n",
      "Epoch: 37 - Batch: 40896 - Loss: 0.545158 - Time:94.27685594558716\n",
      "Epoch: 37 - Batch: 44992 - Loss: 0.140676 - Time:103.68521165847778\n",
      "Epoch: 37 - Batch: 49088 - Loss: 0.241084 - Time:113.09598875045776\n",
      "Epoch: 37 - Batch: 53184 - Loss: 0.206928 - Time:122.50698447227478\n",
      "Epoch: 37 - Batch: 57280 - Loss: 0.061026 - Time:131.91770887374878\n",
      "Epoch: 37 - Batch: 61376 - Loss: 0.443145 - Time:141.3451452255249\n",
      "Epoch: 37 - Batch: 65472 - Loss: 0.112782 - Time:150.756489276886\n",
      "Epoch: 37 - Batch: 69568 - Loss: 0.269636 - Time:160.16611409187317\n",
      "Epoch: 37 - Batch: 73664 - Loss: 0.349477 - Time:169.57805824279785\n",
      "Epoch: 37 - Batch: 77760 - Loss: 0.296121 - Time:179.0060305595398\n",
      "Epoch: 37 - Batch: 81856 - Loss: 0.396270 - Time:188.41562128067017\n",
      "Epoch: 37 - Batch: 85952 - Loss: 0.482660 - Time:197.8259768486023\n",
      "Epoch: 37 - Batch: 90048 - Loss: 0.296712 - Time:207.25289225578308\n",
      "Epoch: 37 - Batch: 94144 - Loss: 0.347024 - Time:216.6609284877777\n",
      "Epoch: 37 - Batch: 98240 - Loss: 0.435305 - Time:226.0744869709015\n",
      "Epoch: 37 - Batch: 102336 - Loss: 0.555357 - Time:235.48571753501892\n",
      "Epoch: 37 - Batch: 106432 - Loss: 0.149113 - Time:244.8956823348999\n",
      "Epoch: 37 - Batch: 110528 - Loss: 0.648444 - Time:254.30657291412354\n",
      "Epoch: 37 - Batch: 114624 - Loss: 0.118425 - Time:263.7187821865082\n",
      "Epoch: 37 - Batch: 118720 - Loss: 0.359523 - Time:273.13079738616943\n",
      "Epoch: 37 - Batch: 122816 - Loss: 0.849704 - Time:282.5592544078827\n",
      "Epoch: 37 - Batch: 126912 - Loss: 0.417385 - Time:291.9711170196533\n",
      "Epoch: 37 - Batch: 131008 - Loss: 0.247197 - Time:301.3821711540222\n",
      "Epoch: 37 - Batch: 135104 - Loss: 0.337005 - Time:310.79243659973145\n",
      "Epoch: 37 - Batch: 139200 - Loss: 0.276211 - Time:320.2202715873718\n",
      "Epoch: 37 - Batch: 143296 - Loss: 0.642805 - Time:329.6311192512512\n",
      "Epoch: 37 - Batch: 147392 - Loss: 0.413660 - Time:339.04172825813293\n",
      "Epoch: 37 - Batch: 151488 - Loss: 0.441964 - Time:348.4697525501251\n",
      "Epoch: 38 - Batch: 4032 - Loss: 0.390189 - Time:9.549381971359253\n",
      "Epoch: 38 - Batch: 8128 - Loss: 0.356277 - Time:18.95833110809326\n",
      "Epoch: 38 - Batch: 12224 - Loss: 0.064132 - Time:28.394239902496338\n",
      "Epoch: 38 - Batch: 16320 - Loss: 0.184161 - Time:37.804120779037476\n",
      "Epoch: 38 - Batch: 20416 - Loss: 0.426082 - Time:47.21363162994385\n",
      "Epoch: 38 - Batch: 24512 - Loss: 0.531303 - Time:56.62599778175354\n",
      "Epoch: 38 - Batch: 28608 - Loss: 0.059384 - Time:66.04387879371643\n",
      "Epoch: 38 - Batch: 32704 - Loss: 0.453245 - Time:75.45563578605652\n",
      "Epoch: 38 - Batch: 36800 - Loss: 0.185326 - Time:84.86827373504639\n",
      "Epoch: 38 - Batch: 40896 - Loss: 0.259055 - Time:94.28018474578857\n",
      "Epoch: 38 - Batch: 44992 - Loss: 0.119035 - Time:103.70715880393982\n",
      "Epoch: 38 - Batch: 49088 - Loss: 0.375546 - Time:113.11761093139648\n",
      "Epoch: 38 - Batch: 53184 - Loss: 0.446419 - Time:122.52850127220154\n",
      "Epoch: 38 - Batch: 57280 - Loss: 0.143885 - Time:131.93917155265808\n",
      "Epoch: 38 - Batch: 61376 - Loss: 0.227458 - Time:141.3686649799347\n",
      "Epoch: 38 - Batch: 65472 - Loss: 0.231020 - Time:150.77919578552246\n",
      "Epoch: 38 - Batch: 69568 - Loss: 0.516147 - Time:160.1870572566986\n",
      "Epoch: 38 - Batch: 73664 - Loss: 0.497203 - Time:169.61159920692444\n",
      "Epoch: 38 - Batch: 77760 - Loss: 0.243514 - Time:179.02069854736328\n",
      "Epoch: 38 - Batch: 81856 - Loss: 0.310675 - Time:188.43217825889587\n",
      "Epoch: 38 - Batch: 85952 - Loss: 0.225002 - Time:197.84193205833435\n",
      "Epoch: 38 - Batch: 90048 - Loss: 0.221457 - Time:207.25121474266052\n",
      "Epoch: 38 - Batch: 94144 - Loss: 0.337718 - Time:216.6636679172516\n",
      "Epoch: 38 - Batch: 98240 - Loss: 0.201589 - Time:226.0725803375244\n",
      "Epoch: 38 - Batch: 102336 - Loss: 0.482959 - Time:235.48509240150452\n",
      "Epoch: 38 - Batch: 106432 - Loss: 0.310384 - Time:244.9121356010437\n",
      "Epoch: 38 - Batch: 110528 - Loss: 0.443338 - Time:254.32054257392883\n",
      "Epoch: 38 - Batch: 114624 - Loss: 0.105240 - Time:263.73048853874207\n",
      "Epoch: 38 - Batch: 118720 - Loss: 0.531239 - Time:273.1412992477417\n",
      "Epoch: 38 - Batch: 122816 - Loss: 0.513873 - Time:282.5517098903656\n",
      "Epoch: 38 - Batch: 126912 - Loss: 0.406304 - Time:291.9784126281738\n",
      "Epoch: 38 - Batch: 131008 - Loss: 0.676565 - Time:301.3911859989166\n",
      "Epoch: 38 - Batch: 135104 - Loss: 0.355099 - Time:310.80413031578064\n",
      "Epoch: 38 - Batch: 139200 - Loss: 0.551462 - Time:320.23462748527527\n",
      "Epoch: 38 - Batch: 143296 - Loss: 0.413752 - Time:329.6447560787201\n",
      "Epoch: 38 - Batch: 147392 - Loss: 0.566743 - Time:339.05707812309265\n",
      "Epoch: 38 - Batch: 151488 - Loss: 0.250816 - Time:348.4716124534607\n",
      "Epoch: 39 - Batch: 4032 - Loss: 0.611675 - Time:9.55199122428894\n",
      "Epoch: 39 - Batch: 8128 - Loss: 0.117960 - Time:18.98772668838501\n",
      "Epoch: 39 - Batch: 12224 - Loss: 0.466570 - Time:28.395378351211548\n",
      "Epoch: 39 - Batch: 16320 - Loss: 0.235190 - Time:37.803680419921875\n",
      "Epoch: 39 - Batch: 20416 - Loss: 0.506787 - Time:47.20975112915039\n",
      "Epoch: 39 - Batch: 24512 - Loss: 0.073795 - Time:56.61773228645325\n",
      "Epoch: 39 - Batch: 28608 - Loss: 0.177304 - Time:66.02790451049805\n",
      "Epoch: 39 - Batch: 32704 - Loss: 0.243900 - Time:75.43742680549622\n",
      "Epoch: 39 - Batch: 36800 - Loss: 0.616264 - Time:84.8464560508728\n",
      "Epoch: 39 - Batch: 40896 - Loss: 0.250264 - Time:94.26917695999146\n",
      "Epoch: 39 - Batch: 44992 - Loss: 0.477299 - Time:103.67641520500183\n",
      "Epoch: 39 - Batch: 49088 - Loss: 0.227470 - Time:113.08350276947021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 - Batch: 53184 - Loss: 0.155559 - Time:122.49054050445557\n",
      "Epoch: 39 - Batch: 57280 - Loss: 0.132241 - Time:131.9167082309723\n",
      "Epoch: 39 - Batch: 61376 - Loss: 0.386727 - Time:141.3252613544464\n",
      "Epoch: 39 - Batch: 65472 - Loss: 0.070879 - Time:150.73263382911682\n",
      "Epoch: 39 - Batch: 69568 - Loss: 0.515747 - Time:160.15639996528625\n",
      "Epoch: 39 - Batch: 73664 - Loss: 0.203336 - Time:169.5642855167389\n",
      "Epoch: 39 - Batch: 77760 - Loss: 0.518242 - Time:178.97021651268005\n",
      "Epoch: 39 - Batch: 81856 - Loss: 0.158121 - Time:188.37719583511353\n",
      "Epoch: 39 - Batch: 85952 - Loss: 0.077965 - Time:197.78525185585022\n",
      "Epoch: 39 - Batch: 90048 - Loss: 0.372340 - Time:207.19203853607178\n",
      "Epoch: 39 - Batch: 94144 - Loss: 0.261585 - Time:216.60058426856995\n",
      "Epoch: 39 - Batch: 98240 - Loss: 0.146862 - Time:226.00933265686035\n",
      "Epoch: 39 - Batch: 102336 - Loss: 0.281568 - Time:235.4332399368286\n",
      "Epoch: 39 - Batch: 106432 - Loss: 0.190551 - Time:244.842031955719\n",
      "Epoch: 39 - Batch: 110528 - Loss: 0.084416 - Time:254.25004768371582\n",
      "Epoch: 39 - Batch: 114624 - Loss: 0.112250 - Time:263.65870571136475\n",
      "Epoch: 39 - Batch: 118720 - Loss: 0.317217 - Time:273.0837287902832\n",
      "Epoch: 39 - Batch: 122816 - Loss: 0.248974 - Time:282.4941234588623\n",
      "Epoch: 39 - Batch: 126912 - Loss: 0.434180 - Time:291.90062856674194\n",
      "Epoch: 39 - Batch: 131008 - Loss: 0.583986 - Time:301.3254773616791\n",
      "Epoch: 39 - Batch: 135104 - Loss: 0.357752 - Time:310.7328221797943\n",
      "Epoch: 39 - Batch: 139200 - Loss: 0.423567 - Time:320.13798356056213\n",
      "Epoch: 39 - Batch: 143296 - Loss: 0.277918 - Time:329.54542326927185\n",
      "Epoch: 39 - Batch: 147392 - Loss: 0.453299 - Time:338.95252752304077\n",
      "Epoch: 39 - Batch: 151488 - Loss: 0.456065 - Time:348.36168122291565\n",
      "Epoch: 40 - Batch: 4032 - Loss: 0.471765 - Time:9.538381576538086\n",
      "Epoch: 40 - Batch: 8128 - Loss: 0.254375 - Time:18.950054168701172\n",
      "Epoch: 40 - Batch: 12224 - Loss: 0.212553 - Time:28.388015270233154\n",
      "Epoch: 40 - Batch: 16320 - Loss: 0.415106 - Time:37.79818534851074\n",
      "Epoch: 40 - Batch: 20416 - Loss: 0.081526 - Time:47.20905137062073\n",
      "Epoch: 40 - Batch: 24512 - Loss: 0.217514 - Time:56.622323989868164\n",
      "Epoch: 40 - Batch: 28608 - Loss: 0.384436 - Time:66.05217838287354\n",
      "Epoch: 40 - Batch: 32704 - Loss: 0.216507 - Time:75.4625198841095\n",
      "Epoch: 40 - Batch: 36800 - Loss: 0.089261 - Time:84.8746907711029\n",
      "Epoch: 40 - Batch: 40896 - Loss: 0.998262 - Time:94.30327033996582\n",
      "Epoch: 40 - Batch: 44992 - Loss: 0.285087 - Time:103.7138831615448\n",
      "Epoch: 40 - Batch: 49088 - Loss: 0.156314 - Time:113.12387752532959\n",
      "Epoch: 40 - Batch: 53184 - Loss: 0.420475 - Time:122.53658652305603\n",
      "Epoch: 40 - Batch: 57280 - Loss: 0.356844 - Time:131.94749093055725\n",
      "Epoch: 40 - Batch: 61376 - Loss: 0.417179 - Time:141.35677075386047\n",
      "Epoch: 40 - Batch: 65472 - Loss: 0.195957 - Time:150.7691526412964\n",
      "Epoch: 40 - Batch: 69568 - Loss: 0.337207 - Time:160.17891573905945\n",
      "Epoch: 40 - Batch: 73664 - Loss: 0.105141 - Time:169.60442209243774\n",
      "Epoch: 40 - Batch: 77760 - Loss: 0.067347 - Time:179.0147624015808\n",
      "Epoch: 40 - Batch: 81856 - Loss: 0.268627 - Time:188.42507791519165\n",
      "Epoch: 40 - Batch: 85952 - Loss: 0.304763 - Time:197.83523297309875\n",
      "Epoch: 40 - Batch: 90048 - Loss: 0.429508 - Time:207.26326942443848\n",
      "Epoch: 40 - Batch: 94144 - Loss: 0.382500 - Time:216.67274951934814\n",
      "Epoch: 40 - Batch: 98240 - Loss: 0.161223 - Time:226.0847680568695\n",
      "Epoch: 40 - Batch: 102336 - Loss: 0.110535 - Time:235.51323652267456\n",
      "Epoch: 40 - Batch: 106432 - Loss: 0.321303 - Time:244.92713856697083\n",
      "Epoch: 40 - Batch: 110528 - Loss: 0.401015 - Time:254.33777379989624\n",
      "Epoch: 40 - Batch: 114624 - Loss: 0.796861 - Time:263.7488377094269\n",
      "Epoch: 40 - Batch: 118720 - Loss: 0.470813 - Time:273.161696434021\n",
      "Epoch: 40 - Batch: 122816 - Loss: 0.623453 - Time:282.5715410709381\n",
      "Epoch: 40 - Batch: 126912 - Loss: 0.425321 - Time:291.9831442832947\n",
      "Epoch: 40 - Batch: 131008 - Loss: 0.360678 - Time:301.39354634284973\n",
      "Epoch: 40 - Batch: 135104 - Loss: 0.211151 - Time:310.82279419898987\n",
      "Epoch: 40 - Batch: 139200 - Loss: 0.190733 - Time:320.23489212989807\n",
      "Epoch: 40 - Batch: 143296 - Loss: 0.291323 - Time:329.6460723876953\n",
      "Epoch: 40 - Batch: 147392 - Loss: 0.465824 - Time:339.0569415092468\n",
      "Epoch: 40 - Batch: 151488 - Loss: 0.640109 - Time:348.48459124565125\n",
      "Epoch: 41 - Batch: 4032 - Loss: 0.422417 - Time:9.551785469055176\n",
      "Epoch: 41 - Batch: 8128 - Loss: 0.356293 - Time:18.96527123451233\n",
      "Epoch: 41 - Batch: 12224 - Loss: 0.103025 - Time:28.377553939819336\n",
      "Epoch: 41 - Batch: 16320 - Loss: 0.395574 - Time:37.81685400009155\n",
      "Epoch: 41 - Batch: 20416 - Loss: 0.128024 - Time:47.22922444343567\n",
      "Epoch: 41 - Batch: 24512 - Loss: 0.454291 - Time:56.639039516448975\n",
      "Epoch: 41 - Batch: 28608 - Loss: 0.290019 - Time:66.06651163101196\n",
      "Epoch: 41 - Batch: 32704 - Loss: 0.248411 - Time:75.47557425498962\n",
      "Epoch: 41 - Batch: 36800 - Loss: 0.498604 - Time:84.88631939888\n",
      "Epoch: 41 - Batch: 40896 - Loss: 0.625058 - Time:94.29505228996277\n",
      "Epoch: 41 - Batch: 44992 - Loss: 0.236355 - Time:103.70543241500854\n",
      "Epoch: 41 - Batch: 49088 - Loss: 0.269265 - Time:113.11476492881775\n",
      "Epoch: 41 - Batch: 53184 - Loss: 0.249905 - Time:122.52670454978943\n",
      "Epoch: 41 - Batch: 57280 - Loss: 0.217358 - Time:131.93798804283142\n",
      "Epoch: 41 - Batch: 61376 - Loss: 0.250830 - Time:141.36763763427734\n",
      "Epoch: 41 - Batch: 65472 - Loss: 0.433308 - Time:150.77868676185608\n",
      "Epoch: 41 - Batch: 69568 - Loss: 0.949110 - Time:160.18745923042297\n",
      "Epoch: 41 - Batch: 73664 - Loss: 0.155077 - Time:169.5981261730194\n",
      "Epoch: 41 - Batch: 77760 - Loss: 0.231381 - Time:179.02498960494995\n",
      "Epoch: 41 - Batch: 81856 - Loss: 0.322336 - Time:188.43569612503052\n",
      "Epoch: 41 - Batch: 85952 - Loss: 0.216593 - Time:197.8472888469696\n",
      "Epoch: 41 - Batch: 90048 - Loss: 0.320767 - Time:207.27550292015076\n",
      "Epoch: 41 - Batch: 94144 - Loss: 0.297736 - Time:216.68685626983643\n",
      "Epoch: 41 - Batch: 98240 - Loss: 0.512173 - Time:226.09773588180542\n",
      "Epoch: 41 - Batch: 102336 - Loss: 0.445002 - Time:235.50988221168518\n",
      "Epoch: 41 - Batch: 106432 - Loss: 0.156951 - Time:244.92157316207886\n",
      "Epoch: 41 - Batch: 110528 - Loss: 0.183190 - Time:254.33008098602295\n",
      "Epoch: 41 - Batch: 114624 - Loss: 0.204021 - Time:263.7382378578186\n",
      "Epoch: 41 - Batch: 118720 - Loss: 0.401797 - Time:273.1471152305603\n",
      "Epoch: 41 - Batch: 122816 - Loss: 0.428835 - Time:282.574524641037\n",
      "Epoch: 41 - Batch: 126912 - Loss: 0.321784 - Time:291.98279094696045\n",
      "Epoch: 41 - Batch: 131008 - Loss: 0.618878 - Time:301.3924262523651\n",
      "Epoch: 41 - Batch: 135104 - Loss: 0.090168 - Time:310.8025186061859\n",
      "Epoch: 41 - Batch: 139200 - Loss: 0.604440 - Time:320.229692697525\n",
      "Epoch: 41 - Batch: 143296 - Loss: 0.558519 - Time:329.6387689113617\n",
      "Epoch: 41 - Batch: 147392 - Loss: 0.060537 - Time:339.0478870868683\n",
      "Epoch: 41 - Batch: 151488 - Loss: 0.650857 - Time:348.4733021259308\n",
      "Epoch: 42 - Batch: 4032 - Loss: 0.416862 - Time:9.539557456970215\n",
      "Epoch: 42 - Batch: 8128 - Loss: 0.282493 - Time:18.949851989746094\n",
      "Epoch: 42 - Batch: 12224 - Loss: 0.211329 - Time:28.385380506515503\n",
      "Epoch: 42 - Batch: 16320 - Loss: 0.122071 - Time:37.79296684265137\n",
      "Epoch: 42 - Batch: 20416 - Loss: 0.187469 - Time:47.20143103599548\n",
      "Epoch: 42 - Batch: 24512 - Loss: 0.408288 - Time:56.610366344451904\n",
      "Epoch: 42 - Batch: 28608 - Loss: 0.097358 - Time:66.01956868171692\n",
      "Epoch: 42 - Batch: 32704 - Loss: 0.567123 - Time:75.42855453491211\n",
      "Epoch: 42 - Batch: 36800 - Loss: 0.149329 - Time:84.83733677864075\n",
      "Epoch: 42 - Batch: 40896 - Loss: 0.122841 - Time:94.24609875679016\n",
      "Epoch: 42 - Batch: 44992 - Loss: 0.496029 - Time:103.67068076133728\n",
      "Epoch: 42 - Batch: 49088 - Loss: 0.394188 - Time:113.0789680480957\n",
      "Epoch: 42 - Batch: 53184 - Loss: 0.145260 - Time:122.48652124404907\n",
      "Epoch: 42 - Batch: 57280 - Loss: 0.198568 - Time:131.89676022529602\n",
      "Epoch: 42 - Batch: 61376 - Loss: 0.439740 - Time:141.32081079483032\n",
      "Epoch: 42 - Batch: 65472 - Loss: 0.347687 - Time:150.7269561290741\n",
      "Epoch: 42 - Batch: 69568 - Loss: 0.199964 - Time:160.13505244255066\n",
      "Epoch: 42 - Batch: 73664 - Loss: 0.220841 - Time:169.5600142478943\n",
      "Epoch: 42 - Batch: 77760 - Loss: 0.257708 - Time:178.96973061561584\n",
      "Epoch: 42 - Batch: 81856 - Loss: 0.146653 - Time:188.37804889678955\n",
      "Epoch: 42 - Batch: 85952 - Loss: 0.255073 - Time:197.7876317501068\n",
      "Epoch: 42 - Batch: 90048 - Loss: 0.119749 - Time:207.1990213394165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 - Batch: 94144 - Loss: 0.287448 - Time:216.60895657539368\n",
      "Epoch: 42 - Batch: 98240 - Loss: 0.854339 - Time:226.01847314834595\n",
      "Epoch: 42 - Batch: 102336 - Loss: 0.304335 - Time:235.4267852306366\n",
      "Epoch: 42 - Batch: 106432 - Loss: 0.176756 - Time:244.85142374038696\n",
      "Epoch: 42 - Batch: 110528 - Loss: 0.257637 - Time:254.26359629631042\n",
      "Epoch: 42 - Batch: 114624 - Loss: 0.153289 - Time:263.6710479259491\n",
      "Epoch: 42 - Batch: 118720 - Loss: 0.369426 - Time:273.0820610523224\n",
      "Epoch: 42 - Batch: 122816 - Loss: 0.200547 - Time:282.49455523490906\n",
      "Epoch: 42 - Batch: 126912 - Loss: 0.131009 - Time:291.92363572120667\n",
      "Epoch: 42 - Batch: 131008 - Loss: 0.186995 - Time:301.3348581790924\n",
      "Epoch: 42 - Batch: 135104 - Loss: 0.159707 - Time:310.7634265422821\n",
      "Epoch: 42 - Batch: 139200 - Loss: 0.722139 - Time:320.1770136356354\n",
      "Epoch: 42 - Batch: 143296 - Loss: 0.138827 - Time:329.58745312690735\n",
      "Epoch: 42 - Batch: 147392 - Loss: 0.703846 - Time:338.99670600891113\n",
      "Epoch: 42 - Batch: 151488 - Loss: 0.099835 - Time:348.4064779281616\n",
      "Epoch: 43 - Batch: 4032 - Loss: 0.384368 - Time:9.559008121490479\n",
      "Epoch: 43 - Batch: 8128 - Loss: 0.428685 - Time:18.998500108718872\n",
      "Epoch: 43 - Batch: 12224 - Loss: 0.497289 - Time:28.406896114349365\n",
      "Epoch: 43 - Batch: 16320 - Loss: 0.174323 - Time:37.816482067108154\n",
      "Epoch: 43 - Batch: 20416 - Loss: 0.158173 - Time:47.22557353973389\n",
      "Epoch: 43 - Batch: 24512 - Loss: 0.320330 - Time:56.633137941360474\n",
      "Epoch: 43 - Batch: 28608 - Loss: 0.227290 - Time:66.03916096687317\n",
      "Epoch: 43 - Batch: 32704 - Loss: 0.388260 - Time:75.44749879837036\n",
      "Epoch: 43 - Batch: 36800 - Loss: 0.639785 - Time:84.85494565963745\n",
      "Epoch: 43 - Batch: 40896 - Loss: 0.511191 - Time:94.27944707870483\n",
      "Epoch: 43 - Batch: 44992 - Loss: 0.357989 - Time:103.68539929389954\n",
      "Epoch: 43 - Batch: 49088 - Loss: 0.180027 - Time:113.09451007843018\n",
      "Epoch: 43 - Batch: 53184 - Loss: 0.856389 - Time:122.50203728675842\n",
      "Epoch: 43 - Batch: 57280 - Loss: 0.362863 - Time:131.9287941455841\n",
      "Epoch: 43 - Batch: 61376 - Loss: 0.181058 - Time:141.336674451828\n",
      "Epoch: 43 - Batch: 65472 - Loss: 0.082171 - Time:150.7456202507019\n",
      "Epoch: 43 - Batch: 69568 - Loss: 0.054967 - Time:160.17173266410828\n",
      "Epoch: 43 - Batch: 73664 - Loss: 0.350505 - Time:169.58269548416138\n",
      "Epoch: 43 - Batch: 77760 - Loss: 0.188690 - Time:178.99128079414368\n",
      "Epoch: 43 - Batch: 81856 - Loss: 0.122765 - Time:188.3992829322815\n",
      "Epoch: 43 - Batch: 85952 - Loss: 0.075685 - Time:197.8080747127533\n",
      "Epoch: 43 - Batch: 90048 - Loss: 0.111346 - Time:207.21703505516052\n",
      "Epoch: 43 - Batch: 94144 - Loss: 0.285052 - Time:216.6264626979828\n",
      "Epoch: 43 - Batch: 98240 - Loss: 0.104498 - Time:226.03603434562683\n",
      "Epoch: 43 - Batch: 102336 - Loss: 0.160245 - Time:235.46236872673035\n",
      "Epoch: 43 - Batch: 106432 - Loss: 0.232225 - Time:244.87079548835754\n",
      "Epoch: 43 - Batch: 110528 - Loss: 0.137738 - Time:254.2792739868164\n",
      "Epoch: 43 - Batch: 114624 - Loss: 0.262337 - Time:263.6927673816681\n",
      "Epoch: 43 - Batch: 118720 - Loss: 0.627396 - Time:273.1202037334442\n",
      "Epoch: 43 - Batch: 122816 - Loss: 0.200796 - Time:282.5296280384064\n",
      "Epoch: 43 - Batch: 126912 - Loss: 0.409097 - Time:291.9384307861328\n",
      "Epoch: 43 - Batch: 131008 - Loss: 0.469213 - Time:301.3666441440582\n",
      "Epoch: 43 - Batch: 135104 - Loss: 0.284331 - Time:310.77579498291016\n",
      "Epoch: 43 - Batch: 139200 - Loss: 0.551321 - Time:320.1892330646515\n",
      "Epoch: 43 - Batch: 143296 - Loss: 0.417491 - Time:329.5986807346344\n",
      "Epoch: 43 - Batch: 147392 - Loss: 0.232160 - Time:339.0105609893799\n",
      "Epoch: 43 - Batch: 151488 - Loss: 0.215606 - Time:348.4203989505768\n",
      "Epoch: 44 - Batch: 4032 - Loss: 0.134590 - Time:9.542632102966309\n",
      "Epoch: 44 - Batch: 8128 - Loss: 0.346517 - Time:18.953438997268677\n",
      "Epoch: 44 - Batch: 12224 - Loss: 0.202982 - Time:28.391016960144043\n",
      "Epoch: 44 - Batch: 16320 - Loss: 0.343598 - Time:37.801121950149536\n",
      "Epoch: 44 - Batch: 20416 - Loss: 0.058924 - Time:47.21132445335388\n",
      "Epoch: 44 - Batch: 24512 - Loss: 0.458177 - Time:56.621843338012695\n",
      "Epoch: 44 - Batch: 28608 - Loss: 0.076035 - Time:66.04916763305664\n",
      "Epoch: 44 - Batch: 32704 - Loss: 0.044613 - Time:75.45991015434265\n",
      "Epoch: 44 - Batch: 36800 - Loss: 0.171330 - Time:84.87082886695862\n",
      "Epoch: 44 - Batch: 40896 - Loss: 0.078689 - Time:94.29665470123291\n",
      "Epoch: 44 - Batch: 44992 - Loss: 0.318930 - Time:103.70835447311401\n",
      "Epoch: 44 - Batch: 49088 - Loss: 0.129423 - Time:113.1202940940857\n",
      "Epoch: 44 - Batch: 53184 - Loss: 0.297169 - Time:122.53043460845947\n",
      "Epoch: 44 - Batch: 57280 - Loss: 0.445377 - Time:131.9408528804779\n",
      "Epoch: 44 - Batch: 61376 - Loss: 0.396231 - Time:141.3518078327179\n",
      "Epoch: 44 - Batch: 65472 - Loss: 0.186020 - Time:150.76398253440857\n",
      "Epoch: 44 - Batch: 69568 - Loss: 0.344401 - Time:160.17474508285522\n",
      "Epoch: 44 - Batch: 73664 - Loss: 0.492442 - Time:169.60003757476807\n",
      "Epoch: 44 - Batch: 77760 - Loss: 0.233996 - Time:179.0106565952301\n",
      "Epoch: 44 - Batch: 81856 - Loss: 0.219093 - Time:188.42164611816406\n",
      "Epoch: 44 - Batch: 85952 - Loss: 0.261873 - Time:197.8321132659912\n",
      "Epoch: 44 - Batch: 90048 - Loss: 0.194099 - Time:207.2587127685547\n",
      "Epoch: 44 - Batch: 94144 - Loss: 0.091851 - Time:216.66857266426086\n",
      "Epoch: 44 - Batch: 98240 - Loss: 0.103352 - Time:226.07820630073547\n",
      "Epoch: 44 - Batch: 102336 - Loss: 0.101570 - Time:235.50395727157593\n",
      "Epoch: 44 - Batch: 106432 - Loss: 0.048681 - Time:244.9146866798401\n",
      "Epoch: 44 - Batch: 110528 - Loss: 0.143864 - Time:254.32478260993958\n",
      "Epoch: 44 - Batch: 114624 - Loss: 0.394970 - Time:263.73581433296204\n",
      "Epoch: 44 - Batch: 118720 - Loss: 0.287614 - Time:273.1453049182892\n",
      "Epoch: 44 - Batch: 122816 - Loss: 0.580349 - Time:282.55539560317993\n",
      "Epoch: 44 - Batch: 126912 - Loss: 0.156445 - Time:291.9653980731964\n",
      "Epoch: 44 - Batch: 131008 - Loss: 0.102644 - Time:301.375483751297\n",
      "Epoch: 44 - Batch: 135104 - Loss: 0.143178 - Time:310.8012411594391\n",
      "Epoch: 44 - Batch: 139200 - Loss: 0.230614 - Time:320.2106785774231\n",
      "Epoch: 44 - Batch: 143296 - Loss: 0.246855 - Time:329.6198990345001\n",
      "Epoch: 44 - Batch: 147392 - Loss: 0.188900 - Time:339.0300214290619\n",
      "Epoch: 44 - Batch: 151488 - Loss: 0.215446 - Time:348.4558992385864\n",
      "Epoch: 45 - Batch: 4032 - Loss: 0.412859 - Time:9.532733678817749\n",
      "Epoch: 45 - Batch: 8128 - Loss: 0.204858 - Time:18.943086862564087\n",
      "Epoch: 45 - Batch: 12224 - Loss: 0.215498 - Time:28.380720853805542\n",
      "Epoch: 45 - Batch: 16320 - Loss: 0.315318 - Time:37.790287733078\n",
      "Epoch: 45 - Batch: 20416 - Loss: 0.078652 - Time:47.20063543319702\n",
      "Epoch: 45 - Batch: 24512 - Loss: 0.290806 - Time:56.631736278533936\n",
      "Epoch: 45 - Batch: 28608 - Loss: 0.436940 - Time:66.04338479042053\n",
      "Epoch: 45 - Batch: 32704 - Loss: 0.206735 - Time:75.45752906799316\n",
      "Epoch: 45 - Batch: 36800 - Loss: 0.213154 - Time:84.86847186088562\n",
      "Epoch: 45 - Batch: 40896 - Loss: 0.266390 - Time:94.27913570404053\n",
      "Epoch: 45 - Batch: 44992 - Loss: 0.255841 - Time:103.69136452674866\n",
      "Epoch: 45 - Batch: 49088 - Loss: 0.367187 - Time:113.10155653953552\n",
      "Epoch: 45 - Batch: 53184 - Loss: 0.215173 - Time:122.51183295249939\n",
      "Epoch: 45 - Batch: 57280 - Loss: 0.225442 - Time:131.93720269203186\n",
      "Epoch: 45 - Batch: 61376 - Loss: 0.076606 - Time:141.34681057929993\n",
      "Epoch: 45 - Batch: 65472 - Loss: 0.455402 - Time:150.7569694519043\n",
      "Epoch: 45 - Batch: 69568 - Loss: 0.280619 - Time:160.1669261455536\n",
      "Epoch: 45 - Batch: 73664 - Loss: 0.142330 - Time:169.57725048065186\n",
      "Epoch: 45 - Batch: 77760 - Loss: 0.238546 - Time:179.00257921218872\n",
      "Epoch: 45 - Batch: 81856 - Loss: 0.332291 - Time:188.41269040107727\n",
      "Epoch: 45 - Batch: 85952 - Loss: 0.260587 - Time:197.82347130775452\n",
      "Epoch: 45 - Batch: 90048 - Loss: 0.145839 - Time:207.25166368484497\n",
      "Epoch: 45 - Batch: 94144 - Loss: 0.154987 - Time:216.66191482543945\n",
      "Epoch: 45 - Batch: 98240 - Loss: 0.385305 - Time:226.07045197486877\n",
      "Epoch: 45 - Batch: 102336 - Loss: 0.268215 - Time:235.481586933136\n",
      "Epoch: 45 - Batch: 106432 - Loss: 0.231919 - Time:244.89330315589905\n",
      "Epoch: 45 - Batch: 110528 - Loss: 0.282668 - Time:254.30250453948975\n",
      "Epoch: 45 - Batch: 114624 - Loss: 0.286704 - Time:263.71235632896423\n",
      "Epoch: 45 - Batch: 118720 - Loss: 0.080457 - Time:273.122181892395\n",
      "Epoch: 45 - Batch: 122816 - Loss: 0.247803 - Time:282.549174785614\n",
      "Epoch: 45 - Batch: 126912 - Loss: 0.429280 - Time:291.9587621688843\n",
      "Epoch: 45 - Batch: 131008 - Loss: 0.223546 - Time:301.3670163154602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 - Batch: 135104 - Loss: 0.265507 - Time:310.7762596607208\n",
      "Epoch: 45 - Batch: 139200 - Loss: 0.340496 - Time:320.20143032073975\n",
      "Epoch: 45 - Batch: 143296 - Loss: 0.484553 - Time:329.6086781024933\n",
      "Epoch: 45 - Batch: 147392 - Loss: 0.527171 - Time:339.01771998405457\n",
      "Epoch: 45 - Batch: 151488 - Loss: 0.401626 - Time:348.4432473182678\n",
      "Epoch: 46 - Batch: 4032 - Loss: 0.036542 - Time:9.550449132919312\n",
      "Epoch: 46 - Batch: 8128 - Loss: 0.187960 - Time:18.963677883148193\n",
      "Epoch: 46 - Batch: 12224 - Loss: 0.417339 - Time:28.40144991874695\n",
      "Epoch: 46 - Batch: 16320 - Loss: 0.168967 - Time:37.813589334487915\n",
      "Epoch: 46 - Batch: 20416 - Loss: 0.037714 - Time:47.225194454193115\n",
      "Epoch: 46 - Batch: 24512 - Loss: 0.335714 - Time:56.636377811431885\n",
      "Epoch: 46 - Batch: 28608 - Loss: 0.076020 - Time:66.04639482498169\n",
      "Epoch: 46 - Batch: 32704 - Loss: 0.426793 - Time:75.45678687095642\n",
      "Epoch: 46 - Batch: 36800 - Loss: 0.130012 - Time:84.86748266220093\n",
      "Epoch: 46 - Batch: 40896 - Loss: 0.057173 - Time:94.27647519111633\n",
      "Epoch: 46 - Batch: 44992 - Loss: 0.320673 - Time:103.70253801345825\n",
      "Epoch: 46 - Batch: 49088 - Loss: 0.282873 - Time:113.1091136932373\n",
      "Epoch: 46 - Batch: 53184 - Loss: 0.143687 - Time:122.51758980751038\n",
      "Epoch: 46 - Batch: 57280 - Loss: 0.077158 - Time:131.9259340763092\n",
      "Epoch: 46 - Batch: 61376 - Loss: 0.193094 - Time:141.3522653579712\n",
      "Epoch: 46 - Batch: 65472 - Loss: 0.304894 - Time:150.7616991996765\n",
      "Epoch: 46 - Batch: 69568 - Loss: 0.463276 - Time:160.17250108718872\n",
      "Epoch: 46 - Batch: 73664 - Loss: 0.299729 - Time:169.5973780155182\n",
      "Epoch: 46 - Batch: 77760 - Loss: 0.335625 - Time:179.00601172447205\n",
      "Epoch: 46 - Batch: 81856 - Loss: 0.632578 - Time:188.41586923599243\n",
      "Epoch: 46 - Batch: 85952 - Loss: 0.102974 - Time:197.82542037963867\n",
      "Epoch: 46 - Batch: 90048 - Loss: 0.093845 - Time:207.23562812805176\n",
      "Epoch: 46 - Batch: 94144 - Loss: 0.120827 - Time:216.64663004875183\n",
      "Epoch: 46 - Batch: 98240 - Loss: 0.268192 - Time:226.05695986747742\n",
      "Epoch: 46 - Batch: 102336 - Loss: 0.184483 - Time:235.46737122535706\n",
      "Epoch: 46 - Batch: 106432 - Loss: 0.409088 - Time:244.89592337608337\n",
      "Epoch: 46 - Batch: 110528 - Loss: 0.578031 - Time:254.306738615036\n",
      "Epoch: 46 - Batch: 114624 - Loss: 0.193062 - Time:263.71648812294006\n",
      "Epoch: 46 - Batch: 118720 - Loss: 0.416715 - Time:273.1276845932007\n",
      "Epoch: 46 - Batch: 122816 - Loss: 0.325514 - Time:282.55272030830383\n",
      "Epoch: 46 - Batch: 126912 - Loss: 0.325182 - Time:291.9606726169586\n",
      "Epoch: 46 - Batch: 131008 - Loss: 0.288767 - Time:301.36946988105774\n",
      "Epoch: 46 - Batch: 135104 - Loss: 0.195936 - Time:310.7973189353943\n",
      "Epoch: 46 - Batch: 139200 - Loss: 0.135297 - Time:320.2067275047302\n",
      "Epoch: 46 - Batch: 143296 - Loss: 0.345207 - Time:329.6144380569458\n",
      "Epoch: 46 - Batch: 147392 - Loss: 0.417756 - Time:339.0225205421448\n",
      "Epoch: 46 - Batch: 151488 - Loss: 0.458162 - Time:348.4327733516693\n",
      "Epoch: 47 - Batch: 4032 - Loss: 0.074775 - Time:9.537487506866455\n",
      "Epoch: 47 - Batch: 8128 - Loss: 0.055535 - Time:18.978002309799194\n",
      "Epoch: 47 - Batch: 12224 - Loss: 0.049421 - Time:28.38913106918335\n",
      "Epoch: 47 - Batch: 16320 - Loss: 0.510895 - Time:37.79913806915283\n",
      "Epoch: 47 - Batch: 20416 - Loss: 0.219299 - Time:47.20960092544556\n",
      "Epoch: 47 - Batch: 24512 - Loss: 0.175245 - Time:56.618454694747925\n",
      "Epoch: 47 - Batch: 28608 - Loss: 0.106071 - Time:66.02752876281738\n",
      "Epoch: 47 - Batch: 32704 - Loss: 0.063681 - Time:75.43716430664062\n",
      "Epoch: 47 - Batch: 36800 - Loss: 0.126372 - Time:84.8460144996643\n",
      "Epoch: 47 - Batch: 40896 - Loss: 0.025556 - Time:94.27356624603271\n",
      "Epoch: 47 - Batch: 44992 - Loss: 0.238602 - Time:103.6838915348053\n",
      "Epoch: 47 - Batch: 49088 - Loss: 0.055237 - Time:113.09316921234131\n",
      "Epoch: 47 - Batch: 53184 - Loss: 0.414416 - Time:122.5018219947815\n",
      "Epoch: 47 - Batch: 57280 - Loss: 0.180598 - Time:131.92746567726135\n",
      "Epoch: 47 - Batch: 61376 - Loss: 0.044604 - Time:141.3362946510315\n",
      "Epoch: 47 - Batch: 65472 - Loss: 0.159795 - Time:150.74538040161133\n",
      "Epoch: 47 - Batch: 69568 - Loss: 0.450991 - Time:160.17380571365356\n",
      "Epoch: 47 - Batch: 73664 - Loss: 0.065121 - Time:169.582941532135\n",
      "Epoch: 47 - Batch: 77760 - Loss: 0.217214 - Time:178.99181127548218\n",
      "Epoch: 47 - Batch: 81856 - Loss: 0.417716 - Time:188.4022388458252\n",
      "Epoch: 47 - Batch: 85952 - Loss: 0.207603 - Time:197.81111478805542\n",
      "Epoch: 47 - Batch: 90048 - Loss: 0.544811 - Time:207.2197024822235\n",
      "Epoch: 47 - Batch: 94144 - Loss: 0.431956 - Time:216.6308364868164\n",
      "Epoch: 47 - Batch: 98240 - Loss: 0.584141 - Time:226.04295325279236\n",
      "Epoch: 47 - Batch: 102336 - Loss: 0.271536 - Time:235.4700424671173\n",
      "Epoch: 47 - Batch: 106432 - Loss: 0.695288 - Time:244.8791320323944\n",
      "Epoch: 47 - Batch: 110528 - Loss: 0.064756 - Time:254.28884506225586\n",
      "Epoch: 47 - Batch: 114624 - Loss: 0.418583 - Time:263.698867559433\n",
      "Epoch: 47 - Batch: 118720 - Loss: 0.313027 - Time:273.12395668029785\n",
      "Epoch: 47 - Batch: 122816 - Loss: 0.093004 - Time:282.5345993041992\n",
      "Epoch: 47 - Batch: 126912 - Loss: 0.031215 - Time:291.9435079097748\n",
      "Epoch: 47 - Batch: 131008 - Loss: 0.260344 - Time:301.3688006401062\n",
      "Epoch: 47 - Batch: 135104 - Loss: 0.314731 - Time:310.7793378829956\n",
      "Epoch: 47 - Batch: 139200 - Loss: 0.309228 - Time:320.18760800361633\n",
      "Epoch: 47 - Batch: 143296 - Loss: 0.315164 - Time:329.5977671146393\n",
      "Epoch: 47 - Batch: 147392 - Loss: 0.267981 - Time:339.00632762908936\n",
      "Epoch: 47 - Batch: 151488 - Loss: 0.324399 - Time:348.41565465927124\n",
      "Epoch: 48 - Batch: 4032 - Loss: 0.062080 - Time:9.53472352027893\n",
      "Epoch: 48 - Batch: 8128 - Loss: 0.080905 - Time:18.946707487106323\n",
      "Epoch: 48 - Batch: 12224 - Loss: 0.148627 - Time:28.386204481124878\n",
      "Epoch: 48 - Batch: 16320 - Loss: 0.267573 - Time:37.79828429222107\n",
      "Epoch: 48 - Batch: 20416 - Loss: 0.080393 - Time:47.2118866443634\n",
      "Epoch: 48 - Batch: 24512 - Loss: 0.308788 - Time:56.62395977973938\n",
      "Epoch: 48 - Batch: 28608 - Loss: 0.053032 - Time:66.0513060092926\n",
      "Epoch: 48 - Batch: 32704 - Loss: 0.238239 - Time:75.4629716873169\n",
      "Epoch: 48 - Batch: 36800 - Loss: 0.086627 - Time:84.87224316596985\n",
      "Epoch: 48 - Batch: 40896 - Loss: 0.099295 - Time:94.29694867134094\n",
      "Epoch: 48 - Batch: 44992 - Loss: 0.047654 - Time:103.70736980438232\n",
      "Epoch: 48 - Batch: 49088 - Loss: 0.276358 - Time:113.12079334259033\n",
      "Epoch: 48 - Batch: 53184 - Loss: 0.156811 - Time:122.53576993942261\n",
      "Epoch: 48 - Batch: 57280 - Loss: 0.327229 - Time:131.9485378265381\n",
      "Epoch: 48 - Batch: 61376 - Loss: 0.152348 - Time:141.3595187664032\n",
      "Epoch: 48 - Batch: 65472 - Loss: 0.230559 - Time:150.77151250839233\n",
      "Epoch: 48 - Batch: 69568 - Loss: 0.279798 - Time:160.1819727420807\n",
      "Epoch: 48 - Batch: 73664 - Loss: 0.055663 - Time:169.61198091506958\n",
      "Epoch: 48 - Batch: 77760 - Loss: 0.278397 - Time:179.0256290435791\n",
      "Epoch: 48 - Batch: 81856 - Loss: 0.380404 - Time:188.43766975402832\n",
      "Epoch: 48 - Batch: 85952 - Loss: 0.094647 - Time:197.8494234085083\n",
      "Epoch: 48 - Batch: 90048 - Loss: 0.053977 - Time:207.27735304832458\n",
      "Epoch: 48 - Batch: 94144 - Loss: 0.307234 - Time:216.68825578689575\n",
      "Epoch: 48 - Batch: 98240 - Loss: 0.287112 - Time:226.10021328926086\n",
      "Epoch: 48 - Batch: 102336 - Loss: 0.093334 - Time:235.52936482429504\n",
      "Epoch: 48 - Batch: 106432 - Loss: 0.113027 - Time:244.93998503684998\n",
      "Epoch: 48 - Batch: 110528 - Loss: 0.637678 - Time:254.3498411178589\n",
      "Epoch: 48 - Batch: 114624 - Loss: 0.485281 - Time:263.75929498672485\n",
      "Epoch: 48 - Batch: 118720 - Loss: 0.561834 - Time:273.16909193992615\n",
      "Epoch: 48 - Batch: 122816 - Loss: 0.350898 - Time:282.578697681427\n",
      "Epoch: 48 - Batch: 126912 - Loss: 0.249433 - Time:291.9900794029236\n",
      "Epoch: 48 - Batch: 131008 - Loss: 0.374071 - Time:301.40040016174316\n",
      "Epoch: 48 - Batch: 135104 - Loss: 0.168643 - Time:310.82613253593445\n",
      "Epoch: 48 - Batch: 139200 - Loss: 0.535572 - Time:320.23692297935486\n",
      "Epoch: 48 - Batch: 143296 - Loss: 0.166724 - Time:329.64710903167725\n",
      "Epoch: 48 - Batch: 147392 - Loss: 0.355012 - Time:339.0576877593994\n",
      "Epoch: 48 - Batch: 151488 - Loss: 0.152060 - Time:348.4844264984131\n",
      "Epoch: 49 - Batch: 4032 - Loss: 0.540325 - Time:9.537442684173584\n",
      "Epoch: 49 - Batch: 8128 - Loss: 0.377903 - Time:18.9453341960907\n",
      "Epoch: 49 - Batch: 12224 - Loss: 0.152941 - Time:28.380223512649536\n",
      "Epoch: 49 - Batch: 16320 - Loss: 0.202289 - Time:37.78996515274048\n",
      "Epoch: 49 - Batch: 20416 - Loss: 0.186560 - Time:47.19812989234924\n",
      "Epoch: 49 - Batch: 24512 - Loss: 0.571623 - Time:56.62451696395874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 - Batch: 28608 - Loss: 0.448009 - Time:66.03374695777893\n",
      "Epoch: 49 - Batch: 32704 - Loss: 0.563774 - Time:75.44343638420105\n",
      "Epoch: 49 - Batch: 36800 - Loss: 0.212558 - Time:84.85266184806824\n",
      "Epoch: 49 - Batch: 40896 - Loss: 0.183064 - Time:94.2620952129364\n",
      "Epoch: 49 - Batch: 44992 - Loss: 0.113673 - Time:103.67393660545349\n",
      "Epoch: 49 - Batch: 49088 - Loss: 0.113718 - Time:113.08560132980347\n",
      "Epoch: 49 - Batch: 53184 - Loss: 0.190049 - Time:122.49586462974548\n",
      "Epoch: 49 - Batch: 57280 - Loss: 0.477261 - Time:131.91975378990173\n",
      "Epoch: 49 - Batch: 61376 - Loss: 0.408229 - Time:141.3272786140442\n",
      "Epoch: 49 - Batch: 65472 - Loss: 0.432227 - Time:150.73498630523682\n",
      "Epoch: 49 - Batch: 69568 - Loss: 0.295702 - Time:160.14146780967712\n",
      "Epoch: 49 - Batch: 73664 - Loss: 0.351904 - Time:169.55134344100952\n",
      "Epoch: 49 - Batch: 77760 - Loss: 0.407936 - Time:178.9801836013794\n",
      "Epoch: 49 - Batch: 81856 - Loss: 0.087282 - Time:188.39079356193542\n",
      "Epoch: 49 - Batch: 85952 - Loss: 0.367317 - Time:197.79992771148682\n",
      "Epoch: 49 - Batch: 90048 - Loss: 0.171710 - Time:207.22530436515808\n",
      "Epoch: 49 - Batch: 94144 - Loss: 0.086638 - Time:216.63399529457092\n",
      "Epoch: 49 - Batch: 98240 - Loss: 0.046098 - Time:226.04551005363464\n",
      "Epoch: 49 - Batch: 102336 - Loss: 0.291885 - Time:235.45476961135864\n",
      "Epoch: 49 - Batch: 106432 - Loss: 0.151409 - Time:244.86502504348755\n",
      "Epoch: 49 - Batch: 110528 - Loss: 0.225069 - Time:254.2736644744873\n",
      "Epoch: 49 - Batch: 114624 - Loss: 0.610607 - Time:263.68555545806885\n",
      "Epoch: 49 - Batch: 118720 - Loss: 0.532889 - Time:273.09514021873474\n",
      "Epoch: 49 - Batch: 122816 - Loss: 0.265424 - Time:282.5226912498474\n",
      "Epoch: 49 - Batch: 126912 - Loss: 0.492248 - Time:291.93486070632935\n",
      "Epoch: 49 - Batch: 131008 - Loss: 0.317331 - Time:301.3438239097595\n",
      "Epoch: 49 - Batch: 135104 - Loss: 0.274575 - Time:310.75281143188477\n",
      "Epoch: 49 - Batch: 139200 - Loss: 0.380937 - Time:320.1776008605957\n",
      "Epoch: 49 - Batch: 143296 - Loss: 0.153815 - Time:329.5856897830963\n",
      "Epoch: 49 - Batch: 147392 - Loss: 0.104067 - Time:338.9941391944885\n",
      "Epoch: 49 - Batch: 151488 - Loss: 0.420740 - Time:348.4184331893921\n",
      "Epoch: 50 - Batch: 4032 - Loss: 0.287783 - Time:9.550126791000366\n",
      "Epoch: 50 - Batch: 8128 - Loss: 0.275964 - Time:18.957963466644287\n",
      "Epoch: 50 - Batch: 12224 - Loss: 0.198368 - Time:28.396599531173706\n",
      "Epoch: 50 - Batch: 16320 - Loss: 0.330866 - Time:37.80442142486572\n",
      "Epoch: 50 - Batch: 20416 - Loss: 0.075904 - Time:47.21447467803955\n",
      "Epoch: 50 - Batch: 24512 - Loss: 0.056292 - Time:56.62775731086731\n",
      "Epoch: 50 - Batch: 28608 - Loss: 0.175086 - Time:66.04026651382446\n",
      "Epoch: 50 - Batch: 32704 - Loss: 0.092452 - Time:75.4504508972168\n",
      "Epoch: 50 - Batch: 36800 - Loss: 0.190143 - Time:84.86089444160461\n",
      "Epoch: 50 - Batch: 40896 - Loss: 0.089614 - Time:94.26916074752808\n",
      "Epoch: 50 - Batch: 44992 - Loss: 0.043867 - Time:103.69805598258972\n",
      "Epoch: 50 - Batch: 49088 - Loss: 0.705255 - Time:113.10860848426819\n",
      "Epoch: 50 - Batch: 53184 - Loss: 0.069803 - Time:122.51658582687378\n",
      "Epoch: 50 - Batch: 57280 - Loss: 0.068112 - Time:131.92720198631287\n",
      "Epoch: 50 - Batch: 61376 - Loss: 0.232645 - Time:141.35555720329285\n",
      "Epoch: 50 - Batch: 65472 - Loss: 0.080521 - Time:150.7640347480774\n",
      "Epoch: 50 - Batch: 69568 - Loss: 0.358997 - Time:160.17360758781433\n",
      "Epoch: 50 - Batch: 73664 - Loss: 0.348489 - Time:169.60067558288574\n",
      "Epoch: 50 - Batch: 77760 - Loss: 0.402572 - Time:179.00940537452698\n",
      "Epoch: 50 - Batch: 81856 - Loss: 0.162766 - Time:188.4176459312439\n",
      "Epoch: 50 - Batch: 85952 - Loss: 0.179803 - Time:197.82584714889526\n",
      "Epoch: 50 - Batch: 90048 - Loss: 0.074356 - Time:207.23430395126343\n",
      "Epoch: 50 - Batch: 94144 - Loss: 0.030631 - Time:216.644775390625\n",
      "Epoch: 50 - Batch: 98240 - Loss: 0.295346 - Time:226.05569791793823\n",
      "Epoch: 50 - Batch: 102336 - Loss: 0.101253 - Time:235.46799874305725\n",
      "Epoch: 50 - Batch: 106432 - Loss: 0.134154 - Time:244.89482045173645\n",
      "Epoch: 50 - Batch: 110528 - Loss: 0.442791 - Time:254.3078351020813\n",
      "Epoch: 50 - Batch: 114624 - Loss: 0.278242 - Time:263.7196717262268\n",
      "Epoch: 50 - Batch: 118720 - Loss: 0.113815 - Time:273.1321837902069\n",
      "Epoch: 50 - Batch: 122816 - Loss: 0.074682 - Time:282.5616512298584\n",
      "Epoch: 50 - Batch: 126912 - Loss: 0.318384 - Time:291.97495436668396\n",
      "Epoch: 50 - Batch: 131008 - Loss: 0.180501 - Time:301.38727617263794\n",
      "Epoch: 50 - Batch: 135104 - Loss: 0.164251 - Time:310.81664991378784\n",
      "Epoch: 50 - Batch: 139200 - Loss: 0.111669 - Time:320.22660732269287\n",
      "Epoch: 50 - Batch: 143296 - Loss: 0.541140 - Time:329.6412544250488\n",
      "Epoch: 50 - Batch: 147392 - Loss: 0.881704 - Time:339.0521984100342\n",
      "Epoch: 50 - Batch: 151488 - Loss: 0.105752 - Time:348.46483421325684\n",
      "Epoch: 51 - Batch: 4032 - Loss: 0.251363 - Time:9.534743309020996\n",
      "Epoch: 51 - Batch: 8128 - Loss: 0.366011 - Time:18.97055983543396\n",
      "Epoch: 51 - Batch: 12224 - Loss: 0.115825 - Time:28.37773108482361\n",
      "Epoch: 51 - Batch: 16320 - Loss: 0.137467 - Time:37.78572106361389\n",
      "Epoch: 51 - Batch: 20416 - Loss: 0.451991 - Time:47.193143129348755\n",
      "Epoch: 51 - Batch: 24512 - Loss: 0.053934 - Time:56.600616455078125\n",
      "Epoch: 51 - Batch: 28608 - Loss: 0.190709 - Time:66.00976228713989\n",
      "Epoch: 51 - Batch: 32704 - Loss: 0.357823 - Time:75.41892313957214\n",
      "Epoch: 51 - Batch: 36800 - Loss: 0.086567 - Time:84.82636332511902\n",
      "Epoch: 51 - Batch: 40896 - Loss: 0.059853 - Time:94.25027346611023\n",
      "Epoch: 51 - Batch: 44992 - Loss: 0.165712 - Time:103.6583514213562\n",
      "Epoch: 51 - Batch: 49088 - Loss: 0.092785 - Time:113.06681895256042\n",
      "Epoch: 51 - Batch: 53184 - Loss: 0.424484 - Time:122.4759304523468\n",
      "Epoch: 51 - Batch: 57280 - Loss: 0.136133 - Time:131.90008068084717\n",
      "Epoch: 51 - Batch: 61376 - Loss: 0.155347 - Time:141.30800008773804\n",
      "Epoch: 51 - Batch: 65472 - Loss: 0.134027 - Time:150.71613073349\n",
      "Epoch: 51 - Batch: 69568 - Loss: 0.212211 - Time:160.1406433582306\n",
      "Epoch: 51 - Batch: 73664 - Loss: 0.219026 - Time:169.54726696014404\n",
      "Epoch: 51 - Batch: 77760 - Loss: 0.056980 - Time:178.95601224899292\n",
      "Epoch: 51 - Batch: 81856 - Loss: 0.185265 - Time:188.36582612991333\n",
      "Epoch: 51 - Batch: 85952 - Loss: 0.077926 - Time:197.7749469280243\n",
      "Epoch: 51 - Batch: 90048 - Loss: 0.060980 - Time:207.18541717529297\n",
      "Epoch: 51 - Batch: 94144 - Loss: 0.250931 - Time:216.5950961112976\n",
      "Epoch: 51 - Batch: 98240 - Loss: 0.240202 - Time:226.0039200782776\n",
      "Epoch: 51 - Batch: 102336 - Loss: 0.387715 - Time:235.4301619529724\n",
      "Epoch: 51 - Batch: 106432 - Loss: 0.157558 - Time:244.83883666992188\n",
      "Epoch: 51 - Batch: 110528 - Loss: 0.137968 - Time:254.24739480018616\n",
      "Epoch: 51 - Batch: 114624 - Loss: 0.152051 - Time:263.655357837677\n",
      "Epoch: 51 - Batch: 118720 - Loss: 0.334138 - Time:273.08081889152527\n",
      "Epoch: 51 - Batch: 122816 - Loss: 0.574550 - Time:282.4904294013977\n",
      "Epoch: 51 - Batch: 126912 - Loss: 0.173791 - Time:291.8992602825165\n",
      "Epoch: 51 - Batch: 131008 - Loss: 0.103094 - Time:301.3245265483856\n",
      "Epoch: 51 - Batch: 135104 - Loss: 0.227496 - Time:310.73466515541077\n",
      "Epoch: 51 - Batch: 139200 - Loss: 0.305592 - Time:320.1430766582489\n",
      "Epoch: 51 - Batch: 143296 - Loss: 0.062648 - Time:329.55221581459045\n",
      "Epoch: 51 - Batch: 147392 - Loss: 0.125771 - Time:338.9626612663269\n",
      "Epoch: 51 - Batch: 151488 - Loss: 0.226467 - Time:348.37270975112915\n",
      "Epoch: 52 - Batch: 4032 - Loss: 0.151518 - Time:9.538350582122803\n",
      "Epoch: 52 - Batch: 8128 - Loss: 0.298858 - Time:18.950146436691284\n",
      "Epoch: 52 - Batch: 12224 - Loss: 0.170651 - Time:28.390342950820923\n",
      "Epoch: 52 - Batch: 16320 - Loss: 0.197782 - Time:37.8027617931366\n",
      "Epoch: 52 - Batch: 20416 - Loss: 0.293082 - Time:47.21575045585632\n",
      "Epoch: 52 - Batch: 24512 - Loss: 0.146205 - Time:56.62789344787598\n",
      "Epoch: 52 - Batch: 28608 - Loss: 0.162446 - Time:66.05679059028625\n",
      "Epoch: 52 - Batch: 32704 - Loss: 0.078213 - Time:75.46725392341614\n",
      "Epoch: 52 - Batch: 36800 - Loss: 0.143958 - Time:84.87656593322754\n",
      "Epoch: 52 - Batch: 40896 - Loss: 0.211691 - Time:94.3047182559967\n",
      "Epoch: 52 - Batch: 44992 - Loss: 0.354005 - Time:103.71336650848389\n",
      "Epoch: 52 - Batch: 49088 - Loss: 0.137016 - Time:113.12214970588684\n",
      "Epoch: 52 - Batch: 53184 - Loss: 0.278735 - Time:122.53271579742432\n",
      "Epoch: 52 - Batch: 57280 - Loss: 0.537325 - Time:131.94364619255066\n",
      "Epoch: 52 - Batch: 61376 - Loss: 0.028477 - Time:141.35259819030762\n",
      "Epoch: 52 - Batch: 65472 - Loss: 0.536373 - Time:150.76069712638855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 - Batch: 69568 - Loss: 0.330195 - Time:160.17153477668762\n",
      "Epoch: 52 - Batch: 73664 - Loss: 0.638461 - Time:169.59843611717224\n",
      "Epoch: 52 - Batch: 77760 - Loss: 0.200165 - Time:179.00872778892517\n",
      "Epoch: 52 - Batch: 81856 - Loss: 0.041252 - Time:188.4204559326172\n",
      "Epoch: 52 - Batch: 85952 - Loss: 0.085110 - Time:197.8309178352356\n",
      "Epoch: 52 - Batch: 90048 - Loss: 0.303624 - Time:207.2578911781311\n",
      "Epoch: 52 - Batch: 94144 - Loss: 0.069033 - Time:216.67086958885193\n",
      "Epoch: 52 - Batch: 98240 - Loss: 0.246846 - Time:226.08259105682373\n",
      "Epoch: 52 - Batch: 102336 - Loss: 0.237214 - Time:235.50917625427246\n",
      "Epoch: 52 - Batch: 106432 - Loss: 0.087902 - Time:244.9203643798828\n",
      "Epoch: 52 - Batch: 110528 - Loss: 0.215582 - Time:254.3318452835083\n",
      "Epoch: 52 - Batch: 114624 - Loss: 0.152845 - Time:263.74128222465515\n",
      "Epoch: 52 - Batch: 118720 - Loss: 0.026937 - Time:273.15181708335876\n",
      "Epoch: 52 - Batch: 122816 - Loss: 0.454354 - Time:282.5612154006958\n",
      "Epoch: 52 - Batch: 126912 - Loss: 0.093307 - Time:291.97009682655334\n",
      "Epoch: 52 - Batch: 131008 - Loss: 0.348329 - Time:301.37910628318787\n",
      "Epoch: 52 - Batch: 135104 - Loss: 0.284887 - Time:310.8080024719238\n",
      "Epoch: 52 - Batch: 139200 - Loss: 0.306123 - Time:320.2180235385895\n",
      "Epoch: 52 - Batch: 143296 - Loss: 0.169336 - Time:329.63118720054626\n",
      "Epoch: 52 - Batch: 147392 - Loss: 0.146779 - Time:339.04438161849976\n",
      "Epoch: 52 - Batch: 151488 - Loss: 0.425859 - Time:348.4697048664093\n",
      "Epoch: 53 - Batch: 4032 - Loss: 0.037835 - Time:9.545809745788574\n",
      "Epoch: 53 - Batch: 8128 - Loss: 0.082556 - Time:18.958281993865967\n",
      "Epoch: 53 - Batch: 12224 - Loss: 0.043189 - Time:28.37130045890808\n",
      "Epoch: 53 - Batch: 16320 - Loss: 0.167771 - Time:37.81022119522095\n",
      "Epoch: 53 - Batch: 20416 - Loss: 0.032958 - Time:47.22464895248413\n",
      "Epoch: 53 - Batch: 24512 - Loss: 0.095111 - Time:56.651840686798096\n",
      "Epoch: 53 - Batch: 28608 - Loss: 0.049325 - Time:66.06197905540466\n",
      "Epoch: 53 - Batch: 32704 - Loss: 0.181209 - Time:75.47640681266785\n",
      "Epoch: 53 - Batch: 36800 - Loss: 0.132916 - Time:84.89028644561768\n",
      "Epoch: 53 - Batch: 40896 - Loss: 0.067612 - Time:94.30070996284485\n",
      "Epoch: 53 - Batch: 44992 - Loss: 0.325389 - Time:103.71311974525452\n",
      "Epoch: 53 - Batch: 49088 - Loss: 0.116203 - Time:113.12341332435608\n",
      "Epoch: 53 - Batch: 53184 - Loss: 0.043573 - Time:122.53338932991028\n",
      "Epoch: 53 - Batch: 57280 - Loss: 0.072089 - Time:131.94559621810913\n",
      "Epoch: 53 - Batch: 61376 - Loss: 0.094262 - Time:141.37577104568481\n",
      "Epoch: 53 - Batch: 65472 - Loss: 0.177898 - Time:150.78912138938904\n",
      "Epoch: 53 - Batch: 69568 - Loss: 0.125047 - Time:160.1993579864502\n",
      "Epoch: 53 - Batch: 73664 - Loss: 0.108926 - Time:169.6079695224762\n",
      "Epoch: 53 - Batch: 77760 - Loss: 0.180191 - Time:179.03564500808716\n",
      "Epoch: 53 - Batch: 81856 - Loss: 0.097288 - Time:188.44518733024597\n",
      "Epoch: 53 - Batch: 85952 - Loss: 0.245633 - Time:197.855318069458\n",
      "Epoch: 53 - Batch: 90048 - Loss: 0.103108 - Time:207.28250169754028\n",
      "Epoch: 53 - Batch: 94144 - Loss: 0.127185 - Time:216.69145154953003\n",
      "Epoch: 53 - Batch: 98240 - Loss: 0.372678 - Time:226.1022651195526\n",
      "Epoch: 53 - Batch: 102336 - Loss: 0.266205 - Time:235.51168274879456\n",
      "Epoch: 53 - Batch: 106432 - Loss: 0.089576 - Time:244.92241883277893\n",
      "Epoch: 53 - Batch: 110528 - Loss: 0.224961 - Time:254.33285927772522\n",
      "Epoch: 53 - Batch: 114624 - Loss: 0.470333 - Time:263.7421889305115\n",
      "Epoch: 53 - Batch: 118720 - Loss: 0.065358 - Time:273.1519944667816\n",
      "Epoch: 53 - Batch: 122816 - Loss: 0.260905 - Time:282.5791914463043\n",
      "Epoch: 53 - Batch: 126912 - Loss: 0.190907 - Time:291.98854899406433\n",
      "Epoch: 53 - Batch: 131008 - Loss: 0.359088 - Time:301.3986659049988\n",
      "Epoch: 53 - Batch: 135104 - Loss: 0.336858 - Time:310.81179022789\n",
      "Epoch: 53 - Batch: 139200 - Loss: 0.169289 - Time:320.23859429359436\n",
      "Epoch: 53 - Batch: 143296 - Loss: 0.389358 - Time:329.6514277458191\n",
      "Epoch: 53 - Batch: 147392 - Loss: 0.305677 - Time:339.06199765205383\n",
      "Epoch: 53 - Batch: 151488 - Loss: 0.063274 - Time:348.49178099632263\n",
      "Epoch: 54 - Batch: 4032 - Loss: 0.054996 - Time:9.531518697738647\n",
      "Epoch: 54 - Batch: 8128 - Loss: 0.324852 - Time:18.94101119041443\n",
      "Epoch: 54 - Batch: 12224 - Loss: 0.247437 - Time:28.378772020339966\n",
      "Epoch: 54 - Batch: 16320 - Loss: 0.188020 - Time:37.79073166847229\n",
      "Epoch: 54 - Batch: 20416 - Loss: 0.186650 - Time:47.203309535980225\n",
      "Epoch: 54 - Batch: 24512 - Loss: 0.044633 - Time:56.618003606796265\n",
      "Epoch: 54 - Batch: 28608 - Loss: 0.101235 - Time:66.02871632575989\n",
      "Epoch: 54 - Batch: 32704 - Loss: 0.060963 - Time:75.44203877449036\n",
      "Epoch: 54 - Batch: 36800 - Loss: 0.029798 - Time:84.85671997070312\n",
      "Epoch: 54 - Batch: 40896 - Loss: 0.157101 - Time:94.27174234390259\n",
      "Epoch: 54 - Batch: 44992 - Loss: 0.055253 - Time:103.69886493682861\n",
      "Epoch: 54 - Batch: 49088 - Loss: 0.380967 - Time:113.11017751693726\n",
      "Epoch: 54 - Batch: 53184 - Loss: 0.057094 - Time:122.5239531993866\n",
      "Epoch: 54 - Batch: 57280 - Loss: 0.111175 - Time:131.9364812374115\n",
      "Epoch: 54 - Batch: 61376 - Loss: 0.393668 - Time:141.36843013763428\n",
      "Epoch: 54 - Batch: 65472 - Loss: 0.084222 - Time:150.78248405456543\n",
      "Epoch: 54 - Batch: 69568 - Loss: 0.324151 - Time:160.1950237751007\n",
      "Epoch: 54 - Batch: 73664 - Loss: 0.142041 - Time:169.6222996711731\n",
      "Epoch: 54 - Batch: 77760 - Loss: 0.116351 - Time:179.03165650367737\n",
      "Epoch: 54 - Batch: 81856 - Loss: 0.301841 - Time:188.44070720672607\n",
      "Epoch: 54 - Batch: 85952 - Loss: 0.196517 - Time:197.85513758659363\n",
      "Epoch: 54 - Batch: 90048 - Loss: 0.077569 - Time:207.26788592338562\n",
      "Epoch: 54 - Batch: 94144 - Loss: 0.238015 - Time:216.67963123321533\n",
      "Epoch: 54 - Batch: 98240 - Loss: 0.285842 - Time:226.09175205230713\n",
      "Epoch: 54 - Batch: 102336 - Loss: 0.127637 - Time:235.50573062896729\n",
      "Epoch: 54 - Batch: 106432 - Loss: 0.233431 - Time:244.93546199798584\n",
      "Epoch: 54 - Batch: 110528 - Loss: 0.075468 - Time:254.347811460495\n",
      "Epoch: 54 - Batch: 114624 - Loss: 0.074254 - Time:263.7584927082062\n",
      "Epoch: 54 - Batch: 118720 - Loss: 0.346232 - Time:273.17018580436707\n",
      "Epoch: 54 - Batch: 122816 - Loss: 0.287113 - Time:282.5819983482361\n",
      "Epoch: 54 - Batch: 126912 - Loss: 0.182987 - Time:292.0114254951477\n",
      "Epoch: 54 - Batch: 131008 - Loss: 0.225087 - Time:301.4241268634796\n",
      "Epoch: 54 - Batch: 135104 - Loss: 0.580412 - Time:310.8380844593048\n",
      "Epoch: 54 - Batch: 139200 - Loss: 0.376695 - Time:320.2667717933655\n",
      "Epoch: 54 - Batch: 143296 - Loss: 0.174235 - Time:329.6770610809326\n",
      "Epoch: 54 - Batch: 147392 - Loss: 0.724006 - Time:339.08856296539307\n",
      "Epoch: 54 - Batch: 151488 - Loss: 0.169049 - Time:348.4998767375946\n",
      "Epoch: 55 - Batch: 4032 - Loss: 0.409754 - Time:9.53922414779663\n",
      "Epoch: 55 - Batch: 8128 - Loss: 0.255646 - Time:18.974129915237427\n",
      "Epoch: 55 - Batch: 12224 - Loss: 0.232686 - Time:28.382138967514038\n",
      "Epoch: 55 - Batch: 16320 - Loss: 0.458737 - Time:37.79105877876282\n",
      "Epoch: 55 - Batch: 20416 - Loss: 0.210004 - Time:47.202595233917236\n",
      "Epoch: 55 - Batch: 24512 - Loss: 0.047975 - Time:56.61304759979248\n",
      "Epoch: 55 - Batch: 28608 - Loss: 0.202733 - Time:66.0224084854126\n",
      "Epoch: 55 - Batch: 32704 - Loss: 0.104162 - Time:75.43297505378723\n",
      "Epoch: 55 - Batch: 36800 - Loss: 0.063535 - Time:84.84407305717468\n",
      "Epoch: 55 - Batch: 40896 - Loss: 0.152324 - Time:94.26949429512024\n",
      "Epoch: 55 - Batch: 44992 - Loss: 0.143899 - Time:103.68051505088806\n",
      "Epoch: 55 - Batch: 49088 - Loss: 0.180268 - Time:113.09163784980774\n",
      "Epoch: 55 - Batch: 53184 - Loss: 0.099559 - Time:122.49986553192139\n",
      "Epoch: 55 - Batch: 57280 - Loss: 0.037533 - Time:131.9244258403778\n",
      "Epoch: 55 - Batch: 61376 - Loss: 0.571730 - Time:141.33053398132324\n",
      "Epoch: 55 - Batch: 65472 - Loss: 0.169512 - Time:150.73813581466675\n",
      "Epoch: 55 - Batch: 69568 - Loss: 0.038393 - Time:160.16315245628357\n",
      "Epoch: 55 - Batch: 73664 - Loss: 0.101559 - Time:169.57129549980164\n",
      "Epoch: 55 - Batch: 77760 - Loss: 0.101647 - Time:178.97908306121826\n",
      "Epoch: 55 - Batch: 81856 - Loss: 0.213468 - Time:188.38455963134766\n",
      "Epoch: 55 - Batch: 85952 - Loss: 0.075135 - Time:197.79254174232483\n",
      "Epoch: 55 - Batch: 90048 - Loss: 0.151233 - Time:207.2035675048828\n",
      "Epoch: 55 - Batch: 94144 - Loss: 0.041972 - Time:216.61180996894836\n",
      "Epoch: 55 - Batch: 98240 - Loss: 0.465196 - Time:226.01929378509521\n",
      "Epoch: 55 - Batch: 102336 - Loss: 0.090222 - Time:235.4434938430786\n",
      "Epoch: 55 - Batch: 106432 - Loss: 0.147138 - Time:244.8510558605194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 - Batch: 110528 - Loss: 0.208089 - Time:254.2593104839325\n",
      "Epoch: 55 - Batch: 114624 - Loss: 0.395585 - Time:263.6691823005676\n",
      "Epoch: 55 - Batch: 118720 - Loss: 0.247338 - Time:273.09323740005493\n",
      "Epoch: 55 - Batch: 122816 - Loss: 0.141509 - Time:282.50018310546875\n",
      "Epoch: 55 - Batch: 126912 - Loss: 0.188092 - Time:291.90783166885376\n",
      "Epoch: 55 - Batch: 131008 - Loss: 0.195504 - Time:301.33189582824707\n",
      "Epoch: 55 - Batch: 135104 - Loss: 0.138380 - Time:310.7415089607239\n",
      "Epoch: 55 - Batch: 139200 - Loss: 0.219216 - Time:320.15135622024536\n",
      "Epoch: 55 - Batch: 143296 - Loss: 0.170049 - Time:329.5610418319702\n",
      "Epoch: 55 - Batch: 147392 - Loss: 0.523503 - Time:338.97159028053284\n",
      "Epoch: 55 - Batch: 151488 - Loss: 0.274319 - Time:348.3830759525299\n",
      "Epoch: 56 - Batch: 4032 - Loss: 0.102622 - Time:9.54338264465332\n",
      "Epoch: 56 - Batch: 8128 - Loss: 0.034521 - Time:18.957618951797485\n",
      "Epoch: 56 - Batch: 12224 - Loss: 0.065614 - Time:28.397583484649658\n",
      "Epoch: 56 - Batch: 16320 - Loss: 0.106229 - Time:37.808695554733276\n",
      "Epoch: 56 - Batch: 20416 - Loss: 0.091388 - Time:47.22002363204956\n",
      "Epoch: 56 - Batch: 24512 - Loss: 0.044673 - Time:56.63097620010376\n",
      "Epoch: 56 - Batch: 28608 - Loss: 0.116335 - Time:66.05957102775574\n",
      "Epoch: 56 - Batch: 32704 - Loss: 0.051624 - Time:75.47155427932739\n",
      "Epoch: 56 - Batch: 36800 - Loss: 0.299245 - Time:84.88133311271667\n",
      "Epoch: 56 - Batch: 40896 - Loss: 0.060772 - Time:94.30890083312988\n",
      "Epoch: 56 - Batch: 44992 - Loss: 0.329598 - Time:103.71882104873657\n",
      "Epoch: 56 - Batch: 49088 - Loss: 0.083094 - Time:113.12779712677002\n",
      "Epoch: 56 - Batch: 53184 - Loss: 0.044691 - Time:122.5418951511383\n",
      "Epoch: 56 - Batch: 57280 - Loss: 0.288984 - Time:131.95198488235474\n",
      "Epoch: 56 - Batch: 61376 - Loss: 0.042103 - Time:141.36194968223572\n",
      "Epoch: 56 - Batch: 65472 - Loss: 0.280644 - Time:150.77315020561218\n",
      "Epoch: 56 - Batch: 69568 - Loss: 0.117114 - Time:160.18181204795837\n",
      "Epoch: 56 - Batch: 73664 - Loss: 0.121749 - Time:169.60741424560547\n",
      "Epoch: 56 - Batch: 77760 - Loss: 0.093357 - Time:179.0173740386963\n",
      "Epoch: 56 - Batch: 81856 - Loss: 0.352535 - Time:188.42706966400146\n",
      "Epoch: 56 - Batch: 85952 - Loss: 0.183432 - Time:197.83583784103394\n",
      "Epoch: 56 - Batch: 90048 - Loss: 0.233732 - Time:207.26324248313904\n",
      "Epoch: 56 - Batch: 94144 - Loss: 0.533227 - Time:216.6714107990265\n",
      "Epoch: 56 - Batch: 98240 - Loss: 0.028791 - Time:226.08130264282227\n",
      "Epoch: 56 - Batch: 102336 - Loss: 0.459562 - Time:235.5070879459381\n",
      "Epoch: 56 - Batch: 106432 - Loss: 0.170854 - Time:244.91604018211365\n",
      "Epoch: 56 - Batch: 110528 - Loss: 0.067408 - Time:254.32396340370178\n",
      "Epoch: 56 - Batch: 114624 - Loss: 0.239416 - Time:263.73315834999084\n",
      "Epoch: 56 - Batch: 118720 - Loss: 0.228216 - Time:273.14435601234436\n",
      "Epoch: 56 - Batch: 122816 - Loss: 0.074663 - Time:282.5554950237274\n",
      "Epoch: 56 - Batch: 126912 - Loss: 0.433553 - Time:291.96553659439087\n",
      "Epoch: 56 - Batch: 131008 - Loss: 0.312966 - Time:301.3761191368103\n",
      "Epoch: 56 - Batch: 135104 - Loss: 0.357500 - Time:310.8031442165375\n",
      "Epoch: 56 - Batch: 139200 - Loss: 0.108385 - Time:320.2140221595764\n",
      "Epoch: 56 - Batch: 143296 - Loss: 0.204422 - Time:329.6245262622833\n",
      "Epoch: 56 - Batch: 147392 - Loss: 0.435271 - Time:339.0330455303192\n",
      "Epoch: 56 - Batch: 151488 - Loss: 0.179410 - Time:348.46119141578674\n",
      "Epoch: 57 - Batch: 4032 - Loss: 0.106098 - Time:9.536912202835083\n",
      "Epoch: 57 - Batch: 8128 - Loss: 0.091928 - Time:18.947256088256836\n",
      "Epoch: 57 - Batch: 12224 - Loss: 0.057897 - Time:28.35961103439331\n",
      "Epoch: 57 - Batch: 16320 - Loss: 0.182143 - Time:37.79671263694763\n",
      "Epoch: 57 - Batch: 20416 - Loss: 0.102321 - Time:47.20916700363159\n",
      "Epoch: 57 - Batch: 24512 - Loss: 0.271041 - Time:56.62008213996887\n",
      "Epoch: 57 - Batch: 28608 - Loss: 0.147360 - Time:66.0471510887146\n",
      "Epoch: 57 - Batch: 32704 - Loss: 0.156283 - Time:75.45737957954407\n",
      "Epoch: 57 - Batch: 36800 - Loss: 0.065261 - Time:84.86831593513489\n",
      "Epoch: 57 - Batch: 40896 - Loss: 0.289294 - Time:94.28056526184082\n",
      "Epoch: 57 - Batch: 44992 - Loss: 0.065168 - Time:103.69210839271545\n",
      "Epoch: 57 - Batch: 49088 - Loss: 0.068012 - Time:113.10173296928406\n",
      "Epoch: 57 - Batch: 53184 - Loss: 0.109099 - Time:122.5125184059143\n",
      "Epoch: 57 - Batch: 57280 - Loss: 0.397169 - Time:131.92207527160645\n",
      "Epoch: 57 - Batch: 61376 - Loss: 0.162924 - Time:141.3500645160675\n",
      "Epoch: 57 - Batch: 65472 - Loss: 0.201418 - Time:150.75928902626038\n",
      "Epoch: 57 - Batch: 69568 - Loss: 0.240770 - Time:160.17075085639954\n",
      "Epoch: 57 - Batch: 73664 - Loss: 0.202093 - Time:169.58151364326477\n",
      "Epoch: 57 - Batch: 77760 - Loss: 0.196661 - Time:179.00616097450256\n",
      "Epoch: 57 - Batch: 81856 - Loss: 0.231282 - Time:188.41618251800537\n",
      "Epoch: 57 - Batch: 85952 - Loss: 0.212647 - Time:197.82508277893066\n",
      "Epoch: 57 - Batch: 90048 - Loss: 0.610304 - Time:207.25085592269897\n",
      "Epoch: 57 - Batch: 94144 - Loss: 0.174000 - Time:216.66094946861267\n",
      "Epoch: 57 - Batch: 98240 - Loss: 0.133847 - Time:226.0698254108429\n",
      "Epoch: 57 - Batch: 102336 - Loss: 0.256202 - Time:235.48140358924866\n",
      "Epoch: 57 - Batch: 106432 - Loss: 0.221155 - Time:244.89338564872742\n",
      "Epoch: 57 - Batch: 110528 - Loss: 0.070220 - Time:254.3034839630127\n",
      "Epoch: 57 - Batch: 114624 - Loss: 0.233303 - Time:263.71536207199097\n",
      "Epoch: 57 - Batch: 118720 - Loss: 0.359212 - Time:273.12626695632935\n",
      "Epoch: 57 - Batch: 122816 - Loss: 0.239646 - Time:282.55274868011475\n",
      "Epoch: 57 - Batch: 126912 - Loss: 0.257247 - Time:291.9634253978729\n",
      "Epoch: 57 - Batch: 131008 - Loss: 0.052007 - Time:301.37210488319397\n",
      "Epoch: 57 - Batch: 135104 - Loss: 0.342809 - Time:310.78163027763367\n",
      "Epoch: 57 - Batch: 139200 - Loss: 0.236389 - Time:320.20789074897766\n",
      "Epoch: 57 - Batch: 143296 - Loss: 0.361200 - Time:329.6169316768646\n",
      "Epoch: 57 - Batch: 147392 - Loss: 0.094322 - Time:339.02746081352234\n",
      "Epoch: 57 - Batch: 151488 - Loss: 0.089454 - Time:348.45643496513367\n",
      "Epoch: 58 - Batch: 4032 - Loss: 0.052695 - Time:9.545621156692505\n",
      "Epoch: 58 - Batch: 8128 - Loss: 0.282180 - Time:18.954413652420044\n",
      "Epoch: 58 - Batch: 12224 - Loss: 0.248426 - Time:28.389163494110107\n",
      "Epoch: 58 - Batch: 16320 - Loss: 0.269814 - Time:37.7979736328125\n",
      "Epoch: 58 - Batch: 20416 - Loss: 0.238238 - Time:47.20960283279419\n",
      "Epoch: 58 - Batch: 24512 - Loss: 0.053863 - Time:56.621363401412964\n",
      "Epoch: 58 - Batch: 28608 - Loss: 0.369976 - Time:66.03191471099854\n",
      "Epoch: 58 - Batch: 32704 - Loss: 0.171404 - Time:75.44380903244019\n",
      "Epoch: 58 - Batch: 36800 - Loss: 0.164833 - Time:84.85433173179626\n",
      "Epoch: 58 - Batch: 40896 - Loss: 0.074536 - Time:94.2653386592865\n",
      "Epoch: 58 - Batch: 44992 - Loss: 0.031428 - Time:103.69499564170837\n",
      "Epoch: 58 - Batch: 49088 - Loss: 0.063297 - Time:113.10407447814941\n",
      "Epoch: 58 - Batch: 53184 - Loss: 0.030242 - Time:122.51470232009888\n",
      "Epoch: 58 - Batch: 57280 - Loss: 0.338146 - Time:131.92445397377014\n",
      "Epoch: 58 - Batch: 61376 - Loss: 0.056975 - Time:141.3509602546692\n",
      "Epoch: 58 - Batch: 65472 - Loss: 0.216712 - Time:150.76108813285828\n",
      "Epoch: 58 - Batch: 69568 - Loss: 0.206514 - Time:160.17335867881775\n",
      "Epoch: 58 - Batch: 73664 - Loss: 0.092607 - Time:169.601380109787\n",
      "Epoch: 58 - Batch: 77760 - Loss: 0.037453 - Time:179.01257634162903\n",
      "Epoch: 58 - Batch: 81856 - Loss: 0.205050 - Time:188.4247374534607\n",
      "Epoch: 58 - Batch: 85952 - Loss: 0.041610 - Time:197.83477401733398\n",
      "Epoch: 58 - Batch: 90048 - Loss: 0.527016 - Time:207.24555659294128\n",
      "Epoch: 58 - Batch: 94144 - Loss: 0.093163 - Time:216.65480875968933\n",
      "Epoch: 58 - Batch: 98240 - Loss: 0.129893 - Time:226.0660274028778\n",
      "Epoch: 58 - Batch: 102336 - Loss: 0.197392 - Time:235.47591090202332\n",
      "Epoch: 58 - Batch: 106432 - Loss: 0.308999 - Time:244.90205335617065\n",
      "Epoch: 58 - Batch: 110528 - Loss: 0.355441 - Time:254.3118965625763\n",
      "Epoch: 58 - Batch: 114624 - Loss: 0.328495 - Time:263.72192192077637\n",
      "Epoch: 58 - Batch: 118720 - Loss: 0.142479 - Time:273.13398933410645\n",
      "Epoch: 58 - Batch: 122816 - Loss: 0.241487 - Time:282.5442934036255\n",
      "Epoch: 58 - Batch: 126912 - Loss: 0.163262 - Time:291.97169733047485\n",
      "Epoch: 58 - Batch: 131008 - Loss: 0.165864 - Time:301.3816397190094\n",
      "Epoch: 58 - Batch: 135104 - Loss: 0.121826 - Time:310.7945444583893\n",
      "Epoch: 58 - Batch: 139200 - Loss: 0.258972 - Time:320.2209484577179\n",
      "Epoch: 58 - Batch: 143296 - Loss: 0.123402 - Time:329.631103515625\n",
      "Epoch: 58 - Batch: 147392 - Loss: 0.155844 - Time:339.0404222011566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 - Batch: 151488 - Loss: 0.646557 - Time:348.4504497051239\n",
      "Epoch: 59 - Batch: 4032 - Loss: 0.093637 - Time:9.546299457550049\n",
      "Epoch: 59 - Batch: 8128 - Loss: 0.053263 - Time:18.98443365097046\n",
      "Epoch: 59 - Batch: 12224 - Loss: 0.187020 - Time:28.39610481262207\n",
      "Epoch: 59 - Batch: 16320 - Loss: 0.040306 - Time:37.80480670928955\n",
      "Epoch: 59 - Batch: 20416 - Loss: 0.036910 - Time:47.21388792991638\n",
      "Epoch: 59 - Batch: 24512 - Loss: 0.172992 - Time:56.62446975708008\n",
      "Epoch: 59 - Batch: 28608 - Loss: 0.160670 - Time:66.0382432937622\n",
      "Epoch: 59 - Batch: 32704 - Loss: 0.043015 - Time:75.44946241378784\n",
      "Epoch: 59 - Batch: 36800 - Loss: 0.094116 - Time:84.86036229133606\n",
      "Epoch: 59 - Batch: 40896 - Loss: 0.162103 - Time:94.28916049003601\n",
      "Epoch: 59 - Batch: 44992 - Loss: 0.040849 - Time:103.69938588142395\n",
      "Epoch: 59 - Batch: 49088 - Loss: 0.050931 - Time:113.10811877250671\n",
      "Epoch: 59 - Batch: 53184 - Loss: 0.367419 - Time:122.51654243469238\n",
      "Epoch: 59 - Batch: 57280 - Loss: 0.283622 - Time:131.94108891487122\n",
      "Epoch: 59 - Batch: 61376 - Loss: 0.102271 - Time:141.3523190021515\n",
      "Epoch: 59 - Batch: 65472 - Loss: 0.300377 - Time:150.76284050941467\n",
      "Epoch: 59 - Batch: 69568 - Loss: 0.084392 - Time:160.19037866592407\n",
      "Epoch: 59 - Batch: 73664 - Loss: 0.209793 - Time:169.60246801376343\n",
      "Epoch: 59 - Batch: 77760 - Loss: 0.229947 - Time:179.01325511932373\n",
      "Epoch: 59 - Batch: 81856 - Loss: 0.466224 - Time:188.42344784736633\n",
      "Epoch: 59 - Batch: 85952 - Loss: 0.161259 - Time:197.83387112617493\n",
      "Epoch: 59 - Batch: 90048 - Loss: 0.058699 - Time:207.2426562309265\n",
      "Epoch: 59 - Batch: 94144 - Loss: 0.392024 - Time:216.6545844078064\n",
      "Epoch: 59 - Batch: 98240 - Loss: 0.426578 - Time:226.06544995307922\n",
      "Epoch: 59 - Batch: 102336 - Loss: 0.227724 - Time:235.492125749588\n",
      "Epoch: 59 - Batch: 106432 - Loss: 0.424862 - Time:244.90028929710388\n",
      "Epoch: 59 - Batch: 110528 - Loss: 0.126038 - Time:254.30903697013855\n",
      "Epoch: 59 - Batch: 114624 - Loss: 0.057364 - Time:263.71933364868164\n",
      "Epoch: 59 - Batch: 118720 - Loss: 0.225837 - Time:273.14484000205994\n",
      "Epoch: 59 - Batch: 122816 - Loss: 0.224802 - Time:282.55497908592224\n",
      "Epoch: 59 - Batch: 126912 - Loss: 0.142337 - Time:291.9642689228058\n",
      "Epoch: 59 - Batch: 131008 - Loss: 0.180002 - Time:301.39164996147156\n",
      "Epoch: 59 - Batch: 135104 - Loss: 0.091716 - Time:310.8009612560272\n",
      "Epoch: 59 - Batch: 139200 - Loss: 0.290867 - Time:320.212167263031\n",
      "Epoch: 59 - Batch: 143296 - Loss: 0.239096 - Time:329.6218137741089\n",
      "Epoch: 59 - Batch: 147392 - Loss: 0.057962 - Time:339.0311689376831\n",
      "Epoch: 59 - Batch: 151488 - Loss: 0.250323 - Time:348.4412684440613\n",
      "Epoch: 60 - Batch: 4032 - Loss: 0.245849 - Time:9.53709101676941\n",
      "Epoch: 60 - Batch: 8128 - Loss: 0.144457 - Time:18.950404405593872\n",
      "Epoch: 60 - Batch: 12224 - Loss: 0.414424 - Time:28.387677431106567\n",
      "Epoch: 60 - Batch: 16320 - Loss: 0.306128 - Time:37.798800230026245\n",
      "Epoch: 60 - Batch: 20416 - Loss: 0.135806 - Time:47.2100772857666\n",
      "Epoch: 60 - Batch: 24512 - Loss: 0.069682 - Time:56.62117290496826\n",
      "Epoch: 60 - Batch: 28608 - Loss: 0.117158 - Time:66.05026364326477\n",
      "Epoch: 60 - Batch: 32704 - Loss: 0.198962 - Time:75.46116065979004\n",
      "Epoch: 60 - Batch: 36800 - Loss: 0.045140 - Time:84.87091779708862\n",
      "Epoch: 60 - Batch: 40896 - Loss: 0.305184 - Time:94.29893445968628\n",
      "Epoch: 60 - Batch: 44992 - Loss: 0.380954 - Time:103.71235203742981\n",
      "Epoch: 60 - Batch: 49088 - Loss: 0.115483 - Time:113.12624406814575\n",
      "Epoch: 60 - Batch: 53184 - Loss: 0.210207 - Time:122.53709149360657\n",
      "Epoch: 60 - Batch: 57280 - Loss: 0.096927 - Time:131.94819951057434\n",
      "Epoch: 60 - Batch: 61376 - Loss: 0.339825 - Time:141.3580551147461\n",
      "Epoch: 60 - Batch: 65472 - Loss: 0.088985 - Time:150.7683253288269\n",
      "Epoch: 60 - Batch: 69568 - Loss: 0.643657 - Time:160.17919445037842\n",
      "Epoch: 60 - Batch: 73664 - Loss: 0.303487 - Time:169.6074423789978\n",
      "Epoch: 60 - Batch: 77760 - Loss: 0.112568 - Time:179.01934695243835\n",
      "Epoch: 60 - Batch: 81856 - Loss: 0.051514 - Time:188.4300513267517\n",
      "Epoch: 60 - Batch: 85952 - Loss: 0.260219 - Time:197.84156560897827\n",
      "Epoch: 60 - Batch: 90048 - Loss: 0.288914 - Time:207.2679889202118\n",
      "Epoch: 60 - Batch: 94144 - Loss: 0.063516 - Time:216.67964792251587\n",
      "Epoch: 60 - Batch: 98240 - Loss: 0.197454 - Time:226.09028673171997\n",
      "Epoch: 60 - Batch: 102336 - Loss: 0.160849 - Time:235.52073884010315\n",
      "Epoch: 60 - Batch: 106432 - Loss: 0.315645 - Time:244.93312859535217\n",
      "Epoch: 60 - Batch: 110528 - Loss: 0.357236 - Time:254.3453929424286\n",
      "Epoch: 60 - Batch: 114624 - Loss: 0.231278 - Time:263.75495958328247\n",
      "Epoch: 60 - Batch: 118720 - Loss: 0.138999 - Time:273.16541028022766\n",
      "Epoch: 60 - Batch: 122816 - Loss: 0.054650 - Time:282.5741333961487\n",
      "Epoch: 60 - Batch: 126912 - Loss: 0.148259 - Time:291.98457860946655\n",
      "Epoch: 60 - Batch: 131008 - Loss: 0.039812 - Time:301.3953974246979\n",
      "Epoch: 60 - Batch: 135104 - Loss: 0.103472 - Time:310.82321763038635\n",
      "Epoch: 60 - Batch: 139200 - Loss: 0.168001 - Time:320.2347753047943\n",
      "Epoch: 60 - Batch: 143296 - Loss: 0.104213 - Time:329.6440637111664\n",
      "Epoch: 60 - Batch: 147392 - Loss: 0.093731 - Time:339.0553970336914\n",
      "Epoch: 60 - Batch: 151488 - Loss: 0.075041 - Time:348.48420786857605\n",
      "Epoch: 61 - Batch: 4032 - Loss: 0.240865 - Time:9.523556470870972\n",
      "Epoch: 61 - Batch: 8128 - Loss: 0.074943 - Time:18.93143057823181\n",
      "Epoch: 61 - Batch: 12224 - Loss: 0.357564 - Time:28.339726209640503\n",
      "Epoch: 61 - Batch: 16320 - Loss: 0.340430 - Time:37.77837347984314\n",
      "Epoch: 61 - Batch: 20416 - Loss: 0.095718 - Time:47.18818640708923\n",
      "Epoch: 61 - Batch: 24512 - Loss: 0.154236 - Time:56.59656095504761\n",
      "Epoch: 61 - Batch: 28608 - Loss: 0.031326 - Time:66.02398014068604\n",
      "Epoch: 61 - Batch: 32704 - Loss: 0.138702 - Time:75.43362617492676\n",
      "Epoch: 61 - Batch: 36800 - Loss: 0.157904 - Time:84.84300208091736\n",
      "Epoch: 61 - Batch: 40896 - Loss: 0.031579 - Time:94.25169396400452\n",
      "Epoch: 61 - Batch: 44992 - Loss: 0.180554 - Time:103.6606376171112\n",
      "Epoch: 61 - Batch: 49088 - Loss: 0.201043 - Time:113.07062196731567\n",
      "Epoch: 61 - Batch: 53184 - Loss: 0.249783 - Time:122.48186182975769\n",
      "Epoch: 61 - Batch: 57280 - Loss: 0.135118 - Time:131.89325523376465\n",
      "Epoch: 61 - Batch: 61376 - Loss: 0.349477 - Time:141.3216860294342\n",
      "Epoch: 61 - Batch: 65472 - Loss: 0.116781 - Time:150.73332166671753\n",
      "Epoch: 61 - Batch: 69568 - Loss: 0.137678 - Time:160.1458556652069\n",
      "Epoch: 61 - Batch: 73664 - Loss: 0.114159 - Time:169.55502772331238\n",
      "Epoch: 61 - Batch: 77760 - Loss: 0.100448 - Time:178.98343753814697\n",
      "Epoch: 61 - Batch: 81856 - Loss: 0.118280 - Time:188.39307403564453\n",
      "Epoch: 61 - Batch: 85952 - Loss: 0.595834 - Time:197.80471229553223\n",
      "Epoch: 61 - Batch: 90048 - Loss: 0.152575 - Time:207.2319209575653\n",
      "Epoch: 61 - Batch: 94144 - Loss: 0.255545 - Time:216.64232635498047\n",
      "Epoch: 61 - Batch: 98240 - Loss: 0.553650 - Time:226.05271458625793\n",
      "Epoch: 61 - Batch: 102336 - Loss: 0.151931 - Time:235.46290063858032\n",
      "Epoch: 61 - Batch: 106432 - Loss: 0.241816 - Time:244.87400197982788\n",
      "Epoch: 61 - Batch: 110528 - Loss: 0.053323 - Time:254.2836310863495\n",
      "Epoch: 61 - Batch: 114624 - Loss: 0.122979 - Time:263.6915521621704\n",
      "Epoch: 61 - Batch: 118720 - Loss: 0.132075 - Time:273.1000874042511\n",
      "Epoch: 61 - Batch: 122816 - Loss: 0.117752 - Time:282.5253839492798\n",
      "Epoch: 61 - Batch: 126912 - Loss: 0.231550 - Time:291.9339349269867\n",
      "Epoch: 61 - Batch: 131008 - Loss: 0.432712 - Time:301.3428637981415\n",
      "Epoch: 61 - Batch: 135104 - Loss: 0.322625 - Time:310.75154995918274\n",
      "Epoch: 61 - Batch: 139200 - Loss: 0.117967 - Time:320.1767122745514\n",
      "Epoch: 61 - Batch: 143296 - Loss: 0.394878 - Time:329.5859122276306\n",
      "Epoch: 61 - Batch: 147392 - Loss: 0.235449 - Time:338.9949505329132\n",
      "Epoch: 61 - Batch: 151488 - Loss: 0.112362 - Time:348.42047905921936\n",
      "Epoch: 62 - Batch: 4032 - Loss: 0.392983 - Time:9.549532651901245\n",
      "Epoch: 62 - Batch: 8128 - Loss: 0.021882 - Time:18.95896029472351\n",
      "Epoch: 62 - Batch: 12224 - Loss: 0.241683 - Time:28.398890733718872\n",
      "Epoch: 62 - Batch: 16320 - Loss: 0.148835 - Time:37.81042528152466\n",
      "Epoch: 62 - Batch: 20416 - Loss: 0.067994 - Time:47.2207727432251\n",
      "Epoch: 62 - Batch: 24512 - Loss: 0.214109 - Time:56.63054323196411\n",
      "Epoch: 62 - Batch: 28608 - Loss: 0.046514 - Time:66.04127645492554\n",
      "Epoch: 62 - Batch: 32704 - Loss: 0.116396 - Time:75.45320582389832\n",
      "Epoch: 62 - Batch: 36800 - Loss: 0.291544 - Time:84.86474227905273\n",
      "Epoch: 62 - Batch: 40896 - Loss: 0.044603 - Time:94.27129745483398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 - Batch: 44992 - Loss: 0.027676 - Time:103.69446229934692\n",
      "Epoch: 62 - Batch: 49088 - Loss: 0.050074 - Time:113.10115194320679\n",
      "Epoch: 62 - Batch: 53184 - Loss: 0.261526 - Time:122.51018595695496\n",
      "Epoch: 62 - Batch: 57280 - Loss: 0.195053 - Time:131.9165642261505\n",
      "Epoch: 62 - Batch: 61376 - Loss: 0.317599 - Time:141.3410439491272\n",
      "Epoch: 62 - Batch: 65472 - Loss: 0.205439 - Time:150.74792170524597\n",
      "Epoch: 62 - Batch: 69568 - Loss: 0.115827 - Time:160.15686416625977\n",
      "Epoch: 62 - Batch: 73664 - Loss: 0.229991 - Time:169.58136224746704\n",
      "Epoch: 62 - Batch: 77760 - Loss: 0.103138 - Time:178.9909257888794\n",
      "Epoch: 62 - Batch: 81856 - Loss: 0.424760 - Time:188.40062546730042\n",
      "Epoch: 62 - Batch: 85952 - Loss: 0.159701 - Time:197.80877900123596\n",
      "Epoch: 62 - Batch: 90048 - Loss: 0.081831 - Time:207.21651911735535\n",
      "Epoch: 62 - Batch: 94144 - Loss: 0.266801 - Time:216.6252601146698\n",
      "Epoch: 62 - Batch: 98240 - Loss: 0.449362 - Time:226.03400921821594\n",
      "Epoch: 62 - Batch: 102336 - Loss: 0.243914 - Time:235.44319438934326\n",
      "Epoch: 62 - Batch: 106432 - Loss: 0.190621 - Time:244.87162685394287\n",
      "Epoch: 62 - Batch: 110528 - Loss: 0.065187 - Time:254.27892351150513\n",
      "Epoch: 62 - Batch: 114624 - Loss: 0.105903 - Time:263.6873679161072\n",
      "Epoch: 62 - Batch: 118720 - Loss: 0.291355 - Time:273.09718441963196\n",
      "Epoch: 62 - Batch: 122816 - Loss: 0.147938 - Time:282.50646209716797\n",
      "Epoch: 62 - Batch: 126912 - Loss: 0.382604 - Time:291.9311308860779\n",
      "Epoch: 62 - Batch: 131008 - Loss: 0.126187 - Time:301.33943152427673\n",
      "Epoch: 62 - Batch: 135104 - Loss: 0.161034 - Time:310.7650330066681\n",
      "Epoch: 62 - Batch: 139200 - Loss: 0.099019 - Time:320.1720473766327\n",
      "Epoch: 62 - Batch: 143296 - Loss: 0.346909 - Time:329.57846093177795\n",
      "Epoch: 62 - Batch: 147392 - Loss: 0.114768 - Time:338.98742032051086\n",
      "Epoch: 62 - Batch: 151488 - Loss: 0.390368 - Time:348.3988037109375\n",
      "Epoch: 63 - Batch: 4032 - Loss: 0.054034 - Time:9.565179347991943\n",
      "Epoch: 63 - Batch: 8128 - Loss: 0.056012 - Time:19.002519607543945\n",
      "Epoch: 63 - Batch: 12224 - Loss: 0.052576 - Time:28.4138503074646\n",
      "Epoch: 63 - Batch: 16320 - Loss: 0.026352 - Time:37.825270891189575\n",
      "Epoch: 63 - Batch: 20416 - Loss: 0.055621 - Time:47.23507070541382\n",
      "Epoch: 63 - Batch: 24512 - Loss: 0.196927 - Time:56.64264130592346\n",
      "Epoch: 63 - Batch: 28608 - Loss: 0.038808 - Time:66.04983472824097\n",
      "Epoch: 63 - Batch: 32704 - Loss: 0.170350 - Time:75.45740413665771\n",
      "Epoch: 63 - Batch: 36800 - Loss: 0.174685 - Time:84.86681342124939\n",
      "Epoch: 63 - Batch: 40896 - Loss: 0.122325 - Time:94.29395842552185\n",
      "Epoch: 63 - Batch: 44992 - Loss: 0.168728 - Time:103.70316457748413\n",
      "Epoch: 63 - Batch: 49088 - Loss: 0.064853 - Time:113.11406135559082\n",
      "Epoch: 63 - Batch: 53184 - Loss: 0.032947 - Time:122.52335929870605\n",
      "Epoch: 63 - Batch: 57280 - Loss: 0.157428 - Time:131.94934225082397\n",
      "Epoch: 63 - Batch: 61376 - Loss: 0.081685 - Time:141.35816836357117\n",
      "Epoch: 63 - Batch: 65472 - Loss: 0.072733 - Time:150.7667944431305\n",
      "Epoch: 63 - Batch: 69568 - Loss: 0.026990 - Time:160.19225144386292\n",
      "Epoch: 63 - Batch: 73664 - Loss: 0.052048 - Time:169.60122084617615\n",
      "Epoch: 63 - Batch: 77760 - Loss: 0.175140 - Time:179.0118236541748\n",
      "Epoch: 63 - Batch: 81856 - Loss: 0.045389 - Time:188.42462396621704\n",
      "Epoch: 63 - Batch: 85952 - Loss: 0.173214 - Time:197.83450508117676\n",
      "Epoch: 63 - Batch: 90048 - Loss: 0.130282 - Time:207.24448037147522\n",
      "Epoch: 63 - Batch: 94144 - Loss: 0.214356 - Time:216.6538941860199\n",
      "Epoch: 63 - Batch: 98240 - Loss: 0.100198 - Time:226.06317377090454\n",
      "Epoch: 63 - Batch: 102336 - Loss: 0.208402 - Time:235.48798298835754\n",
      "Epoch: 63 - Batch: 106432 - Loss: 0.144783 - Time:244.89618921279907\n",
      "Epoch: 63 - Batch: 110528 - Loss: 0.031089 - Time:254.30454850196838\n",
      "Epoch: 63 - Batch: 114624 - Loss: 0.098383 - Time:263.71219754219055\n",
      "Epoch: 63 - Batch: 118720 - Loss: 0.051365 - Time:273.13877987861633\n",
      "Epoch: 63 - Batch: 122816 - Loss: 0.169525 - Time:282.54700779914856\n",
      "Epoch: 63 - Batch: 126912 - Loss: 0.051791 - Time:291.95515155792236\n",
      "Epoch: 63 - Batch: 131008 - Loss: 0.110440 - Time:301.3802914619446\n",
      "Epoch: 63 - Batch: 135104 - Loss: 0.154483 - Time:310.79123854637146\n",
      "Epoch: 63 - Batch: 139200 - Loss: 0.212711 - Time:320.20309925079346\n",
      "Epoch: 63 - Batch: 143296 - Loss: 0.127124 - Time:329.611465215683\n",
      "Epoch: 63 - Batch: 147392 - Loss: 0.169131 - Time:339.02336597442627\n",
      "Epoch: 63 - Batch: 151488 - Loss: 0.082542 - Time:348.43637132644653\n",
      "Epoch: 64 - Batch: 4032 - Loss: 0.035107 - Time:9.553409814834595\n",
      "Epoch: 64 - Batch: 8128 - Loss: 0.218152 - Time:18.963691234588623\n",
      "Epoch: 64 - Batch: 12224 - Loss: 0.201144 - Time:28.400474548339844\n",
      "Epoch: 64 - Batch: 16320 - Loss: 0.261465 - Time:37.8137001991272\n",
      "Epoch: 64 - Batch: 20416 - Loss: 0.293522 - Time:47.22484278678894\n",
      "Epoch: 64 - Batch: 24512 - Loss: 0.084634 - Time:56.63545083999634\n",
      "Epoch: 64 - Batch: 28608 - Loss: 0.054190 - Time:66.06392097473145\n",
      "Epoch: 64 - Batch: 32704 - Loss: 0.061703 - Time:75.47495794296265\n",
      "Epoch: 64 - Batch: 36800 - Loss: 0.046761 - Time:84.88631439208984\n",
      "Epoch: 64 - Batch: 40896 - Loss: 0.056239 - Time:94.31255316734314\n",
      "Epoch: 64 - Batch: 44992 - Loss: 0.362456 - Time:103.72346329689026\n",
      "Epoch: 64 - Batch: 49088 - Loss: 0.060799 - Time:113.13330030441284\n",
      "Epoch: 64 - Batch: 53184 - Loss: 0.243630 - Time:122.54350113868713\n",
      "Epoch: 64 - Batch: 57280 - Loss: 0.047788 - Time:131.9563171863556\n",
      "Epoch: 64 - Batch: 61376 - Loss: 0.092609 - Time:141.3661880493164\n",
      "Epoch: 64 - Batch: 65472 - Loss: 0.228719 - Time:150.7774908542633\n",
      "Epoch: 64 - Batch: 69568 - Loss: 0.081675 - Time:160.18786811828613\n",
      "Epoch: 64 - Batch: 73664 - Loss: 0.072319 - Time:169.61673736572266\n",
      "Epoch: 64 - Batch: 77760 - Loss: 0.074396 - Time:179.0271725654602\n",
      "Epoch: 64 - Batch: 81856 - Loss: 0.399477 - Time:188.43765091896057\n",
      "Epoch: 64 - Batch: 85952 - Loss: 0.073305 - Time:197.84957313537598\n",
      "Epoch: 64 - Batch: 90048 - Loss: 0.042190 - Time:207.27674436569214\n",
      "Epoch: 64 - Batch: 94144 - Loss: 0.131594 - Time:216.6865963935852\n",
      "Epoch: 64 - Batch: 98240 - Loss: 0.353383 - Time:226.09767413139343\n",
      "Epoch: 64 - Batch: 102336 - Loss: 0.225255 - Time:235.52492904663086\n",
      "Epoch: 64 - Batch: 106432 - Loss: 0.073745 - Time:244.93530225753784\n",
      "Epoch: 64 - Batch: 110528 - Loss: 0.036711 - Time:254.34709882736206\n",
      "Epoch: 64 - Batch: 114624 - Loss: 0.212702 - Time:263.7600886821747\n",
      "Epoch: 64 - Batch: 118720 - Loss: 0.136489 - Time:273.17202496528625\n",
      "Epoch: 64 - Batch: 122816 - Loss: 0.224875 - Time:282.58238983154297\n",
      "Epoch: 64 - Batch: 126912 - Loss: 0.056866 - Time:291.9923584461212\n",
      "Epoch: 64 - Batch: 131008 - Loss: 0.144384 - Time:301.4013226032257\n",
      "Epoch: 64 - Batch: 135104 - Loss: 0.103255 - Time:310.8315100669861\n",
      "Epoch: 64 - Batch: 139200 - Loss: 0.504311 - Time:320.2402720451355\n",
      "Epoch: 64 - Batch: 143296 - Loss: 0.042629 - Time:329.6506087779999\n",
      "Epoch: 64 - Batch: 147392 - Loss: 0.119467 - Time:339.0610523223877\n",
      "Epoch: 64 - Batch: 151488 - Loss: 0.214145 - Time:348.48863077163696\n",
      "Epoch: 65 - Batch: 4032 - Loss: 0.054576 - Time:9.568953037261963\n",
      "Epoch: 65 - Batch: 8128 - Loss: 0.196695 - Time:18.980304956436157\n",
      "Epoch: 65 - Batch: 12224 - Loss: 0.106716 - Time:28.41914176940918\n",
      "Epoch: 65 - Batch: 16320 - Loss: 0.224449 - Time:37.82936954498291\n",
      "Epoch: 65 - Batch: 20416 - Loss: 0.169967 - Time:47.24206876754761\n",
      "Epoch: 65 - Batch: 24512 - Loss: 0.042978 - Time:56.670576333999634\n",
      "Epoch: 65 - Batch: 28608 - Loss: 0.117564 - Time:66.0813410282135\n",
      "Epoch: 65 - Batch: 32704 - Loss: 0.072833 - Time:75.49521017074585\n",
      "Epoch: 65 - Batch: 36800 - Loss: 0.118220 - Time:84.90821719169617\n",
      "Epoch: 65 - Batch: 40896 - Loss: 0.041393 - Time:94.3202395439148\n",
      "Epoch: 65 - Batch: 44992 - Loss: 0.188617 - Time:103.72989916801453\n",
      "Epoch: 65 - Batch: 49088 - Loss: 0.361037 - Time:113.1407561302185\n",
      "Epoch: 65 - Batch: 53184 - Loss: 0.361950 - Time:122.55150651931763\n",
      "Epoch: 65 - Batch: 57280 - Loss: 0.199595 - Time:131.98125171661377\n",
      "Epoch: 65 - Batch: 61376 - Loss: 0.145045 - Time:141.39370703697205\n",
      "Epoch: 65 - Batch: 65472 - Loss: 0.115796 - Time:150.8037130832672\n",
      "Epoch: 65 - Batch: 69568 - Loss: 0.030095 - Time:160.21503591537476\n",
      "Epoch: 65 - Batch: 73664 - Loss: 0.337341 - Time:169.62546491622925\n",
      "Epoch: 65 - Batch: 77760 - Loss: 0.092520 - Time:179.0529019832611\n",
      "Epoch: 65 - Batch: 81856 - Loss: 0.056030 - Time:188.46311473846436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 - Batch: 85952 - Loss: 0.186630 - Time:197.87468004226685\n",
      "Epoch: 65 - Batch: 90048 - Loss: 0.029781 - Time:207.30205512046814\n",
      "Epoch: 65 - Batch: 94144 - Loss: 0.100494 - Time:216.71324968338013\n",
      "Epoch: 65 - Batch: 98240 - Loss: 0.179024 - Time:226.12428975105286\n",
      "Epoch: 65 - Batch: 102336 - Loss: 0.174619 - Time:235.53337860107422\n",
      "Epoch: 65 - Batch: 106432 - Loss: 0.223231 - Time:244.94331812858582\n",
      "Epoch: 65 - Batch: 110528 - Loss: 0.195474 - Time:254.35431623458862\n",
      "Epoch: 65 - Batch: 114624 - Loss: 0.349708 - Time:263.76633882522583\n",
      "Epoch: 65 - Batch: 118720 - Loss: 0.079689 - Time:273.1784155368805\n",
      "Epoch: 65 - Batch: 122816 - Loss: 0.456154 - Time:282.6062059402466\n",
      "Epoch: 65 - Batch: 126912 - Loss: 0.036730 - Time:292.0161051750183\n",
      "Epoch: 65 - Batch: 131008 - Loss: 0.144821 - Time:301.4264235496521\n",
      "Epoch: 65 - Batch: 135104 - Loss: 0.060010 - Time:310.8395700454712\n",
      "Epoch: 65 - Batch: 139200 - Loss: 0.153934 - Time:320.2687141895294\n",
      "Epoch: 65 - Batch: 143296 - Loss: 0.410121 - Time:329.67866039276123\n",
      "Epoch: 65 - Batch: 147392 - Loss: 0.154799 - Time:339.0860159397125\n",
      "Epoch: 65 - Batch: 151488 - Loss: 0.109726 - Time:348.5122661590576\n",
      "Epoch: 66 - Batch: 4032 - Loss: 0.033047 - Time:9.547031164169312\n",
      "Epoch: 66 - Batch: 8128 - Loss: 0.234405 - Time:18.95610737800598\n",
      "Epoch: 66 - Batch: 12224 - Loss: 0.187116 - Time:28.366618633270264\n",
      "Epoch: 66 - Batch: 16320 - Loss: 0.095977 - Time:37.77785325050354\n",
      "Epoch: 66 - Batch: 20416 - Loss: 0.040255 - Time:47.18778657913208\n",
      "Epoch: 66 - Batch: 24512 - Loss: 0.109592 - Time:56.59838533401489\n",
      "Epoch: 66 - Batch: 28608 - Loss: 0.066883 - Time:66.03472185134888\n",
      "Epoch: 66 - Batch: 32704 - Loss: 0.122362 - Time:75.44540405273438\n",
      "Epoch: 66 - Batch: 36800 - Loss: 0.082675 - Time:84.85501098632812\n",
      "Epoch: 66 - Batch: 40896 - Loss: 0.091714 - Time:94.26532888412476\n",
      "Epoch: 66 - Batch: 44992 - Loss: 0.045985 - Time:103.69590091705322\n",
      "Epoch: 66 - Batch: 49088 - Loss: 0.173912 - Time:113.10695481300354\n",
      "Epoch: 66 - Batch: 53184 - Loss: 0.177491 - Time:122.51847696304321\n",
      "Epoch: 66 - Batch: 57280 - Loss: 0.222472 - Time:131.94600820541382\n",
      "Epoch: 66 - Batch: 61376 - Loss: 0.067453 - Time:141.35940599441528\n",
      "Epoch: 66 - Batch: 65472 - Loss: 0.241208 - Time:150.77200412750244\n",
      "Epoch: 66 - Batch: 69568 - Loss: 0.105393 - Time:160.1810417175293\n",
      "Epoch: 66 - Batch: 73664 - Loss: 0.091566 - Time:169.59078884124756\n",
      "Epoch: 66 - Batch: 77760 - Loss: 0.133988 - Time:179.0020728111267\n",
      "Epoch: 66 - Batch: 81856 - Loss: 0.220415 - Time:188.4120090007782\n",
      "Epoch: 66 - Batch: 85952 - Loss: 0.113955 - Time:197.82244515419006\n",
      "Epoch: 66 - Batch: 90048 - Loss: 0.071413 - Time:207.24988794326782\n",
      "Epoch: 66 - Batch: 94144 - Loss: 0.051735 - Time:216.656494140625\n",
      "Epoch: 66 - Batch: 98240 - Loss: 0.147454 - Time:226.0622534751892\n",
      "Epoch: 66 - Batch: 102336 - Loss: 0.384803 - Time:235.4704566001892\n",
      "Epoch: 66 - Batch: 106432 - Loss: 0.166916 - Time:244.87903833389282\n",
      "Epoch: 66 - Batch: 110528 - Loss: 0.212073 - Time:254.30647945404053\n",
      "Epoch: 66 - Batch: 114624 - Loss: 0.058400 - Time:263.71845388412476\n",
      "Epoch: 66 - Batch: 118720 - Loss: 0.054618 - Time:273.1290969848633\n",
      "Epoch: 66 - Batch: 122816 - Loss: 0.435433 - Time:282.55626821517944\n",
      "Epoch: 66 - Batch: 126912 - Loss: 0.134599 - Time:291.9647169113159\n",
      "Epoch: 66 - Batch: 131008 - Loss: 0.061808 - Time:301.37687611579895\n",
      "Epoch: 66 - Batch: 135104 - Loss: 0.064981 - Time:310.7881543636322\n",
      "Epoch: 66 - Batch: 139200 - Loss: 0.499016 - Time:320.2013747692108\n",
      "Epoch: 66 - Batch: 143296 - Loss: 0.556351 - Time:329.6124458312988\n",
      "Epoch: 66 - Batch: 147392 - Loss: 0.092908 - Time:339.02234268188477\n",
      "Epoch: 66 - Batch: 151488 - Loss: 0.273002 - Time:348.434202671051\n",
      "Epoch: 67 - Batch: 4032 - Loss: 0.031563 - Time:9.551148891448975\n",
      "Epoch: 67 - Batch: 8128 - Loss: 0.177706 - Time:18.96359872817993\n",
      "Epoch: 67 - Batch: 12224 - Loss: 0.089054 - Time:28.376818418502808\n",
      "Epoch: 67 - Batch: 16320 - Loss: 0.051407 - Time:37.788596868515015\n",
      "Epoch: 67 - Batch: 20416 - Loss: 0.123060 - Time:47.20056366920471\n",
      "Epoch: 67 - Batch: 24512 - Loss: 0.164012 - Time:56.612847566604614\n",
      "Epoch: 67 - Batch: 28608 - Loss: 0.091252 - Time:66.02547931671143\n",
      "Epoch: 67 - Batch: 32704 - Loss: 0.055111 - Time:75.4626612663269\n",
      "Epoch: 67 - Batch: 36800 - Loss: 0.151048 - Time:84.87450742721558\n",
      "Epoch: 67 - Batch: 40896 - Loss: 0.087339 - Time:94.28804969787598\n",
      "Epoch: 67 - Batch: 44992 - Loss: 0.044677 - Time:103.7020514011383\n",
      "Epoch: 67 - Batch: 49088 - Loss: 0.058735 - Time:113.1310703754425\n",
      "Epoch: 67 - Batch: 53184 - Loss: 0.072767 - Time:122.54192304611206\n",
      "Epoch: 67 - Batch: 57280 - Loss: 0.097025 - Time:131.9546582698822\n",
      "Epoch: 67 - Batch: 61376 - Loss: 0.114566 - Time:141.3822774887085\n",
      "Epoch: 67 - Batch: 65472 - Loss: 0.258521 - Time:150.7947747707367\n",
      "Epoch: 67 - Batch: 69568 - Loss: 0.103466 - Time:160.20429944992065\n",
      "Epoch: 67 - Batch: 73664 - Loss: 0.245934 - Time:169.6145293712616\n",
      "Epoch: 67 - Batch: 77760 - Loss: 0.229561 - Time:179.02309226989746\n",
      "Epoch: 67 - Batch: 81856 - Loss: 0.503214 - Time:188.4338653087616\n",
      "Epoch: 67 - Batch: 85952 - Loss: 0.063549 - Time:197.84210538864136\n",
      "Epoch: 67 - Batch: 90048 - Loss: 0.145654 - Time:207.25347447395325\n",
      "Epoch: 67 - Batch: 94144 - Loss: 0.092002 - Time:216.68124794960022\n",
      "Epoch: 67 - Batch: 98240 - Loss: 0.357098 - Time:226.0929560661316\n",
      "Epoch: 67 - Batch: 102336 - Loss: 0.329993 - Time:235.5042326450348\n",
      "Epoch: 67 - Batch: 106432 - Loss: 0.176781 - Time:244.91579151153564\n",
      "Epoch: 67 - Batch: 110528 - Loss: 0.033951 - Time:254.34473848342896\n",
      "Epoch: 67 - Batch: 114624 - Loss: 0.075398 - Time:263.7555100917816\n",
      "Epoch: 67 - Batch: 118720 - Loss: 0.028447 - Time:273.16485714912415\n",
      "Epoch: 67 - Batch: 122816 - Loss: 0.073661 - Time:282.5912837982178\n",
      "Epoch: 67 - Batch: 126912 - Loss: 0.069781 - Time:292.0019452571869\n",
      "Epoch: 67 - Batch: 131008 - Loss: 0.094579 - Time:301.41356348991394\n",
      "Epoch: 67 - Batch: 135104 - Loss: 0.107812 - Time:310.8222141265869\n",
      "Epoch: 67 - Batch: 139200 - Loss: 0.079758 - Time:320.2305715084076\n",
      "Epoch: 67 - Batch: 143296 - Loss: 0.315302 - Time:329.6414706707001\n",
      "Epoch: 67 - Batch: 147392 - Loss: 0.225116 - Time:339.04927349090576\n",
      "Epoch: 67 - Batch: 151488 - Loss: 0.027712 - Time:348.45817160606384\n",
      "Epoch: 68 - Batch: 4032 - Loss: 0.229226 - Time:9.565242767333984\n",
      "Epoch: 68 - Batch: 8128 - Loss: 0.538515 - Time:18.97770404815674\n",
      "Epoch: 68 - Batch: 12224 - Loss: 0.402940 - Time:28.38738226890564\n",
      "Epoch: 68 - Batch: 16320 - Loss: 0.191775 - Time:37.79815411567688\n",
      "Epoch: 68 - Batch: 20416 - Loss: 0.066975 - Time:47.22375798225403\n",
      "Epoch: 68 - Batch: 24512 - Loss: 0.203769 - Time:56.633458614349365\n",
      "Epoch: 68 - Batch: 28608 - Loss: 0.163863 - Time:66.04315900802612\n",
      "Epoch: 68 - Batch: 32704 - Loss: 0.103956 - Time:75.46860766410828\n",
      "Epoch: 68 - Batch: 36800 - Loss: 0.236018 - Time:84.87744474411011\n",
      "Epoch: 68 - Batch: 40896 - Loss: 0.156183 - Time:94.2853844165802\n",
      "Epoch: 68 - Batch: 44992 - Loss: 0.146289 - Time:103.69426918029785\n",
      "Epoch: 68 - Batch: 49088 - Loss: 0.113960 - Time:113.102703332901\n",
      "Epoch: 68 - Batch: 53184 - Loss: 0.039172 - Time:122.51243472099304\n",
      "Epoch: 68 - Batch: 57280 - Loss: 0.082204 - Time:131.92234659194946\n",
      "Epoch: 68 - Batch: 61376 - Loss: 0.101950 - Time:141.33073711395264\n",
      "Epoch: 68 - Batch: 65472 - Loss: 0.042853 - Time:150.75817728042603\n",
      "Epoch: 68 - Batch: 69568 - Loss: 0.157241 - Time:160.16751194000244\n",
      "Epoch: 68 - Batch: 73664 - Loss: 0.109062 - Time:169.5757486820221\n",
      "Epoch: 68 - Batch: 77760 - Loss: 0.688743 - Time:178.98307466506958\n",
      "Epoch: 68 - Batch: 81856 - Loss: 0.144464 - Time:188.40764355659485\n",
      "Epoch: 68 - Batch: 85952 - Loss: 0.136326 - Time:197.814772605896\n",
      "Epoch: 68 - Batch: 90048 - Loss: 0.093024 - Time:207.22308826446533\n",
      "Epoch: 68 - Batch: 94144 - Loss: 0.070116 - Time:216.64826679229736\n",
      "Epoch: 68 - Batch: 98240 - Loss: 0.067935 - Time:226.05717587471008\n",
      "Epoch: 68 - Batch: 102336 - Loss: 0.107419 - Time:235.46848511695862\n",
      "Epoch: 68 - Batch: 106432 - Loss: 0.073211 - Time:244.8806984424591\n",
      "Epoch: 68 - Batch: 110528 - Loss: 0.043367 - Time:254.29140329360962\n",
      "Epoch: 68 - Batch: 114624 - Loss: 0.521905 - Time:263.7016351222992\n",
      "Epoch: 68 - Batch: 118720 - Loss: 0.032237 - Time:273.11233830451965\n",
      "Epoch: 68 - Batch: 122816 - Loss: 0.242262 - Time:282.52350211143494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 - Batch: 126912 - Loss: 0.057805 - Time:291.9528422355652\n",
      "Epoch: 68 - Batch: 131008 - Loss: 0.224394 - Time:301.3642301559448\n",
      "Epoch: 68 - Batch: 135104 - Loss: 0.180301 - Time:310.7740693092346\n",
      "Epoch: 68 - Batch: 139200 - Loss: 0.047032 - Time:320.18550729751587\n",
      "Epoch: 68 - Batch: 143296 - Loss: 0.037925 - Time:329.61404824256897\n",
      "Epoch: 68 - Batch: 147392 - Loss: 0.151780 - Time:339.02600598335266\n",
      "Epoch: 68 - Batch: 151488 - Loss: 0.046315 - Time:348.4355573654175\n",
      "Epoch: 69 - Batch: 4032 - Loss: 0.198888 - Time:9.556978464126587\n",
      "Epoch: 69 - Batch: 8128 - Loss: 0.073280 - Time:18.966484785079956\n",
      "Epoch: 69 - Batch: 12224 - Loss: 0.025924 - Time:28.376256227493286\n",
      "Epoch: 69 - Batch: 16320 - Loss: 0.131707 - Time:37.80354428291321\n",
      "Epoch: 69 - Batch: 20416 - Loss: 0.047919 - Time:47.212992668151855\n",
      "Epoch: 69 - Batch: 24512 - Loss: 0.022324 - Time:56.62195372581482\n",
      "Epoch: 69 - Batch: 28608 - Loss: 0.335997 - Time:66.03097128868103\n",
      "Epoch: 69 - Batch: 32704 - Loss: 0.101857 - Time:75.44068312644958\n",
      "Epoch: 69 - Batch: 36800 - Loss: 0.116795 - Time:84.8500804901123\n",
      "Epoch: 69 - Batch: 40896 - Loss: 0.227604 - Time:94.25966238975525\n",
      "Epoch: 69 - Batch: 44992 - Loss: 0.172128 - Time:103.67161154747009\n",
      "Epoch: 69 - Batch: 49088 - Loss: 0.039890 - Time:113.09802222251892\n",
      "Epoch: 69 - Batch: 53184 - Loss: 0.136179 - Time:122.50758600234985\n",
      "Epoch: 69 - Batch: 57280 - Loss: 0.666167 - Time:131.91751980781555\n",
      "Epoch: 69 - Batch: 61376 - Loss: 0.102935 - Time:141.32579350471497\n",
      "Epoch: 69 - Batch: 65472 - Loss: 0.060914 - Time:150.75173592567444\n",
      "Epoch: 69 - Batch: 69568 - Loss: 0.282155 - Time:160.1631953716278\n",
      "Epoch: 69 - Batch: 73664 - Loss: 0.034424 - Time:169.57081985473633\n",
      "Epoch: 69 - Batch: 77760 - Loss: 0.047395 - Time:178.99810075759888\n",
      "Epoch: 69 - Batch: 81856 - Loss: 0.041766 - Time:188.40707206726074\n",
      "Epoch: 69 - Batch: 85952 - Loss: 0.076330 - Time:197.8139042854309\n",
      "Epoch: 69 - Batch: 90048 - Loss: 0.047428 - Time:207.22359490394592\n",
      "Epoch: 69 - Batch: 94144 - Loss: 0.188486 - Time:216.63490843772888\n",
      "Epoch: 69 - Batch: 98240 - Loss: 0.223056 - Time:226.04513144493103\n",
      "Epoch: 69 - Batch: 102336 - Loss: 0.205533 - Time:235.45456647872925\n",
      "Epoch: 69 - Batch: 106432 - Loss: 0.271030 - Time:244.8633918762207\n",
      "Epoch: 69 - Batch: 110528 - Loss: 0.167046 - Time:254.29182648658752\n",
      "Epoch: 69 - Batch: 114624 - Loss: 0.113378 - Time:263.70247411727905\n",
      "Epoch: 69 - Batch: 118720 - Loss: 0.108146 - Time:273.1153860092163\n",
      "Epoch: 69 - Batch: 122816 - Loss: 0.390341 - Time:282.52599692344666\n",
      "Epoch: 69 - Batch: 126912 - Loss: 0.066591 - Time:291.9540181159973\n",
      "Epoch: 69 - Batch: 131008 - Loss: 0.157329 - Time:301.3654453754425\n",
      "Epoch: 69 - Batch: 135104 - Loss: 0.107012 - Time:310.7792148590088\n",
      "Epoch: 69 - Batch: 139200 - Loss: 0.077352 - Time:320.2065131664276\n",
      "Epoch: 69 - Batch: 143296 - Loss: 0.086918 - Time:329.6180317401886\n",
      "Epoch: 69 - Batch: 147392 - Loss: 0.096880 - Time:339.02767872810364\n",
      "Epoch: 69 - Batch: 151488 - Loss: 0.040456 - Time:348.43807101249695\n",
      "Epoch: 70 - Batch: 4032 - Loss: 0.026964 - Time:9.555868148803711\n",
      "Epoch: 70 - Batch: 8128 - Loss: 0.357925 - Time:18.9922833442688\n",
      "Epoch: 70 - Batch: 12224 - Loss: 0.186074 - Time:28.399775505065918\n",
      "Epoch: 70 - Batch: 16320 - Loss: 0.140708 - Time:37.808186531066895\n",
      "Epoch: 70 - Batch: 20416 - Loss: 0.046629 - Time:47.21695399284363\n",
      "Epoch: 70 - Batch: 24512 - Loss: 0.072775 - Time:56.626670122146606\n",
      "Epoch: 70 - Batch: 28608 - Loss: 0.069529 - Time:66.03657031059265\n",
      "Epoch: 70 - Batch: 32704 - Loss: 0.051494 - Time:75.44420218467712\n",
      "Epoch: 70 - Batch: 36800 - Loss: 0.062317 - Time:84.85116624832153\n",
      "Epoch: 70 - Batch: 40896 - Loss: 0.081891 - Time:94.27680087089539\n",
      "Epoch: 70 - Batch: 44992 - Loss: 0.246880 - Time:103.68407225608826\n",
      "Epoch: 70 - Batch: 49088 - Loss: 0.303826 - Time:113.09248971939087\n",
      "Epoch: 70 - Batch: 53184 - Loss: 0.104764 - Time:122.50104093551636\n",
      "Epoch: 70 - Batch: 57280 - Loss: 0.362731 - Time:131.9259934425354\n",
      "Epoch: 70 - Batch: 61376 - Loss: 0.041264 - Time:141.3343150615692\n",
      "Epoch: 70 - Batch: 65472 - Loss: 0.332355 - Time:150.74447631835938\n",
      "Epoch: 70 - Batch: 69568 - Loss: 0.039650 - Time:160.1729474067688\n",
      "Epoch: 70 - Batch: 73664 - Loss: 0.245918 - Time:169.5821192264557\n",
      "Epoch: 70 - Batch: 77760 - Loss: 0.106750 - Time:178.9914746284485\n",
      "Epoch: 70 - Batch: 81856 - Loss: 0.197524 - Time:188.4016978740692\n",
      "Epoch: 70 - Batch: 85952 - Loss: 0.089196 - Time:197.81463479995728\n",
      "Epoch: 70 - Batch: 90048 - Loss: 0.200859 - Time:207.22550177574158\n",
      "Epoch: 70 - Batch: 94144 - Loss: 0.024240 - Time:216.63549709320068\n",
      "Epoch: 70 - Batch: 98240 - Loss: 0.293168 - Time:226.04417371749878\n",
      "Epoch: 70 - Batch: 102336 - Loss: 0.132433 - Time:235.46916818618774\n",
      "Epoch: 70 - Batch: 106432 - Loss: 0.051406 - Time:244.88026762008667\n",
      "Epoch: 70 - Batch: 110528 - Loss: 0.107196 - Time:254.28823852539062\n",
      "Epoch: 70 - Batch: 114624 - Loss: 0.058546 - Time:263.69584226608276\n",
      "Epoch: 70 - Batch: 118720 - Loss: 0.047945 - Time:273.1042642593384\n",
      "Epoch: 70 - Batch: 122816 - Loss: 0.475200 - Time:282.5289375782013\n",
      "Epoch: 70 - Batch: 126912 - Loss: 0.238103 - Time:291.93833231925964\n",
      "Epoch: 70 - Batch: 131008 - Loss: 0.062077 - Time:301.344806432724\n",
      "Epoch: 70 - Batch: 135104 - Loss: 0.085457 - Time:310.76852893829346\n",
      "Epoch: 70 - Batch: 139200 - Loss: 0.031549 - Time:320.1758711338043\n",
      "Epoch: 70 - Batch: 143296 - Loss: 0.205826 - Time:329.5828769207001\n",
      "Epoch: 70 - Batch: 147392 - Loss: 0.068715 - Time:338.9904234409332\n",
      "Epoch: 70 - Batch: 151488 - Loss: 0.212789 - Time:348.39825201034546\n",
      "Epoch: 71 - Batch: 4032 - Loss: 0.041151 - Time:9.553833246231079\n",
      "Epoch: 71 - Batch: 8128 - Loss: 0.107863 - Time:18.963340044021606\n",
      "Epoch: 71 - Batch: 12224 - Loss: 0.112519 - Time:28.36945390701294\n",
      "Epoch: 71 - Batch: 16320 - Loss: 0.166163 - Time:37.77640891075134\n",
      "Epoch: 71 - Batch: 20416 - Loss: 0.109249 - Time:47.18391132354736\n",
      "Epoch: 71 - Batch: 24512 - Loss: 0.039149 - Time:56.590784788131714\n",
      "Epoch: 71 - Batch: 28608 - Loss: 0.279257 - Time:65.99841952323914\n",
      "Epoch: 71 - Batch: 32704 - Loss: 0.350133 - Time:75.407142162323\n",
      "Epoch: 71 - Batch: 36800 - Loss: 0.086309 - Time:84.83099436759949\n",
      "Epoch: 71 - Batch: 40896 - Loss: 0.493531 - Time:94.23854541778564\n",
      "Epoch: 71 - Batch: 44992 - Loss: 0.315174 - Time:103.64760518074036\n",
      "Epoch: 71 - Batch: 49088 - Loss: 0.189171 - Time:113.05635690689087\n",
      "Epoch: 71 - Batch: 53184 - Loss: 0.172550 - Time:122.4654049873352\n",
      "Epoch: 71 - Batch: 57280 - Loss: 0.034779 - Time:131.88920545578003\n",
      "Epoch: 71 - Batch: 61376 - Loss: 0.485862 - Time:141.2980875968933\n",
      "Epoch: 71 - Batch: 65472 - Loss: 0.056894 - Time:150.70505166053772\n",
      "Epoch: 71 - Batch: 69568 - Loss: 0.037036 - Time:160.1297950744629\n",
      "Epoch: 71 - Batch: 73664 - Loss: 0.056323 - Time:169.53869247436523\n",
      "Epoch: 71 - Batch: 77760 - Loss: 0.088945 - Time:178.94916701316833\n",
      "Epoch: 71 - Batch: 81856 - Loss: 0.139274 - Time:188.358553647995\n",
      "Epoch: 71 - Batch: 85952 - Loss: 0.184795 - Time:197.7648355960846\n",
      "Epoch: 71 - Batch: 90048 - Loss: 0.202023 - Time:207.17167377471924\n",
      "Epoch: 71 - Batch: 94144 - Loss: 0.083049 - Time:216.5801718235016\n",
      "Epoch: 71 - Batch: 98240 - Loss: 0.117626 - Time:225.9897711277008\n",
      "Epoch: 71 - Batch: 102336 - Loss: 0.061902 - Time:235.41311264038086\n",
      "Epoch: 71 - Batch: 106432 - Loss: 0.311269 - Time:244.81860518455505\n",
      "Epoch: 71 - Batch: 110528 - Loss: 0.179772 - Time:254.22600293159485\n",
      "Epoch: 71 - Batch: 114624 - Loss: 0.117103 - Time:263.63340425491333\n",
      "Epoch: 71 - Batch: 118720 - Loss: 0.072992 - Time:273.0580749511719\n",
      "Epoch: 71 - Batch: 122816 - Loss: 0.068622 - Time:282.4658441543579\n",
      "Epoch: 71 - Batch: 126912 - Loss: 0.176954 - Time:291.87385058403015\n",
      "Epoch: 71 - Batch: 131008 - Loss: 0.067248 - Time:301.2987275123596\n",
      "Epoch: 71 - Batch: 135104 - Loss: 0.079715 - Time:310.70743465423584\n",
      "Epoch: 71 - Batch: 139200 - Loss: 0.083003 - Time:320.1181242465973\n",
      "Epoch: 71 - Batch: 143296 - Loss: 0.040899 - Time:329.5260875225067\n",
      "Epoch: 71 - Batch: 147392 - Loss: 0.054516 - Time:338.9333863258362\n",
      "Epoch: 71 - Batch: 151488 - Loss: 0.114799 - Time:348.3394703865051\n",
      "Epoch: 72 - Batch: 4032 - Loss: 0.040985 - Time:9.56386113166809\n",
      "Epoch: 72 - Batch: 8128 - Loss: 0.116594 - Time:18.999847650527954\n",
      "Epoch: 72 - Batch: 12224 - Loss: 0.123350 - Time:28.40917444229126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 - Batch: 16320 - Loss: 0.199040 - Time:37.82029724121094\n",
      "Epoch: 72 - Batch: 20416 - Loss: 0.101181 - Time:47.229750871658325\n",
      "Epoch: 72 - Batch: 24512 - Loss: 0.198317 - Time:56.639869689941406\n",
      "Epoch: 72 - Batch: 28608 - Loss: 0.201960 - Time:66.06412816047668\n",
      "Epoch: 72 - Batch: 32704 - Loss: 0.074577 - Time:75.47130250930786\n",
      "Epoch: 72 - Batch: 36800 - Loss: 0.125845 - Time:84.89750647544861\n",
      "Epoch: 72 - Batch: 40896 - Loss: 0.244625 - Time:94.3100893497467\n",
      "Epoch: 72 - Batch: 44992 - Loss: 0.352962 - Time:103.72074604034424\n",
      "Epoch: 72 - Batch: 49088 - Loss: 0.311079 - Time:113.13242936134338\n",
      "Epoch: 72 - Batch: 53184 - Loss: 0.050236 - Time:122.543301820755\n",
      "Epoch: 72 - Batch: 57280 - Loss: 0.120163 - Time:131.9571671485901\n",
      "Epoch: 72 - Batch: 61376 - Loss: 0.026021 - Time:141.3671989440918\n",
      "Epoch: 72 - Batch: 65472 - Loss: 0.061321 - Time:150.77842020988464\n",
      "Epoch: 72 - Batch: 69568 - Loss: 0.037232 - Time:160.20528554916382\n",
      "Epoch: 72 - Batch: 73664 - Loss: 0.160881 - Time:169.61550879478455\n",
      "Epoch: 72 - Batch: 77760 - Loss: 0.022314 - Time:179.02528500556946\n",
      "Epoch: 72 - Batch: 81856 - Loss: 0.247391 - Time:188.43394923210144\n",
      "Epoch: 72 - Batch: 85952 - Loss: 0.083712 - Time:197.84433484077454\n",
      "Epoch: 72 - Batch: 90048 - Loss: 0.107721 - Time:207.27016425132751\n",
      "Epoch: 72 - Batch: 94144 - Loss: 0.353404 - Time:216.68002080917358\n",
      "Epoch: 72 - Batch: 98240 - Loss: 0.064174 - Time:226.08976197242737\n",
      "Epoch: 72 - Batch: 102336 - Loss: 0.173454 - Time:235.51774835586548\n",
      "Epoch: 72 - Batch: 106432 - Loss: 0.064606 - Time:244.9286630153656\n",
      "Epoch: 72 - Batch: 110528 - Loss: 0.144636 - Time:254.3398072719574\n",
      "Epoch: 72 - Batch: 114624 - Loss: 0.074795 - Time:263.74812030792236\n",
      "Epoch: 72 - Batch: 118720 - Loss: 0.216235 - Time:273.15605092048645\n",
      "Epoch: 72 - Batch: 122816 - Loss: 0.336118 - Time:282.5640425682068\n",
      "Epoch: 72 - Batch: 126912 - Loss: 0.133820 - Time:291.9719166755676\n",
      "Epoch: 72 - Batch: 131008 - Loss: 0.161350 - Time:301.3821380138397\n",
      "Epoch: 72 - Batch: 135104 - Loss: 0.168150 - Time:310.8103737831116\n",
      "Epoch: 72 - Batch: 139200 - Loss: 0.390012 - Time:320.2192237377167\n",
      "Epoch: 72 - Batch: 143296 - Loss: 0.123862 - Time:329.627405166626\n",
      "Epoch: 72 - Batch: 147392 - Loss: 0.114615 - Time:339.03703594207764\n",
      "Epoch: 72 - Batch: 151488 - Loss: 0.060353 - Time:348.46303033828735\n",
      "Epoch: 73 - Batch: 4032 - Loss: 0.022297 - Time:9.542692422866821\n",
      "Epoch: 73 - Batch: 8128 - Loss: 0.046122 - Time:18.953240871429443\n",
      "Epoch: 73 - Batch: 12224 - Loss: 0.170616 - Time:28.390714645385742\n",
      "Epoch: 73 - Batch: 16320 - Loss: 0.168691 - Time:37.80390548706055\n",
      "Epoch: 73 - Batch: 20416 - Loss: 0.259625 - Time:47.21589016914368\n",
      "Epoch: 73 - Batch: 24512 - Loss: 0.032696 - Time:56.64324331283569\n",
      "Epoch: 73 - Batch: 28608 - Loss: 0.039061 - Time:66.05339431762695\n",
      "Epoch: 73 - Batch: 32704 - Loss: 0.018396 - Time:75.46390128135681\n",
      "Epoch: 73 - Batch: 36800 - Loss: 0.019662 - Time:84.8748996257782\n",
      "Epoch: 73 - Batch: 40896 - Loss: 0.021390 - Time:94.28427362442017\n",
      "Epoch: 73 - Batch: 44992 - Loss: 0.071266 - Time:103.69454669952393\n",
      "Epoch: 73 - Batch: 49088 - Loss: 0.076004 - Time:113.10459160804749\n",
      "Epoch: 73 - Batch: 53184 - Loss: 0.129152 - Time:122.51472091674805\n",
      "Epoch: 73 - Batch: 57280 - Loss: 0.152412 - Time:131.94252061843872\n",
      "Epoch: 73 - Batch: 61376 - Loss: 0.049357 - Time:141.3510479927063\n",
      "Epoch: 73 - Batch: 65472 - Loss: 0.033448 - Time:150.76178884506226\n",
      "Epoch: 73 - Batch: 69568 - Loss: 0.095184 - Time:160.17046904563904\n",
      "Epoch: 73 - Batch: 73664 - Loss: 0.043310 - Time:169.59631204605103\n",
      "Epoch: 73 - Batch: 77760 - Loss: 0.036292 - Time:179.00833535194397\n",
      "Epoch: 73 - Batch: 81856 - Loss: 0.057325 - Time:188.41804885864258\n",
      "Epoch: 73 - Batch: 85952 - Loss: 0.244440 - Time:197.84651923179626\n",
      "Epoch: 73 - Batch: 90048 - Loss: 0.257052 - Time:207.25654220581055\n",
      "Epoch: 73 - Batch: 94144 - Loss: 0.183361 - Time:216.6675055027008\n",
      "Epoch: 73 - Batch: 98240 - Loss: 0.262588 - Time:226.07970714569092\n",
      "Epoch: 73 - Batch: 102336 - Loss: 0.120654 - Time:235.49179244041443\n",
      "Epoch: 73 - Batch: 106432 - Loss: 0.260178 - Time:244.90349435806274\n",
      "Epoch: 73 - Batch: 110528 - Loss: 0.302695 - Time:254.31496000289917\n",
      "Epoch: 73 - Batch: 114624 - Loss: 0.121135 - Time:263.72741532325745\n",
      "Epoch: 73 - Batch: 118720 - Loss: 0.137151 - Time:273.1583261489868\n",
      "Epoch: 73 - Batch: 122816 - Loss: 0.040310 - Time:282.56961488723755\n",
      "Epoch: 73 - Batch: 126912 - Loss: 0.222019 - Time:291.98259973526\n",
      "Epoch: 73 - Batch: 131008 - Loss: 0.083107 - Time:301.3972234725952\n",
      "Epoch: 73 - Batch: 135104 - Loss: 0.128960 - Time:310.826135635376\n",
      "Epoch: 73 - Batch: 139200 - Loss: 0.039743 - Time:320.2380905151367\n",
      "Epoch: 73 - Batch: 143296 - Loss: 0.087770 - Time:329.6483631134033\n",
      "Epoch: 73 - Batch: 147392 - Loss: 0.151514 - Time:339.0776743888855\n",
      "Epoch: 73 - Batch: 151488 - Loss: 0.024036 - Time:348.48939871788025\n",
      "Epoch: 74 - Batch: 4032 - Loss: 0.027657 - Time:9.54122543334961\n",
      "Epoch: 74 - Batch: 8128 - Loss: 0.134998 - Time:18.950366497039795\n",
      "Epoch: 74 - Batch: 12224 - Loss: 0.038919 - Time:28.358834266662598\n",
      "Epoch: 74 - Batch: 16320 - Loss: 0.169346 - Time:37.76735067367554\n",
      "Epoch: 74 - Batch: 20416 - Loss: 0.037032 - Time:47.177425146102905\n",
      "Epoch: 74 - Batch: 24512 - Loss: 0.090031 - Time:56.586206912994385\n",
      "Epoch: 74 - Batch: 28608 - Loss: 0.091870 - Time:66.02309894561768\n",
      "Epoch: 74 - Batch: 32704 - Loss: 0.179320 - Time:75.43288111686707\n",
      "Epoch: 74 - Batch: 36800 - Loss: 0.084906 - Time:84.8428909778595\n",
      "Epoch: 74 - Batch: 40896 - Loss: 0.110617 - Time:94.25199270248413\n",
      "Epoch: 74 - Batch: 44992 - Loss: 0.048273 - Time:103.67919039726257\n",
      "Epoch: 74 - Batch: 49088 - Loss: 0.033601 - Time:113.0891432762146\n",
      "Epoch: 74 - Batch: 53184 - Loss: 0.034108 - Time:122.49905252456665\n",
      "Epoch: 74 - Batch: 57280 - Loss: 0.062289 - Time:131.92301082611084\n",
      "Epoch: 74 - Batch: 61376 - Loss: 0.105555 - Time:141.32944440841675\n",
      "Epoch: 74 - Batch: 65472 - Loss: 0.179260 - Time:150.73660254478455\n",
      "Epoch: 74 - Batch: 69568 - Loss: 0.190397 - Time:160.14620518684387\n",
      "Epoch: 74 - Batch: 73664 - Loss: 0.151052 - Time:169.55715084075928\n",
      "Epoch: 74 - Batch: 77760 - Loss: 0.244765 - Time:178.96752858161926\n",
      "Epoch: 74 - Batch: 81856 - Loss: 0.075665 - Time:188.37909126281738\n",
      "Epoch: 74 - Batch: 85952 - Loss: 0.055259 - Time:197.78794622421265\n",
      "Epoch: 74 - Batch: 90048 - Loss: 0.069735 - Time:207.21555399894714\n",
      "Epoch: 74 - Batch: 94144 - Loss: 0.095192 - Time:216.62642645835876\n",
      "Epoch: 74 - Batch: 98240 - Loss: 0.136402 - Time:226.03663086891174\n",
      "Epoch: 74 - Batch: 102336 - Loss: 1.023778 - Time:235.44609260559082\n",
      "Epoch: 74 - Batch: 106432 - Loss: 0.054312 - Time:244.872967004776\n",
      "Epoch: 74 - Batch: 110528 - Loss: 0.088151 - Time:254.28165483474731\n",
      "Epoch: 74 - Batch: 114624 - Loss: 0.031534 - Time:263.6904106140137\n",
      "Epoch: 74 - Batch: 118720 - Loss: 0.085598 - Time:273.1163339614868\n",
      "Epoch: 74 - Batch: 122816 - Loss: 0.099176 - Time:282.525514125824\n",
      "Epoch: 74 - Batch: 126912 - Loss: 0.030512 - Time:291.9337348937988\n",
      "Epoch: 74 - Batch: 131008 - Loss: 0.123403 - Time:301.34193205833435\n",
      "Epoch: 74 - Batch: 135104 - Loss: 0.260551 - Time:310.7523727416992\n",
      "Epoch: 74 - Batch: 139200 - Loss: 0.051226 - Time:320.1611547470093\n",
      "Epoch: 74 - Batch: 143296 - Loss: 0.090580 - Time:329.5695860385895\n",
      "Epoch: 74 - Batch: 147392 - Loss: 0.271746 - Time:338.9785737991333\n",
      "Epoch: 74 - Batch: 151488 - Loss: 0.026648 - Time:348.404399394989\n",
      "Epoch: 75 - Batch: 4032 - Loss: 0.044742 - Time:9.552715301513672\n",
      "Epoch: 75 - Batch: 8128 - Loss: 0.120757 - Time:18.962893962860107\n",
      "Epoch: 75 - Batch: 12224 - Loss: 0.144616 - Time:28.373620748519897\n",
      "Epoch: 75 - Batch: 16320 - Loss: 0.075347 - Time:37.78416061401367\n",
      "Epoch: 75 - Batch: 20416 - Loss: 0.045479 - Time:47.1937472820282\n",
      "Epoch: 75 - Batch: 24512 - Loss: 0.125827 - Time:56.6049919128418\n",
      "Epoch: 75 - Batch: 28608 - Loss: 0.155420 - Time:66.04267382621765\n",
      "Epoch: 75 - Batch: 32704 - Loss: 0.029529 - Time:75.45222353935242\n",
      "Epoch: 75 - Batch: 36800 - Loss: 0.052982 - Time:84.86212158203125\n",
      "Epoch: 75 - Batch: 40896 - Loss: 0.047847 - Time:94.2735607624054\n",
      "Epoch: 75 - Batch: 44992 - Loss: 0.172768 - Time:103.70183944702148\n",
      "Epoch: 75 - Batch: 49088 - Loss: 0.023631 - Time:113.11291432380676\n",
      "Epoch: 75 - Batch: 53184 - Loss: 0.025398 - Time:122.52138924598694\n",
      "Epoch: 75 - Batch: 57280 - Loss: 0.161037 - Time:131.94699788093567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 - Batch: 61376 - Loss: 0.061916 - Time:141.35863423347473\n",
      "Epoch: 75 - Batch: 65472 - Loss: 0.207314 - Time:150.76755928993225\n",
      "Epoch: 75 - Batch: 69568 - Loss: 0.142116 - Time:160.17663645744324\n",
      "Epoch: 75 - Batch: 73664 - Loss: 0.306076 - Time:169.58605670928955\n",
      "Epoch: 75 - Batch: 77760 - Loss: 0.436232 - Time:178.9967396259308\n",
      "Epoch: 75 - Batch: 81856 - Loss: 0.071428 - Time:188.4069480895996\n",
      "Epoch: 75 - Batch: 85952 - Loss: 0.127280 - Time:197.8186538219452\n",
      "Epoch: 75 - Batch: 90048 - Loss: 0.049381 - Time:207.24604296684265\n",
      "Epoch: 75 - Batch: 94144 - Loss: 0.071700 - Time:216.65611290931702\n",
      "Epoch: 75 - Batch: 98240 - Loss: 0.091833 - Time:226.06712651252747\n",
      "Epoch: 75 - Batch: 102336 - Loss: 0.105718 - Time:235.47659492492676\n",
      "Epoch: 75 - Batch: 106432 - Loss: 0.242371 - Time:244.90492367744446\n",
      "Epoch: 75 - Batch: 110528 - Loss: 0.188781 - Time:254.31657814979553\n",
      "Epoch: 75 - Batch: 114624 - Loss: 0.188405 - Time:263.72890615463257\n",
      "Epoch: 75 - Batch: 118720 - Loss: 0.099471 - Time:273.1566445827484\n",
      "Epoch: 75 - Batch: 122816 - Loss: 0.061557 - Time:282.56837701797485\n",
      "Epoch: 75 - Batch: 126912 - Loss: 0.299932 - Time:291.97956562042236\n",
      "Epoch: 75 - Batch: 131008 - Loss: 0.154780 - Time:301.3899476528168\n",
      "Epoch: 75 - Batch: 135104 - Loss: 0.159867 - Time:310.80068922042847\n",
      "Epoch: 75 - Batch: 139200 - Loss: 0.089253 - Time:320.2120156288147\n",
      "Epoch: 75 - Batch: 143296 - Loss: 0.213293 - Time:329.62182235717773\n",
      "Epoch: 75 - Batch: 147392 - Loss: 0.090843 - Time:339.03297305107117\n",
      "Epoch: 75 - Batch: 151488 - Loss: 0.357847 - Time:348.46156764030457\n",
      "Epoch: 76 - Batch: 4032 - Loss: 0.040435 - Time:9.547801971435547\n",
      "Epoch: 76 - Batch: 8128 - Loss: 0.139043 - Time:18.95957589149475\n",
      "Epoch: 76 - Batch: 12224 - Loss: 0.096934 - Time:28.369014978408813\n",
      "Epoch: 76 - Batch: 16320 - Loss: 0.050743 - Time:37.80622625350952\n",
      "Epoch: 76 - Batch: 20416 - Loss: 0.525038 - Time:47.21844935417175\n",
      "Epoch: 76 - Batch: 24512 - Loss: 0.246652 - Time:56.62975263595581\n",
      "Epoch: 76 - Batch: 28608 - Loss: 0.107033 - Time:66.05528783798218\n",
      "Epoch: 76 - Batch: 32704 - Loss: 0.256974 - Time:75.46751737594604\n",
      "Epoch: 76 - Batch: 36800 - Loss: 0.033754 - Time:84.87848615646362\n",
      "Epoch: 76 - Batch: 40896 - Loss: 0.015193 - Time:94.29118156433105\n",
      "Epoch: 76 - Batch: 44992 - Loss: 0.022180 - Time:103.70405054092407\n",
      "Epoch: 76 - Batch: 49088 - Loss: 0.135667 - Time:113.11719584465027\n",
      "Epoch: 76 - Batch: 53184 - Loss: 0.219521 - Time:122.52949285507202\n",
      "Epoch: 76 - Batch: 57280 - Loss: 0.433799 - Time:131.94081020355225\n",
      "Epoch: 76 - Batch: 61376 - Loss: 0.292999 - Time:141.3714566230774\n",
      "Epoch: 76 - Batch: 65472 - Loss: 0.127242 - Time:150.7826542854309\n",
      "Epoch: 76 - Batch: 69568 - Loss: 0.062234 - Time:160.19557666778564\n",
      "Epoch: 76 - Batch: 73664 - Loss: 0.160672 - Time:169.60947942733765\n",
      "Epoch: 76 - Batch: 77760 - Loss: 0.038313 - Time:179.03836464881897\n",
      "Epoch: 76 - Batch: 81856 - Loss: 0.210443 - Time:188.45042276382446\n",
      "Epoch: 76 - Batch: 85952 - Loss: 0.088717 - Time:197.8651638031006\n",
      "Epoch: 76 - Batch: 90048 - Loss: 0.022655 - Time:207.29556250572205\n",
      "Epoch: 76 - Batch: 94144 - Loss: 0.317095 - Time:216.70786881446838\n",
      "Epoch: 76 - Batch: 98240 - Loss: 0.163293 - Time:226.1206636428833\n",
      "Epoch: 76 - Batch: 102336 - Loss: 0.162895 - Time:235.5304205417633\n",
      "Epoch: 76 - Batch: 106432 - Loss: 0.031128 - Time:244.94074964523315\n",
      "Epoch: 76 - Batch: 110528 - Loss: 0.131230 - Time:254.35123419761658\n",
      "Epoch: 76 - Batch: 114624 - Loss: 0.181222 - Time:263.7611722946167\n",
      "Epoch: 76 - Batch: 118720 - Loss: 0.032841 - Time:273.17087960243225\n",
      "Epoch: 76 - Batch: 122816 - Loss: 0.047663 - Time:282.59810757637024\n",
      "Epoch: 76 - Batch: 126912 - Loss: 0.461596 - Time:292.0098237991333\n",
      "Epoch: 76 - Batch: 131008 - Loss: 0.072016 - Time:301.4207754135132\n",
      "Epoch: 76 - Batch: 135104 - Loss: 0.117743 - Time:310.83170890808105\n",
      "Epoch: 76 - Batch: 139200 - Loss: 0.079821 - Time:320.2625639438629\n",
      "Epoch: 76 - Batch: 143296 - Loss: 0.114946 - Time:329.6748969554901\n",
      "Epoch: 76 - Batch: 147392 - Loss: 0.054746 - Time:339.08610129356384\n",
      "Epoch: 76 - Batch: 151488 - Loss: 0.060777 - Time:348.51353430747986\n",
      "Epoch: 77 - Batch: 4032 - Loss: 0.095990 - Time:9.559027910232544\n",
      "Epoch: 77 - Batch: 8128 - Loss: 0.320469 - Time:18.969059705734253\n",
      "Epoch: 77 - Batch: 12224 - Loss: 0.030868 - Time:28.40496253967285\n",
      "Epoch: 77 - Batch: 16320 - Loss: 0.020194 - Time:37.81390333175659\n",
      "Epoch: 77 - Batch: 20416 - Loss: 0.048662 - Time:47.22415232658386\n",
      "Epoch: 77 - Batch: 24512 - Loss: 0.049571 - Time:56.63204526901245\n",
      "Epoch: 77 - Batch: 28608 - Loss: 0.082626 - Time:66.04463696479797\n",
      "Epoch: 77 - Batch: 32704 - Loss: 0.078817 - Time:75.45557379722595\n",
      "Epoch: 77 - Batch: 36800 - Loss: 0.046852 - Time:84.86914467811584\n",
      "Epoch: 77 - Batch: 40896 - Loss: 0.024603 - Time:94.27908277511597\n",
      "Epoch: 77 - Batch: 44992 - Loss: 0.078018 - Time:103.706374168396\n",
      "Epoch: 77 - Batch: 49088 - Loss: 0.191976 - Time:113.1165280342102\n",
      "Epoch: 77 - Batch: 53184 - Loss: 0.033898 - Time:122.52741765975952\n",
      "Epoch: 77 - Batch: 57280 - Loss: 0.192982 - Time:131.93703985214233\n",
      "Epoch: 77 - Batch: 61376 - Loss: 0.102385 - Time:141.36518359184265\n",
      "Epoch: 77 - Batch: 65472 - Loss: 0.133131 - Time:150.77697324752808\n",
      "Epoch: 77 - Batch: 69568 - Loss: 0.215372 - Time:160.18922662734985\n",
      "Epoch: 77 - Batch: 73664 - Loss: 0.036216 - Time:169.61737513542175\n",
      "Epoch: 77 - Batch: 77760 - Loss: 0.508412 - Time:179.02983617782593\n",
      "Epoch: 77 - Batch: 81856 - Loss: 0.216246 - Time:188.43776154518127\n",
      "Epoch: 77 - Batch: 85952 - Loss: 0.268500 - Time:197.84891271591187\n",
      "Epoch: 77 - Batch: 90048 - Loss: 0.034331 - Time:207.26056265830994\n",
      "Epoch: 77 - Batch: 94144 - Loss: 0.116478 - Time:216.6725800037384\n",
      "Epoch: 77 - Batch: 98240 - Loss: 0.070342 - Time:226.0862100124359\n",
      "Epoch: 77 - Batch: 102336 - Loss: 0.050697 - Time:235.49883913993835\n",
      "Epoch: 77 - Batch: 106432 - Loss: 0.034245 - Time:244.92853808403015\n",
      "Epoch: 77 - Batch: 110528 - Loss: 0.085171 - Time:254.33651685714722\n",
      "Epoch: 77 - Batch: 114624 - Loss: 0.094300 - Time:263.7481243610382\n",
      "Epoch: 77 - Batch: 118720 - Loss: 0.043510 - Time:273.15890765190125\n",
      "Epoch: 77 - Batch: 122816 - Loss: 0.296386 - Time:282.5682649612427\n",
      "Epoch: 77 - Batch: 126912 - Loss: 0.406652 - Time:291.9956941604614\n",
      "Epoch: 77 - Batch: 131008 - Loss: 0.153246 - Time:301.4062798023224\n",
      "Epoch: 77 - Batch: 135104 - Loss: 0.283885 - Time:310.8162727355957\n",
      "Epoch: 77 - Batch: 139200 - Loss: 0.039068 - Time:320.245810508728\n",
      "Epoch: 77 - Batch: 143296 - Loss: 0.066078 - Time:329.6586434841156\n",
      "Epoch: 77 - Batch: 147392 - Loss: 0.263084 - Time:339.0692069530487\n",
      "Epoch: 77 - Batch: 151488 - Loss: 0.096446 - Time:348.4788007736206\n",
      "Epoch: 78 - Batch: 4032 - Loss: 0.174016 - Time:9.547322034835815\n",
      "Epoch: 78 - Batch: 8128 - Loss: 0.048343 - Time:18.98747229576111\n",
      "Epoch: 78 - Batch: 12224 - Loss: 0.113647 - Time:28.398555755615234\n",
      "Epoch: 78 - Batch: 16320 - Loss: 0.051974 - Time:37.808202266693115\n",
      "Epoch: 78 - Batch: 20416 - Loss: 0.018427 - Time:47.218343019485474\n",
      "Epoch: 78 - Batch: 24512 - Loss: 0.041072 - Time:56.631003618240356\n",
      "Epoch: 78 - Batch: 28608 - Loss: 0.052409 - Time:66.04118704795837\n",
      "Epoch: 78 - Batch: 32704 - Loss: 0.020511 - Time:75.45258331298828\n",
      "Epoch: 78 - Batch: 36800 - Loss: 0.033375 - Time:84.86499309539795\n",
      "Epoch: 78 - Batch: 40896 - Loss: 0.180216 - Time:94.29442429542542\n",
      "Epoch: 78 - Batch: 44992 - Loss: 0.471810 - Time:103.70271348953247\n",
      "Epoch: 78 - Batch: 49088 - Loss: 0.108514 - Time:113.1114296913147\n",
      "Epoch: 78 - Batch: 53184 - Loss: 0.038450 - Time:122.52220129966736\n",
      "Epoch: 78 - Batch: 57280 - Loss: 0.133487 - Time:131.9501564502716\n",
      "Epoch: 78 - Batch: 61376 - Loss: 0.225058 - Time:141.3598358631134\n",
      "Epoch: 78 - Batch: 65472 - Loss: 0.215090 - Time:150.76932001113892\n",
      "Epoch: 78 - Batch: 69568 - Loss: 0.080578 - Time:160.1945321559906\n",
      "Epoch: 78 - Batch: 73664 - Loss: 0.051712 - Time:169.60587120056152\n",
      "Epoch: 78 - Batch: 77760 - Loss: 0.031533 - Time:179.01483535766602\n",
      "Epoch: 78 - Batch: 81856 - Loss: 0.302942 - Time:188.42529487609863\n",
      "Epoch: 78 - Batch: 85952 - Loss: 0.029381 - Time:197.83344101905823\n",
      "Epoch: 78 - Batch: 90048 - Loss: 0.100932 - Time:207.24306535720825\n",
      "Epoch: 78 - Batch: 94144 - Loss: 0.066294 - Time:216.65321397781372\n",
      "Epoch: 78 - Batch: 98240 - Loss: 0.038279 - Time:226.06134581565857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 - Batch: 102336 - Loss: 0.104830 - Time:235.4877598285675\n",
      "Epoch: 78 - Batch: 106432 - Loss: 0.074533 - Time:244.89917135238647\n",
      "Epoch: 78 - Batch: 110528 - Loss: 0.028580 - Time:254.3069486618042\n",
      "Epoch: 78 - Batch: 114624 - Loss: 0.059782 - Time:263.71640133857727\n",
      "Epoch: 78 - Batch: 118720 - Loss: 0.239470 - Time:273.1426136493683\n",
      "Epoch: 78 - Batch: 122816 - Loss: 0.102429 - Time:282.5521562099457\n",
      "Epoch: 78 - Batch: 126912 - Loss: 0.254446 - Time:291.96079421043396\n",
      "Epoch: 78 - Batch: 131008 - Loss: 0.137239 - Time:301.38763451576233\n",
      "Epoch: 78 - Batch: 135104 - Loss: 0.055643 - Time:310.79627442359924\n",
      "Epoch: 78 - Batch: 139200 - Loss: 0.128128 - Time:320.207044839859\n",
      "Epoch: 78 - Batch: 143296 - Loss: 0.643331 - Time:329.618625164032\n",
      "Epoch: 78 - Batch: 147392 - Loss: 0.417563 - Time:339.02746748924255\n",
      "Epoch: 78 - Batch: 151488 - Loss: 0.027128 - Time:348.43864822387695\n",
      "Epoch: 79 - Batch: 4032 - Loss: 0.241687 - Time:9.582717657089233\n",
      "Epoch: 79 - Batch: 8128 - Loss: 0.050966 - Time:18.99195122718811\n",
      "Epoch: 79 - Batch: 12224 - Loss: 0.488434 - Time:28.399906158447266\n",
      "Epoch: 79 - Batch: 16320 - Loss: 0.042941 - Time:37.80769968032837\n",
      "Epoch: 79 - Batch: 20416 - Loss: 0.052234 - Time:47.216240882873535\n",
      "Epoch: 79 - Batch: 24512 - Loss: 0.065087 - Time:56.626564025878906\n",
      "Epoch: 79 - Batch: 28608 - Loss: 0.063208 - Time:66.03331470489502\n",
      "Epoch: 79 - Batch: 32704 - Loss: 0.130794 - Time:75.44047427177429\n",
      "Epoch: 79 - Batch: 36800 - Loss: 0.042106 - Time:84.86598896980286\n",
      "Epoch: 79 - Batch: 40896 - Loss: 0.106501 - Time:94.27604603767395\n",
      "Epoch: 79 - Batch: 44992 - Loss: 0.304537 - Time:103.68423748016357\n",
      "Epoch: 79 - Batch: 49088 - Loss: 0.084032 - Time:113.09077215194702\n",
      "Epoch: 79 - Batch: 53184 - Loss: 0.084970 - Time:122.50114107131958\n",
      "Epoch: 79 - Batch: 57280 - Loss: 0.357172 - Time:131.92744517326355\n",
      "Epoch: 79 - Batch: 61376 - Loss: 0.097571 - Time:141.335693359375\n",
      "Epoch: 79 - Batch: 65472 - Loss: 0.073025 - Time:150.74504160881042\n",
      "Epoch: 79 - Batch: 69568 - Loss: 0.142696 - Time:160.17014575004578\n",
      "Epoch: 79 - Batch: 73664 - Loss: 0.080623 - Time:169.5778591632843\n",
      "Epoch: 79 - Batch: 77760 - Loss: 0.082056 - Time:178.9862449169159\n",
      "Epoch: 79 - Batch: 81856 - Loss: 0.093590 - Time:188.39463710784912\n",
      "Epoch: 79 - Batch: 85952 - Loss: 0.068673 - Time:197.80349373817444\n",
      "Epoch: 79 - Batch: 90048 - Loss: 0.082346 - Time:207.21270418167114\n",
      "Epoch: 79 - Batch: 94144 - Loss: 0.073141 - Time:216.62491369247437\n",
      "Epoch: 79 - Batch: 98240 - Loss: 0.067765 - Time:226.03312420845032\n",
      "Epoch: 79 - Batch: 102336 - Loss: 0.546076 - Time:235.4630515575409\n",
      "Epoch: 79 - Batch: 106432 - Loss: 0.289581 - Time:244.8738934993744\n",
      "Epoch: 79 - Batch: 110528 - Loss: 0.070453 - Time:254.28172945976257\n",
      "Epoch: 79 - Batch: 114624 - Loss: 0.071555 - Time:263.68900513648987\n",
      "Epoch: 79 - Batch: 118720 - Loss: 0.208208 - Time:273.11457324028015\n",
      "Epoch: 79 - Batch: 122816 - Loss: 0.051755 - Time:282.5218436717987\n",
      "Epoch: 79 - Batch: 126912 - Loss: 0.256851 - Time:291.9283137321472\n",
      "Epoch: 79 - Batch: 131008 - Loss: 0.223876 - Time:301.3553771972656\n",
      "Epoch: 79 - Batch: 135104 - Loss: 0.050323 - Time:310.76528811454773\n",
      "Epoch: 79 - Batch: 139200 - Loss: 0.045494 - Time:320.1746518611908\n",
      "Epoch: 79 - Batch: 143296 - Loss: 0.171931 - Time:329.58597350120544\n",
      "Epoch: 79 - Batch: 147392 - Loss: 0.049185 - Time:338.9987096786499\n",
      "Epoch: 79 - Batch: 151488 - Loss: 0.094157 - Time:348.4088976383209\n",
      "Epoch: 80 - Batch: 4032 - Loss: 0.290501 - Time:9.535089254379272\n",
      "Epoch: 80 - Batch: 8128 - Loss: 0.328273 - Time:18.974013566970825\n",
      "Epoch: 80 - Batch: 12224 - Loss: 0.032520 - Time:28.388990879058838\n",
      "Epoch: 80 - Batch: 16320 - Loss: 0.223650 - Time:37.80132460594177\n",
      "Epoch: 80 - Batch: 20416 - Loss: 0.179014 - Time:47.210975646972656\n",
      "Epoch: 80 - Batch: 24512 - Loss: 0.087002 - Time:56.63775420188904\n",
      "Epoch: 80 - Batch: 28608 - Loss: 0.073596 - Time:66.04716038703918\n",
      "Epoch: 80 - Batch: 32704 - Loss: 0.285208 - Time:75.45717930793762\n",
      "Epoch: 80 - Batch: 36800 - Loss: 0.137820 - Time:84.88348603248596\n",
      "Epoch: 80 - Batch: 40896 - Loss: 0.042907 - Time:94.29510188102722\n",
      "Epoch: 80 - Batch: 44992 - Loss: 0.019980 - Time:103.70709943771362\n",
      "Epoch: 80 - Batch: 49088 - Loss: 0.067141 - Time:113.11491560935974\n",
      "Epoch: 80 - Batch: 53184 - Loss: 0.018750 - Time:122.52465271949768\n",
      "Epoch: 80 - Batch: 57280 - Loss: 0.066598 - Time:131.933776140213\n",
      "Epoch: 80 - Batch: 61376 - Loss: 0.043027 - Time:141.34329795837402\n",
      "Epoch: 80 - Batch: 65472 - Loss: 0.129066 - Time:150.75272226333618\n",
      "Epoch: 80 - Batch: 69568 - Loss: 0.142867 - Time:160.17828130722046\n",
      "Epoch: 80 - Batch: 73664 - Loss: 0.215361 - Time:169.58989191055298\n",
      "Epoch: 80 - Batch: 77760 - Loss: 0.044976 - Time:179.00109791755676\n",
      "Epoch: 80 - Batch: 81856 - Loss: 0.283139 - Time:188.41135239601135\n",
      "Epoch: 80 - Batch: 85952 - Loss: 1.236661 - Time:197.82397270202637\n",
      "Epoch: 80 - Batch: 90048 - Loss: 0.289131 - Time:207.2528567314148\n",
      "Epoch: 80 - Batch: 94144 - Loss: 0.245596 - Time:216.66304278373718\n",
      "Epoch: 80 - Batch: 98240 - Loss: 0.168485 - Time:226.07654237747192\n",
      "Epoch: 80 - Batch: 102336 - Loss: 0.046061 - Time:235.5047152042389\n",
      "Epoch: 80 - Batch: 106432 - Loss: 0.030821 - Time:244.91825342178345\n",
      "Epoch: 80 - Batch: 110528 - Loss: 0.023673 - Time:254.3285675048828\n",
      "Epoch: 80 - Batch: 114624 - Loss: 0.184107 - Time:263.7373101711273\n",
      "Epoch: 80 - Batch: 118720 - Loss: 0.263031 - Time:273.1481065750122\n",
      "Epoch: 80 - Batch: 122816 - Loss: 0.499389 - Time:282.5602810382843\n",
      "Epoch: 80 - Batch: 126912 - Loss: 0.063163 - Time:291.9725480079651\n",
      "Epoch: 80 - Batch: 131008 - Loss: 0.043147 - Time:301.38487577438354\n",
      "Epoch: 80 - Batch: 135104 - Loss: 0.052443 - Time:310.8105962276459\n",
      "Epoch: 80 - Batch: 139200 - Loss: 0.132835 - Time:320.21949529647827\n",
      "Epoch: 80 - Batch: 143296 - Loss: 0.089253 - Time:329.6293189525604\n",
      "Epoch: 80 - Batch: 147392 - Loss: 0.021010 - Time:339.0379309654236\n",
      "Epoch: 80 - Batch: 151488 - Loss: 0.163169 - Time:348.4642457962036\n",
      "Epoch: 81 - Batch: 4032 - Loss: 0.101217 - Time:9.535454273223877\n",
      "Epoch: 81 - Batch: 8128 - Loss: 0.051169 - Time:18.944088459014893\n",
      "Epoch: 81 - Batch: 12224 - Loss: 0.145296 - Time:28.380789756774902\n",
      "Epoch: 81 - Batch: 16320 - Loss: 0.094254 - Time:37.78921294212341\n",
      "Epoch: 81 - Batch: 20416 - Loss: 0.217584 - Time:47.200167179107666\n",
      "Epoch: 81 - Batch: 24512 - Loss: 0.022867 - Time:56.6253457069397\n",
      "Epoch: 81 - Batch: 28608 - Loss: 0.107232 - Time:66.03562235832214\n",
      "Epoch: 81 - Batch: 32704 - Loss: 0.186193 - Time:75.4457151889801\n",
      "Epoch: 81 - Batch: 36800 - Loss: 0.740298 - Time:84.8560209274292\n",
      "Epoch: 81 - Batch: 40896 - Loss: 0.079340 - Time:94.26484680175781\n",
      "Epoch: 81 - Batch: 44992 - Loss: 0.089165 - Time:103.67613768577576\n",
      "Epoch: 81 - Batch: 49088 - Loss: 0.133827 - Time:113.08467054367065\n",
      "Epoch: 81 - Batch: 53184 - Loss: 0.061324 - Time:122.49485421180725\n",
      "Epoch: 81 - Batch: 57280 - Loss: 0.067958 - Time:131.92338824272156\n",
      "Epoch: 81 - Batch: 61376 - Loss: 0.020628 - Time:141.33284211158752\n",
      "Epoch: 81 - Batch: 65472 - Loss: 0.032951 - Time:150.74330830574036\n",
      "Epoch: 81 - Batch: 69568 - Loss: 0.089312 - Time:160.154363155365\n",
      "Epoch: 81 - Batch: 73664 - Loss: 0.066234 - Time:169.58081603050232\n",
      "Epoch: 81 - Batch: 77760 - Loss: 0.022346 - Time:178.98981618881226\n",
      "Epoch: 81 - Batch: 81856 - Loss: 0.311243 - Time:188.39987182617188\n",
      "Epoch: 81 - Batch: 85952 - Loss: 0.062917 - Time:197.82628750801086\n",
      "Epoch: 81 - Batch: 90048 - Loss: 0.041437 - Time:207.2353708744049\n",
      "Epoch: 81 - Batch: 94144 - Loss: 0.235341 - Time:216.64426279067993\n",
      "Epoch: 81 - Batch: 98240 - Loss: 0.029702 - Time:226.05686378479004\n",
      "Epoch: 81 - Batch: 102336 - Loss: 0.155697 - Time:235.46697330474854\n",
      "Epoch: 81 - Batch: 106432 - Loss: 0.200730 - Time:244.8764021396637\n",
      "Epoch: 81 - Batch: 110528 - Loss: 0.477888 - Time:254.28700804710388\n",
      "Epoch: 81 - Batch: 114624 - Loss: 0.083539 - Time:263.69792675971985\n",
      "Epoch: 81 - Batch: 118720 - Loss: 0.032054 - Time:273.1251275539398\n",
      "Epoch: 81 - Batch: 122816 - Loss: 0.176973 - Time:282.5338134765625\n",
      "Epoch: 81 - Batch: 126912 - Loss: 0.136086 - Time:291.9429485797882\n",
      "Epoch: 81 - Batch: 131008 - Loss: 0.153303 - Time:301.3520607948303\n",
      "Epoch: 81 - Batch: 135104 - Loss: 0.294784 - Time:310.76402592658997\n",
      "Epoch: 81 - Batch: 139200 - Loss: 0.049517 - Time:320.19206523895264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 - Batch: 143296 - Loss: 0.190814 - Time:329.6030430793762\n",
      "Epoch: 81 - Batch: 147392 - Loss: 0.070252 - Time:339.0322437286377\n",
      "Epoch: 81 - Batch: 151488 - Loss: 0.034496 - Time:348.4414162635803\n",
      "Epoch: 82 - Batch: 4032 - Loss: 0.201542 - Time:9.56610655784607\n",
      "Epoch: 82 - Batch: 8128 - Loss: 0.040279 - Time:18.977662324905396\n",
      "Epoch: 82 - Batch: 12224 - Loss: 0.118152 - Time:28.414085149765015\n",
      "Epoch: 82 - Batch: 16320 - Loss: 0.180192 - Time:37.824729681015015\n",
      "Epoch: 82 - Batch: 20416 - Loss: 0.042756 - Time:47.233643531799316\n",
      "Epoch: 82 - Batch: 24512 - Loss: 0.115198 - Time:56.64445781707764\n",
      "Epoch: 82 - Batch: 28608 - Loss: 0.029874 - Time:66.05622220039368\n",
      "Epoch: 82 - Batch: 32704 - Loss: 0.147137 - Time:75.46498441696167\n",
      "Epoch: 82 - Batch: 36800 - Loss: 0.160827 - Time:84.87509441375732\n",
      "Epoch: 82 - Batch: 40896 - Loss: 0.050802 - Time:94.28776049613953\n",
      "Epoch: 82 - Batch: 44992 - Loss: 0.145222 - Time:103.71882820129395\n",
      "Epoch: 82 - Batch: 49088 - Loss: 0.028564 - Time:113.12994623184204\n",
      "Epoch: 82 - Batch: 53184 - Loss: 0.099131 - Time:122.54160284996033\n",
      "Epoch: 82 - Batch: 57280 - Loss: 0.154482 - Time:131.95200872421265\n",
      "Epoch: 82 - Batch: 61376 - Loss: 0.065077 - Time:141.38058066368103\n",
      "Epoch: 82 - Batch: 65472 - Loss: 0.119958 - Time:150.79269313812256\n",
      "Epoch: 82 - Batch: 69568 - Loss: 0.027972 - Time:160.20708417892456\n",
      "Epoch: 82 - Batch: 73664 - Loss: 0.151919 - Time:169.64766478538513\n",
      "Epoch: 82 - Batch: 77760 - Loss: 0.265643 - Time:179.0616295337677\n",
      "Epoch: 82 - Batch: 81856 - Loss: 0.179540 - Time:188.47477626800537\n",
      "Epoch: 82 - Batch: 85952 - Loss: 0.084269 - Time:197.88355898857117\n",
      "Epoch: 82 - Batch: 90048 - Loss: 0.205970 - Time:207.2931718826294\n",
      "Epoch: 82 - Batch: 94144 - Loss: 0.044124 - Time:216.70324158668518\n",
      "Epoch: 82 - Batch: 98240 - Loss: 0.082903 - Time:226.11420917510986\n",
      "Epoch: 82 - Batch: 102336 - Loss: 0.100951 - Time:235.52547931671143\n",
      "Epoch: 82 - Batch: 106432 - Loss: 0.036908 - Time:244.95284748077393\n",
      "Epoch: 82 - Batch: 110528 - Loss: 0.148646 - Time:254.3624701499939\n",
      "Epoch: 82 - Batch: 114624 - Loss: 0.027215 - Time:263.7728977203369\n",
      "Epoch: 82 - Batch: 118720 - Loss: 0.386093 - Time:273.1871373653412\n",
      "Epoch: 82 - Batch: 122816 - Loss: 0.174915 - Time:282.6162407398224\n",
      "Epoch: 82 - Batch: 126912 - Loss: 0.176964 - Time:292.0290274620056\n",
      "Epoch: 82 - Batch: 131008 - Loss: 0.178256 - Time:301.43962383270264\n",
      "Epoch: 82 - Batch: 135104 - Loss: 0.029755 - Time:310.8653390407562\n",
      "Epoch: 82 - Batch: 139200 - Loss: 0.300072 - Time:320.27643966674805\n",
      "Epoch: 82 - Batch: 143296 - Loss: 0.061831 - Time:329.6874279975891\n",
      "Epoch: 82 - Batch: 147392 - Loss: 0.045743 - Time:339.1002733707428\n",
      "Epoch: 82 - Batch: 151488 - Loss: 0.055453 - Time:348.51415038108826\n",
      "Epoch: 83 - Batch: 4032 - Loss: 0.041244 - Time:9.544622659683228\n",
      "Epoch: 83 - Batch: 8128 - Loss: 0.027805 - Time:18.985188722610474\n",
      "Epoch: 83 - Batch: 12224 - Loss: 0.068298 - Time:28.3954439163208\n",
      "Epoch: 83 - Batch: 16320 - Loss: 0.109154 - Time:37.80503010749817\n",
      "Epoch: 83 - Batch: 20416 - Loss: 0.208697 - Time:47.21409034729004\n",
      "Epoch: 83 - Batch: 24512 - Loss: 0.037258 - Time:56.62269854545593\n",
      "Epoch: 83 - Batch: 28608 - Loss: 0.129165 - Time:66.03299188613892\n",
      "Epoch: 83 - Batch: 32704 - Loss: 0.066420 - Time:75.44084596633911\n",
      "Epoch: 83 - Batch: 36800 - Loss: 0.037111 - Time:84.85165905952454\n",
      "Epoch: 83 - Batch: 40896 - Loss: 0.306138 - Time:94.27747225761414\n",
      "Epoch: 83 - Batch: 44992 - Loss: 0.031321 - Time:103.68722224235535\n",
      "Epoch: 83 - Batch: 49088 - Loss: 0.156047 - Time:113.09792375564575\n",
      "Epoch: 83 - Batch: 53184 - Loss: 0.095311 - Time:122.50766181945801\n",
      "Epoch: 83 - Batch: 57280 - Loss: 0.211003 - Time:131.9342954158783\n",
      "Epoch: 83 - Batch: 61376 - Loss: 0.053446 - Time:141.34420704841614\n",
      "Epoch: 83 - Batch: 65472 - Loss: 0.260768 - Time:150.75206756591797\n",
      "Epoch: 83 - Batch: 69568 - Loss: 0.187991 - Time:160.17736315727234\n",
      "Epoch: 83 - Batch: 73664 - Loss: 0.048770 - Time:169.58457612991333\n",
      "Epoch: 83 - Batch: 77760 - Loss: 0.105684 - Time:178.99473881721497\n",
      "Epoch: 83 - Batch: 81856 - Loss: 0.072620 - Time:188.40333032608032\n",
      "Epoch: 83 - Batch: 85952 - Loss: 0.032227 - Time:197.81347370147705\n",
      "Epoch: 83 - Batch: 90048 - Loss: 0.035585 - Time:207.22405552864075\n",
      "Epoch: 83 - Batch: 94144 - Loss: 0.046362 - Time:216.63434624671936\n",
      "Epoch: 83 - Batch: 98240 - Loss: 0.057226 - Time:226.0434126853943\n",
      "Epoch: 83 - Batch: 102336 - Loss: 0.105998 - Time:235.47217750549316\n",
      "Epoch: 83 - Batch: 106432 - Loss: 0.050988 - Time:244.88029098510742\n",
      "Epoch: 83 - Batch: 110528 - Loss: 0.072599 - Time:254.2891776561737\n",
      "Epoch: 83 - Batch: 114624 - Loss: 0.161539 - Time:263.69746017456055\n",
      "Epoch: 83 - Batch: 118720 - Loss: 0.025718 - Time:273.12204456329346\n",
      "Epoch: 83 - Batch: 122816 - Loss: 0.061955 - Time:282.5312614440918\n",
      "Epoch: 83 - Batch: 126912 - Loss: 0.386029 - Time:291.9422399997711\n",
      "Epoch: 83 - Batch: 131008 - Loss: 0.044435 - Time:301.3673279285431\n",
      "Epoch: 83 - Batch: 135104 - Loss: 0.071332 - Time:310.77811884880066\n",
      "Epoch: 83 - Batch: 139200 - Loss: 0.081039 - Time:320.18612480163574\n",
      "Epoch: 83 - Batch: 143296 - Loss: 0.290673 - Time:329.5942807197571\n",
      "Epoch: 83 - Batch: 147392 - Loss: 0.250095 - Time:339.00216913223267\n",
      "Epoch: 83 - Batch: 151488 - Loss: 0.219075 - Time:348.41247940063477\n",
      "Epoch: 84 - Batch: 4032 - Loss: 0.051150 - Time:9.552133321762085\n",
      "Epoch: 84 - Batch: 8128 - Loss: 0.216689 - Time:18.991149425506592\n",
      "Epoch: 84 - Batch: 12224 - Loss: 0.329408 - Time:28.402184009552002\n",
      "Epoch: 84 - Batch: 16320 - Loss: 0.019815 - Time:37.812485456466675\n",
      "Epoch: 84 - Batch: 20416 - Loss: 0.136243 - Time:47.224600315093994\n",
      "Epoch: 84 - Batch: 24512 - Loss: 0.207814 - Time:56.6358745098114\n",
      "Epoch: 84 - Batch: 28608 - Loss: 0.062324 - Time:66.06387996673584\n",
      "Epoch: 84 - Batch: 32704 - Loss: 0.149960 - Time:75.47681593894958\n",
      "Epoch: 84 - Batch: 36800 - Loss: 0.103592 - Time:84.88927388191223\n",
      "Epoch: 84 - Batch: 40896 - Loss: 0.102332 - Time:94.31633353233337\n",
      "Epoch: 84 - Batch: 44992 - Loss: 0.182536 - Time:103.72653913497925\n",
      "Epoch: 84 - Batch: 49088 - Loss: 0.169043 - Time:113.13807821273804\n",
      "Epoch: 84 - Batch: 53184 - Loss: 0.042326 - Time:122.55040645599365\n",
      "Epoch: 84 - Batch: 57280 - Loss: 0.020378 - Time:131.96160411834717\n",
      "Epoch: 84 - Batch: 61376 - Loss: 0.019458 - Time:141.3721604347229\n",
      "Epoch: 84 - Batch: 65472 - Loss: 0.119902 - Time:150.78291630744934\n",
      "Epoch: 84 - Batch: 69568 - Loss: 0.044225 - Time:160.19401121139526\n",
      "Epoch: 84 - Batch: 73664 - Loss: 0.187545 - Time:169.62236666679382\n",
      "Epoch: 84 - Batch: 77760 - Loss: 0.113659 - Time:179.03307008743286\n",
      "Epoch: 84 - Batch: 81856 - Loss: 0.128911 - Time:188.44530749320984\n",
      "Epoch: 84 - Batch: 85952 - Loss: 0.188161 - Time:197.8543541431427\n",
      "Epoch: 84 - Batch: 90048 - Loss: 0.417979 - Time:207.27839183807373\n",
      "Epoch: 84 - Batch: 94144 - Loss: 0.154786 - Time:216.6873905658722\n",
      "Epoch: 84 - Batch: 98240 - Loss: 0.355176 - Time:226.09728240966797\n",
      "Epoch: 84 - Batch: 102336 - Loss: 0.031313 - Time:235.52361822128296\n",
      "Epoch: 84 - Batch: 106432 - Loss: 0.136932 - Time:244.93563270568848\n",
      "Epoch: 84 - Batch: 110528 - Loss: 0.177898 - Time:254.34570002555847\n",
      "Epoch: 84 - Batch: 114624 - Loss: 0.023523 - Time:263.75533175468445\n",
      "Epoch: 84 - Batch: 118720 - Loss: 0.055749 - Time:273.16345167160034\n",
      "Epoch: 84 - Batch: 122816 - Loss: 0.026368 - Time:282.57207441329956\n",
      "Epoch: 84 - Batch: 126912 - Loss: 0.092107 - Time:291.9818158149719\n",
      "Epoch: 84 - Batch: 131008 - Loss: 0.106272 - Time:301.3920404911041\n",
      "Epoch: 84 - Batch: 135104 - Loss: 0.038923 - Time:310.8196702003479\n",
      "Epoch: 84 - Batch: 139200 - Loss: 0.071510 - Time:320.22862815856934\n",
      "Epoch: 84 - Batch: 143296 - Loss: 0.106647 - Time:329.63826870918274\n",
      "Epoch: 84 - Batch: 147392 - Loss: 0.053028 - Time:339.0481572151184\n",
      "Epoch: 84 - Batch: 151488 - Loss: 0.055849 - Time:348.47599148750305\n",
      "Epoch: 85 - Batch: 4032 - Loss: 0.107019 - Time:9.557495355606079\n",
      "Epoch: 85 - Batch: 8128 - Loss: 0.109857 - Time:18.971649646759033\n",
      "Epoch: 85 - Batch: 12224 - Loss: 0.236289 - Time:28.41375207901001\n",
      "Epoch: 85 - Batch: 16320 - Loss: 0.032171 - Time:37.82865285873413\n",
      "Epoch: 85 - Batch: 20416 - Loss: 0.030240 - Time:47.2417516708374\n",
      "Epoch: 85 - Batch: 24512 - Loss: 0.042708 - Time:56.67210054397583\n",
      "Epoch: 85 - Batch: 28608 - Loss: 0.024826 - Time:66.08437967300415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 - Batch: 32704 - Loss: 0.039104 - Time:75.49637842178345\n",
      "Epoch: 85 - Batch: 36800 - Loss: 0.022437 - Time:84.91107702255249\n",
      "Epoch: 85 - Batch: 40896 - Loss: 0.194298 - Time:94.3228690624237\n",
      "Epoch: 85 - Batch: 44992 - Loss: 0.208708 - Time:103.73381805419922\n",
      "Epoch: 85 - Batch: 49088 - Loss: 0.032618 - Time:113.14385986328125\n",
      "Epoch: 85 - Batch: 53184 - Loss: 0.079302 - Time:122.55760097503662\n",
      "Epoch: 85 - Batch: 57280 - Loss: 0.033627 - Time:131.9842336177826\n",
      "Epoch: 85 - Batch: 61376 - Loss: 0.033689 - Time:141.39586353302002\n",
      "Epoch: 85 - Batch: 65472 - Loss: 0.030375 - Time:150.80797839164734\n",
      "Epoch: 85 - Batch: 69568 - Loss: 0.028511 - Time:160.21960639953613\n",
      "Epoch: 85 - Batch: 73664 - Loss: 0.029791 - Time:169.63049960136414\n",
      "Epoch: 85 - Batch: 77760 - Loss: 0.047236 - Time:179.06203532218933\n",
      "Epoch: 85 - Batch: 81856 - Loss: 0.054007 - Time:188.47618460655212\n",
      "Epoch: 85 - Batch: 85952 - Loss: 0.074729 - Time:197.88746309280396\n",
      "Epoch: 85 - Batch: 90048 - Loss: 0.856875 - Time:207.31593799591064\n",
      "Epoch: 85 - Batch: 94144 - Loss: 0.035709 - Time:216.7268350124359\n",
      "Epoch: 85 - Batch: 98240 - Loss: 0.060143 - Time:226.13825678825378\n",
      "Epoch: 85 - Batch: 102336 - Loss: 0.134810 - Time:235.55146026611328\n",
      "Epoch: 85 - Batch: 106432 - Loss: 0.040836 - Time:244.9642150402069\n",
      "Epoch: 85 - Batch: 110528 - Loss: 0.193988 - Time:254.37494015693665\n",
      "Epoch: 85 - Batch: 114624 - Loss: 0.053774 - Time:263.78952741622925\n",
      "Epoch: 85 - Batch: 118720 - Loss: 0.150748 - Time:273.20377373695374\n",
      "Epoch: 85 - Batch: 122816 - Loss: 0.062594 - Time:282.63263392448425\n",
      "Epoch: 85 - Batch: 126912 - Loss: 0.033956 - Time:292.0449240207672\n",
      "Epoch: 85 - Batch: 131008 - Loss: 0.173733 - Time:301.4567070007324\n",
      "Epoch: 85 - Batch: 135104 - Loss: 0.129579 - Time:310.8701128959656\n",
      "Epoch: 85 - Batch: 139200 - Loss: 0.115376 - Time:320.2988760471344\n",
      "Epoch: 85 - Batch: 143296 - Loss: 0.236470 - Time:329.71180176734924\n",
      "Epoch: 85 - Batch: 147392 - Loss: 0.035181 - Time:339.1271674633026\n",
      "Epoch: 85 - Batch: 151488 - Loss: 0.072258 - Time:348.5577480792999\n",
      "Epoch: 86 - Batch: 4032 - Loss: 0.027962 - Time:9.558703184127808\n",
      "Epoch: 86 - Batch: 8128 - Loss: 0.028295 - Time:18.968887329101562\n",
      "Epoch: 86 - Batch: 12224 - Loss: 0.222063 - Time:28.37840175628662\n",
      "Epoch: 86 - Batch: 16320 - Loss: 0.023159 - Time:37.78670072555542\n",
      "Epoch: 86 - Batch: 20416 - Loss: 0.086067 - Time:47.1966814994812\n",
      "Epoch: 86 - Batch: 24512 - Loss: 0.019412 - Time:56.605319023132324\n",
      "Epoch: 86 - Batch: 28608 - Loss: 0.027216 - Time:66.04432606697083\n",
      "Epoch: 86 - Batch: 32704 - Loss: 0.099228 - Time:75.4534809589386\n",
      "Epoch: 86 - Batch: 36800 - Loss: 0.040973 - Time:84.86248278617859\n",
      "Epoch: 86 - Batch: 40896 - Loss: 0.032364 - Time:94.27220702171326\n",
      "Epoch: 86 - Batch: 44992 - Loss: 0.023968 - Time:103.69758462905884\n",
      "Epoch: 86 - Batch: 49088 - Loss: 0.129725 - Time:113.10626983642578\n",
      "Epoch: 86 - Batch: 53184 - Loss: 0.148022 - Time:122.51339030265808\n",
      "Epoch: 86 - Batch: 57280 - Loss: 0.061198 - Time:131.93971395492554\n",
      "Epoch: 86 - Batch: 61376 - Loss: 0.051355 - Time:141.34845685958862\n",
      "Epoch: 86 - Batch: 65472 - Loss: 0.032627 - Time:150.7577497959137\n",
      "Epoch: 86 - Batch: 69568 - Loss: 0.198088 - Time:160.16771793365479\n",
      "Epoch: 86 - Batch: 73664 - Loss: 0.043055 - Time:169.57889652252197\n",
      "Epoch: 86 - Batch: 77760 - Loss: 0.146392 - Time:178.99046301841736\n",
      "Epoch: 86 - Batch: 81856 - Loss: 0.090573 - Time:188.40006709098816\n",
      "Epoch: 86 - Batch: 85952 - Loss: 0.172366 - Time:197.81034517288208\n",
      "Epoch: 86 - Batch: 90048 - Loss: 0.154731 - Time:207.23479223251343\n",
      "Epoch: 86 - Batch: 94144 - Loss: 0.079637 - Time:216.6431212425232\n",
      "Epoch: 86 - Batch: 98240 - Loss: 0.135189 - Time:226.053076505661\n",
      "Epoch: 86 - Batch: 102336 - Loss: 0.033448 - Time:235.46228671073914\n",
      "Epoch: 86 - Batch: 106432 - Loss: 0.025379 - Time:244.87155890464783\n",
      "Epoch: 86 - Batch: 110528 - Loss: 0.156117 - Time:254.29965829849243\n",
      "Epoch: 86 - Batch: 114624 - Loss: 0.042693 - Time:263.7118413448334\n",
      "Epoch: 86 - Batch: 118720 - Loss: 0.158090 - Time:273.1235001087189\n",
      "Epoch: 86 - Batch: 122816 - Loss: 0.185128 - Time:282.55101585388184\n",
      "Epoch: 86 - Batch: 126912 - Loss: 0.240299 - Time:291.96267080307007\n",
      "Epoch: 86 - Batch: 131008 - Loss: 0.047058 - Time:301.37505555152893\n",
      "Epoch: 86 - Batch: 135104 - Loss: 0.099932 - Time:310.78650975227356\n",
      "Epoch: 86 - Batch: 139200 - Loss: 0.109141 - Time:320.1971962451935\n",
      "Epoch: 86 - Batch: 143296 - Loss: 0.115831 - Time:329.6078441143036\n",
      "Epoch: 86 - Batch: 147392 - Loss: 0.069122 - Time:339.01707196235657\n",
      "Epoch: 86 - Batch: 151488 - Loss: 0.049066 - Time:348.4273638725281\n",
      "Epoch: 87 - Batch: 4032 - Loss: 0.151571 - Time:9.543319702148438\n",
      "Epoch: 87 - Batch: 8128 - Loss: 0.018353 - Time:18.95216131210327\n",
      "Epoch: 87 - Batch: 12224 - Loss: 0.025752 - Time:28.362069606781006\n",
      "Epoch: 87 - Batch: 16320 - Loss: 0.039916 - Time:37.801368951797485\n",
      "Epoch: 87 - Batch: 20416 - Loss: 0.014303 - Time:47.21390390396118\n",
      "Epoch: 87 - Batch: 24512 - Loss: 0.026698 - Time:56.62662076950073\n",
      "Epoch: 87 - Batch: 28608 - Loss: 0.129854 - Time:66.05479145050049\n",
      "Epoch: 87 - Batch: 32704 - Loss: 0.123157 - Time:75.46454119682312\n",
      "Epoch: 87 - Batch: 36800 - Loss: 0.398678 - Time:84.87580895423889\n",
      "Epoch: 87 - Batch: 40896 - Loss: 0.017336 - Time:94.2872097492218\n",
      "Epoch: 87 - Batch: 44992 - Loss: 0.032641 - Time:103.69936513900757\n",
      "Epoch: 87 - Batch: 49088 - Loss: 0.221487 - Time:113.10909962654114\n",
      "Epoch: 87 - Batch: 53184 - Loss: 0.053095 - Time:122.51916980743408\n",
      "Epoch: 87 - Batch: 57280 - Loss: 0.413326 - Time:131.92767071723938\n",
      "Epoch: 87 - Batch: 61376 - Loss: 0.365808 - Time:141.355224609375\n",
      "Epoch: 87 - Batch: 65472 - Loss: 0.220433 - Time:150.7655954360962\n",
      "Epoch: 87 - Batch: 69568 - Loss: 0.025627 - Time:160.17840456962585\n",
      "Epoch: 87 - Batch: 73664 - Loss: 0.026418 - Time:169.58592700958252\n",
      "Epoch: 87 - Batch: 77760 - Loss: 0.028155 - Time:178.99406003952026\n",
      "Epoch: 87 - Batch: 81856 - Loss: 0.275884 - Time:188.42132353782654\n",
      "Epoch: 87 - Batch: 85952 - Loss: 0.032421 - Time:197.8317472934723\n",
      "Epoch: 87 - Batch: 90048 - Loss: 0.024617 - Time:207.2573003768921\n",
      "Epoch: 87 - Batch: 94144 - Loss: 0.059225 - Time:216.6673629283905\n",
      "Epoch: 87 - Batch: 98240 - Loss: 0.085157 - Time:226.07624769210815\n",
      "Epoch: 87 - Batch: 102336 - Loss: 0.115948 - Time:235.48794150352478\n",
      "Epoch: 87 - Batch: 106432 - Loss: 0.200421 - Time:244.89924025535583\n",
      "Epoch: 87 - Batch: 110528 - Loss: 0.049874 - Time:254.31121039390564\n",
      "Epoch: 87 - Batch: 114624 - Loss: 0.558535 - Time:263.72249031066895\n",
      "Epoch: 87 - Batch: 118720 - Loss: 0.130956 - Time:273.13410234451294\n",
      "Epoch: 87 - Batch: 122816 - Loss: 0.251944 - Time:282.5485620498657\n",
      "Epoch: 87 - Batch: 126912 - Loss: 0.125927 - Time:291.97611808776855\n",
      "Epoch: 87 - Batch: 131008 - Loss: 0.041197 - Time:301.38409662246704\n",
      "Epoch: 87 - Batch: 135104 - Loss: 0.254629 - Time:310.7946515083313\n",
      "Epoch: 87 - Batch: 139200 - Loss: 0.405155 - Time:320.20490288734436\n",
      "Epoch: 87 - Batch: 143296 - Loss: 0.022949 - Time:329.63198614120483\n",
      "Epoch: 87 - Batch: 147392 - Loss: 0.022618 - Time:339.04246068000793\n",
      "Epoch: 87 - Batch: 151488 - Loss: 0.119462 - Time:348.4543089866638\n",
      "Epoch: 88 - Batch: 4032 - Loss: 0.016238 - Time:9.558412551879883\n",
      "Epoch: 88 - Batch: 8128 - Loss: 0.041261 - Time:18.97224760055542\n",
      "Epoch: 88 - Batch: 12224 - Loss: 0.050833 - Time:28.38533329963684\n",
      "Epoch: 88 - Batch: 16320 - Loss: 0.147209 - Time:37.79799008369446\n",
      "Epoch: 88 - Batch: 20416 - Loss: 0.435397 - Time:47.21149969100952\n",
      "Epoch: 88 - Batch: 24512 - Loss: 0.065807 - Time:56.626394748687744\n",
      "Epoch: 88 - Batch: 28608 - Loss: 0.059201 - Time:66.03949284553528\n",
      "Epoch: 88 - Batch: 32704 - Loss: 0.072963 - Time:75.48068714141846\n",
      "Epoch: 88 - Batch: 36800 - Loss: 0.024584 - Time:84.89471459388733\n",
      "Epoch: 88 - Batch: 40896 - Loss: 0.046336 - Time:94.30835795402527\n",
      "Epoch: 88 - Batch: 44992 - Loss: 0.042881 - Time:103.72069120407104\n",
      "Epoch: 88 - Batch: 49088 - Loss: 0.121499 - Time:113.14906311035156\n",
      "Epoch: 88 - Batch: 53184 - Loss: 0.028307 - Time:122.56017446517944\n",
      "Epoch: 88 - Batch: 57280 - Loss: 0.056906 - Time:131.97116708755493\n",
      "Epoch: 88 - Batch: 61376 - Loss: 0.036638 - Time:141.39931440353394\n",
      "Epoch: 88 - Batch: 65472 - Loss: 0.090830 - Time:150.81018209457397\n",
      "Epoch: 88 - Batch: 69568 - Loss: 0.113112 - Time:160.22198057174683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 - Batch: 73664 - Loss: 0.199038 - Time:169.6334810256958\n",
      "Epoch: 88 - Batch: 77760 - Loss: 0.218985 - Time:179.04432344436646\n",
      "Epoch: 88 - Batch: 81856 - Loss: 0.108210 - Time:188.4551386833191\n",
      "Epoch: 88 - Batch: 85952 - Loss: 0.028952 - Time:197.86748719215393\n",
      "Epoch: 88 - Batch: 90048 - Loss: 0.342223 - Time:207.2802538871765\n",
      "Epoch: 88 - Batch: 94144 - Loss: 0.019517 - Time:216.7075366973877\n",
      "Epoch: 88 - Batch: 98240 - Loss: 0.029686 - Time:226.1195547580719\n",
      "Epoch: 88 - Batch: 102336 - Loss: 0.073261 - Time:235.53033065795898\n",
      "Epoch: 88 - Batch: 106432 - Loss: 0.217617 - Time:244.94371366500854\n",
      "Epoch: 88 - Batch: 110528 - Loss: 0.124855 - Time:254.35665369033813\n",
      "Epoch: 88 - Batch: 114624 - Loss: 0.064393 - Time:263.78671646118164\n",
      "Epoch: 88 - Batch: 118720 - Loss: 0.115821 - Time:273.19865822792053\n",
      "Epoch: 88 - Batch: 122816 - Loss: 0.155418 - Time:282.62674927711487\n",
      "Epoch: 88 - Batch: 126912 - Loss: 0.509344 - Time:292.03842067718506\n",
      "Epoch: 88 - Batch: 131008 - Loss: 0.046400 - Time:301.4487359523773\n",
      "Epoch: 88 - Batch: 135104 - Loss: 0.049586 - Time:310.85871291160583\n",
      "Epoch: 88 - Batch: 139200 - Loss: 0.258592 - Time:320.27090787887573\n",
      "Epoch: 88 - Batch: 143296 - Loss: 0.043186 - Time:329.68255519866943\n",
      "Epoch: 88 - Batch: 147392 - Loss: 0.036957 - Time:339.094514131546\n",
      "Epoch: 88 - Batch: 151488 - Loss: 0.091423 - Time:348.5045123100281\n",
      "Epoch: 89 - Batch: 4032 - Loss: 0.023341 - Time:9.591469764709473\n",
      "Epoch: 89 - Batch: 8128 - Loss: 0.162704 - Time:19.003859996795654\n",
      "Epoch: 89 - Batch: 12224 - Loss: 0.029793 - Time:28.416661262512207\n",
      "Epoch: 89 - Batch: 16320 - Loss: 0.134758 - Time:37.82741904258728\n",
      "Epoch: 89 - Batch: 20416 - Loss: 0.064456 - Time:47.24028968811035\n",
      "Epoch: 89 - Batch: 24512 - Loss: 0.041983 - Time:56.65151238441467\n",
      "Epoch: 89 - Batch: 28608 - Loss: 0.047062 - Time:66.06297993659973\n",
      "Epoch: 89 - Batch: 32704 - Loss: 0.062428 - Time:75.4762487411499\n",
      "Epoch: 89 - Batch: 36800 - Loss: 0.023824 - Time:84.90337133407593\n",
      "Epoch: 89 - Batch: 40896 - Loss: 0.050383 - Time:94.31380653381348\n",
      "Epoch: 89 - Batch: 44992 - Loss: 0.031753 - Time:103.72492480278015\n",
      "Epoch: 89 - Batch: 49088 - Loss: 0.031562 - Time:113.13651275634766\n",
      "Epoch: 89 - Batch: 53184 - Loss: 0.136518 - Time:122.56553864479065\n",
      "Epoch: 89 - Batch: 57280 - Loss: 0.032851 - Time:131.97533750534058\n",
      "Epoch: 89 - Batch: 61376 - Loss: 0.082758 - Time:141.386137008667\n",
      "Epoch: 89 - Batch: 65472 - Loss: 0.034653 - Time:150.81621289253235\n",
      "Epoch: 89 - Batch: 69568 - Loss: 0.302886 - Time:160.2298436164856\n",
      "Epoch: 89 - Batch: 73664 - Loss: 0.152931 - Time:169.6440305709839\n",
      "Epoch: 89 - Batch: 77760 - Loss: 0.239898 - Time:179.05753445625305\n",
      "Epoch: 89 - Batch: 81856 - Loss: 0.037870 - Time:188.47145318984985\n",
      "Epoch: 89 - Batch: 85952 - Loss: 0.043654 - Time:197.88552236557007\n",
      "Epoch: 89 - Batch: 90048 - Loss: 0.279515 - Time:207.2989637851715\n",
      "Epoch: 89 - Batch: 94144 - Loss: 0.033116 - Time:216.71258568763733\n",
      "Epoch: 89 - Batch: 98240 - Loss: 0.160866 - Time:226.14008355140686\n",
      "Epoch: 89 - Batch: 102336 - Loss: 0.075913 - Time:235.55095195770264\n",
      "Epoch: 89 - Batch: 106432 - Loss: 0.284642 - Time:244.9621045589447\n",
      "Epoch: 89 - Batch: 110528 - Loss: 0.033626 - Time:254.37305307388306\n",
      "Epoch: 89 - Batch: 114624 - Loss: 0.316589 - Time:263.8022918701172\n",
      "Epoch: 89 - Batch: 118720 - Loss: 0.210205 - Time:273.21309542655945\n",
      "Epoch: 89 - Batch: 122816 - Loss: 0.109383 - Time:282.62424492836\n",
      "Epoch: 89 - Batch: 126912 - Loss: 0.095535 - Time:292.05190658569336\n",
      "Epoch: 89 - Batch: 131008 - Loss: 0.197452 - Time:301.461101770401\n",
      "Epoch: 89 - Batch: 135104 - Loss: 0.055951 - Time:310.87211418151855\n",
      "Epoch: 89 - Batch: 139200 - Loss: 0.093679 - Time:320.2853627204895\n",
      "Epoch: 89 - Batch: 143296 - Loss: 0.072597 - Time:329.6957767009735\n",
      "Epoch: 89 - Batch: 147392 - Loss: 0.049742 - Time:339.1082270145416\n",
      "Epoch: 89 - Batch: 151488 - Loss: 0.251402 - Time:348.52152919769287\n",
      "Epoch: 90 - Batch: 4032 - Loss: 0.052578 - Time:9.59601640701294\n",
      "Epoch: 90 - Batch: 8128 - Loss: 0.040339 - Time:19.00653076171875\n",
      "Epoch: 90 - Batch: 12224 - Loss: 0.097674 - Time:28.420207738876343\n",
      "Epoch: 90 - Batch: 16320 - Loss: 0.036748 - Time:37.833733797073364\n",
      "Epoch: 90 - Batch: 20416 - Loss: 0.060896 - Time:47.246296644210815\n",
      "Epoch: 90 - Batch: 24512 - Loss: 0.105204 - Time:56.65842032432556\n",
      "Epoch: 90 - Batch: 28608 - Loss: 0.076648 - Time:66.06957864761353\n",
      "Epoch: 90 - Batch: 32704 - Loss: 0.044050 - Time:75.48028683662415\n",
      "Epoch: 90 - Batch: 36800 - Loss: 0.090355 - Time:84.90730023384094\n",
      "Epoch: 90 - Batch: 40896 - Loss: 0.352838 - Time:94.31714344024658\n",
      "Epoch: 90 - Batch: 44992 - Loss: 0.082632 - Time:103.72578763961792\n",
      "Epoch: 90 - Batch: 49088 - Loss: 0.121875 - Time:113.1347770690918\n",
      "Epoch: 90 - Batch: 53184 - Loss: 0.112153 - Time:122.5605685710907\n",
      "Epoch: 90 - Batch: 57280 - Loss: 0.038219 - Time:131.9690682888031\n",
      "Epoch: 90 - Batch: 61376 - Loss: 0.042451 - Time:141.3793761730194\n",
      "Epoch: 90 - Batch: 65472 - Loss: 0.018565 - Time:150.8070297241211\n",
      "Epoch: 90 - Batch: 69568 - Loss: 0.056410 - Time:160.2175271511078\n",
      "Epoch: 90 - Batch: 73664 - Loss: 0.029202 - Time:169.62707996368408\n",
      "Epoch: 90 - Batch: 77760 - Loss: 0.054976 - Time:179.0356900691986\n",
      "Epoch: 90 - Batch: 81856 - Loss: 0.157415 - Time:188.44460344314575\n",
      "Epoch: 90 - Batch: 85952 - Loss: 0.112591 - Time:197.85233879089355\n",
      "Epoch: 90 - Batch: 90048 - Loss: 0.055175 - Time:207.25956392288208\n",
      "Epoch: 90 - Batch: 94144 - Loss: 0.032325 - Time:216.67100977897644\n",
      "Epoch: 90 - Batch: 98240 - Loss: 0.055071 - Time:226.09817790985107\n",
      "Epoch: 90 - Batch: 102336 - Loss: 0.140548 - Time:235.50667929649353\n",
      "Epoch: 90 - Batch: 106432 - Loss: 0.121205 - Time:244.91663336753845\n",
      "Epoch: 90 - Batch: 110528 - Loss: 0.204789 - Time:254.32621669769287\n",
      "Epoch: 90 - Batch: 114624 - Loss: 0.029768 - Time:263.7530393600464\n",
      "Epoch: 90 - Batch: 118720 - Loss: 0.052321 - Time:273.16461873054504\n",
      "Epoch: 90 - Batch: 122816 - Loss: 0.175934 - Time:282.57550621032715\n",
      "Epoch: 90 - Batch: 126912 - Loss: 0.055642 - Time:292.00554370880127\n",
      "Epoch: 90 - Batch: 131008 - Loss: 0.046306 - Time:301.4176208972931\n",
      "Epoch: 90 - Batch: 135104 - Loss: 0.263432 - Time:310.8275876045227\n",
      "Epoch: 90 - Batch: 139200 - Loss: 0.058574 - Time:320.2382607460022\n",
      "Epoch: 90 - Batch: 143296 - Loss: 0.159136 - Time:329.6475684642792\n",
      "Epoch: 90 - Batch: 147392 - Loss: 0.053778 - Time:339.0586061477661\n",
      "Epoch: 90 - Batch: 151488 - Loss: 0.031989 - Time:348.4688422679901\n",
      "Epoch: 91 - Batch: 4032 - Loss: 0.036397 - Time:9.590759992599487\n",
      "Epoch: 91 - Batch: 8128 - Loss: 0.116276 - Time:18.999690532684326\n",
      "Epoch: 91 - Batch: 12224 - Loss: 0.027899 - Time:28.410359144210815\n",
      "Epoch: 91 - Batch: 16320 - Loss: 0.029106 - Time:37.82342720031738\n",
      "Epoch: 91 - Batch: 20416 - Loss: 0.064721 - Time:47.234454870224\n",
      "Epoch: 91 - Batch: 24512 - Loss: 0.042203 - Time:56.64213228225708\n",
      "Epoch: 91 - Batch: 28608 - Loss: 0.032082 - Time:66.05076050758362\n",
      "Epoch: 91 - Batch: 32704 - Loss: 0.146697 - Time:75.45913648605347\n",
      "Epoch: 91 - Batch: 36800 - Loss: 0.259562 - Time:84.88427543640137\n",
      "Epoch: 91 - Batch: 40896 - Loss: 0.037655 - Time:94.29494452476501\n",
      "Epoch: 91 - Batch: 44992 - Loss: 0.036801 - Time:103.70578813552856\n",
      "Epoch: 91 - Batch: 49088 - Loss: 0.073136 - Time:113.11416792869568\n",
      "Epoch: 91 - Batch: 53184 - Loss: 0.120705 - Time:122.54042363166809\n",
      "Epoch: 91 - Batch: 57280 - Loss: 0.072920 - Time:131.9501805305481\n",
      "Epoch: 91 - Batch: 61376 - Loss: 0.086656 - Time:141.35878777503967\n",
      "Epoch: 91 - Batch: 65472 - Loss: 0.215976 - Time:150.7871367931366\n",
      "Epoch: 91 - Batch: 69568 - Loss: 0.652926 - Time:160.1992518901825\n",
      "Epoch: 91 - Batch: 73664 - Loss: 0.114851 - Time:169.61075162887573\n",
      "Epoch: 91 - Batch: 77760 - Loss: 0.021052 - Time:179.01934695243835\n",
      "Epoch: 91 - Batch: 81856 - Loss: 0.020010 - Time:188.43246698379517\n",
      "Epoch: 91 - Batch: 85952 - Loss: 0.396830 - Time:197.84145307540894\n",
      "Epoch: 91 - Batch: 90048 - Loss: 0.119303 - Time:207.2506730556488\n",
      "Epoch: 91 - Batch: 94144 - Loss: 0.066413 - Time:216.65966892242432\n",
      "Epoch: 91 - Batch: 98240 - Loss: 0.104758 - Time:226.0855278968811\n",
      "Epoch: 91 - Batch: 102336 - Loss: 0.139877 - Time:235.49382281303406\n",
      "Epoch: 91 - Batch: 106432 - Loss: 0.118029 - Time:244.9036409854889\n",
      "Epoch: 91 - Batch: 110528 - Loss: 0.112994 - Time:254.31116390228271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 - Batch: 114624 - Loss: 0.024579 - Time:263.7205505371094\n",
      "Epoch: 91 - Batch: 118720 - Loss: 0.075275 - Time:273.14756417274475\n",
      "Epoch: 91 - Batch: 122816 - Loss: 0.020097 - Time:282.5588870048523\n",
      "Epoch: 91 - Batch: 126912 - Loss: 0.227313 - Time:291.98433017730713\n",
      "Epoch: 91 - Batch: 131008 - Loss: 0.019901 - Time:301.3958423137665\n",
      "Epoch: 91 - Batch: 135104 - Loss: 0.054942 - Time:310.8061935901642\n",
      "Epoch: 91 - Batch: 139200 - Loss: 0.041509 - Time:320.2151930332184\n",
      "Epoch: 91 - Batch: 143296 - Loss: 0.047840 - Time:329.6225917339325\n",
      "Epoch: 91 - Batch: 147392 - Loss: 0.123527 - Time:339.03097438812256\n",
      "Epoch: 91 - Batch: 151488 - Loss: 0.039056 - Time:348.4404032230377\n",
      "Epoch: 92 - Batch: 4032 - Loss: 0.026664 - Time:9.536698579788208\n",
      "Epoch: 92 - Batch: 8128 - Loss: 0.025687 - Time:18.975086450576782\n",
      "Epoch: 92 - Batch: 12224 - Loss: 0.035156 - Time:28.387681245803833\n",
      "Epoch: 92 - Batch: 16320 - Loss: 0.031470 - Time:37.7989444732666\n",
      "Epoch: 92 - Batch: 20416 - Loss: 0.022725 - Time:47.20893359184265\n",
      "Epoch: 92 - Batch: 24512 - Loss: 0.028274 - Time:56.63538885116577\n",
      "Epoch: 92 - Batch: 28608 - Loss: 0.051649 - Time:66.04501128196716\n",
      "Epoch: 92 - Batch: 32704 - Loss: 0.039398 - Time:75.45598292350769\n",
      "Epoch: 92 - Batch: 36800 - Loss: 0.125963 - Time:84.88367486000061\n",
      "Epoch: 92 - Batch: 40896 - Loss: 0.065258 - Time:94.29556179046631\n",
      "Epoch: 92 - Batch: 44992 - Loss: 0.064424 - Time:103.7059953212738\n",
      "Epoch: 92 - Batch: 49088 - Loss: 0.355780 - Time:113.1181206703186\n",
      "Epoch: 92 - Batch: 53184 - Loss: 0.035583 - Time:122.5288496017456\n",
      "Epoch: 92 - Batch: 57280 - Loss: 0.026482 - Time:131.93949818611145\n",
      "Epoch: 92 - Batch: 61376 - Loss: 0.234217 - Time:141.35154795646667\n",
      "Epoch: 92 - Batch: 65472 - Loss: 0.052845 - Time:150.7643940448761\n",
      "Epoch: 92 - Batch: 69568 - Loss: 0.285136 - Time:160.19163966178894\n",
      "Epoch: 92 - Batch: 73664 - Loss: 0.349192 - Time:169.60046005249023\n",
      "Epoch: 92 - Batch: 77760 - Loss: 0.048096 - Time:179.01074194908142\n",
      "Epoch: 92 - Batch: 81856 - Loss: 0.064905 - Time:188.4201967716217\n",
      "Epoch: 92 - Batch: 85952 - Loss: 0.219068 - Time:197.8474190235138\n",
      "Epoch: 92 - Batch: 90048 - Loss: 0.310878 - Time:207.25900673866272\n",
      "Epoch: 92 - Batch: 94144 - Loss: 0.186815 - Time:216.66816759109497\n",
      "Epoch: 92 - Batch: 98240 - Loss: 0.336597 - Time:226.0969443321228\n",
      "Epoch: 92 - Batch: 102336 - Loss: 0.160256 - Time:235.50791668891907\n",
      "Epoch: 92 - Batch: 106432 - Loss: 0.059159 - Time:244.91778254508972\n",
      "Epoch: 92 - Batch: 110528 - Loss: 0.028986 - Time:254.32659769058228\n",
      "Epoch: 92 - Batch: 114624 - Loss: 0.055403 - Time:263.736172914505\n",
      "Epoch: 92 - Batch: 118720 - Loss: 0.061950 - Time:273.1469793319702\n",
      "Epoch: 92 - Batch: 122816 - Loss: 0.362040 - Time:282.55827164649963\n",
      "Epoch: 92 - Batch: 126912 - Loss: 0.038914 - Time:291.9676797389984\n",
      "Epoch: 92 - Batch: 131008 - Loss: 0.048231 - Time:301.3929901123047\n",
      "Epoch: 92 - Batch: 135104 - Loss: 0.030051 - Time:310.80269026756287\n",
      "Epoch: 92 - Batch: 139200 - Loss: 0.203969 - Time:320.21363735198975\n",
      "Epoch: 92 - Batch: 143296 - Loss: 0.043826 - Time:329.62692403793335\n",
      "Epoch: 92 - Batch: 147392 - Loss: 0.100135 - Time:339.039479970932\n",
      "Epoch: 92 - Batch: 151488 - Loss: 0.049895 - Time:348.46920466423035\n",
      "Epoch: 93 - Batch: 4032 - Loss: 0.054148 - Time:9.547208786010742\n",
      "Epoch: 93 - Batch: 8128 - Loss: 0.012978 - Time:18.984052658081055\n",
      "Epoch: 93 - Batch: 12224 - Loss: 0.060900 - Time:28.393349170684814\n",
      "Epoch: 93 - Batch: 16320 - Loss: 0.042908 - Time:37.802852630615234\n",
      "Epoch: 93 - Batch: 20416 - Loss: 0.075721 - Time:47.231900215148926\n",
      "Epoch: 93 - Batch: 24512 - Loss: 0.122508 - Time:56.64366292953491\n",
      "Epoch: 93 - Batch: 28608 - Loss: 0.305275 - Time:66.05373120307922\n",
      "Epoch: 93 - Batch: 32704 - Loss: 0.095145 - Time:75.46441292762756\n",
      "Epoch: 93 - Batch: 36800 - Loss: 0.031520 - Time:84.87592625617981\n",
      "Epoch: 93 - Batch: 40896 - Loss: 0.058709 - Time:94.28596234321594\n",
      "Epoch: 93 - Batch: 44992 - Loss: 0.050410 - Time:103.69555687904358\n",
      "Epoch: 93 - Batch: 49088 - Loss: 0.024552 - Time:113.10717725753784\n",
      "Epoch: 93 - Batch: 53184 - Loss: 0.019973 - Time:122.53424954414368\n",
      "Epoch: 93 - Batch: 57280 - Loss: 0.042057 - Time:131.94234609603882\n",
      "Epoch: 93 - Batch: 61376 - Loss: 0.047016 - Time:141.35173416137695\n",
      "Epoch: 93 - Batch: 65472 - Loss: 0.075885 - Time:150.76269054412842\n",
      "Epoch: 93 - Batch: 69568 - Loss: 0.032739 - Time:160.17341804504395\n",
      "Epoch: 93 - Batch: 73664 - Loss: 0.081781 - Time:169.60211563110352\n",
      "Epoch: 93 - Batch: 77760 - Loss: 0.092144 - Time:179.02031707763672\n",
      "Epoch: 93 - Batch: 81856 - Loss: 0.019888 - Time:188.43216180801392\n",
      "Epoch: 93 - Batch: 85952 - Loss: 0.037522 - Time:197.86292719841003\n",
      "Epoch: 93 - Batch: 90048 - Loss: 0.024948 - Time:207.27705812454224\n",
      "Epoch: 93 - Batch: 94144 - Loss: 0.066195 - Time:216.68934035301208\n",
      "Epoch: 93 - Batch: 98240 - Loss: 0.115409 - Time:226.10129833221436\n",
      "Epoch: 93 - Batch: 102336 - Loss: 0.023588 - Time:235.51501607894897\n",
      "Epoch: 93 - Batch: 106432 - Loss: 0.128660 - Time:244.9268934726715\n",
      "Epoch: 93 - Batch: 110528 - Loss: 0.027991 - Time:254.33816003799438\n",
      "Epoch: 93 - Batch: 114624 - Loss: 0.162744 - Time:263.74966168403625\n",
      "Epoch: 93 - Batch: 118720 - Loss: 0.190079 - Time:273.1775059700012\n",
      "Epoch: 93 - Batch: 122816 - Loss: 0.222696 - Time:282.58910393714905\n",
      "Epoch: 93 - Batch: 126912 - Loss: 0.016256 - Time:292.0015766620636\n",
      "Epoch: 93 - Batch: 131008 - Loss: 0.090790 - Time:301.40975999832153\n",
      "Epoch: 93 - Batch: 135104 - Loss: 0.067109 - Time:310.83536767959595\n",
      "Epoch: 93 - Batch: 139200 - Loss: 0.138133 - Time:320.24599504470825\n",
      "Epoch: 93 - Batch: 143296 - Loss: 0.137463 - Time:329.65550923347473\n",
      "Epoch: 93 - Batch: 147392 - Loss: 0.034486 - Time:339.08177065849304\n",
      "Epoch: 93 - Batch: 151488 - Loss: 0.123494 - Time:348.4921922683716\n",
      "Epoch: 94 - Batch: 4032 - Loss: 0.104853 - Time:9.531529664993286\n",
      "Epoch: 94 - Batch: 8128 - Loss: 0.207121 - Time:18.93888807296753\n",
      "Epoch: 94 - Batch: 12224 - Loss: 0.028101 - Time:28.375433444976807\n",
      "Epoch: 94 - Batch: 16320 - Loss: 0.053891 - Time:37.78437876701355\n",
      "Epoch: 94 - Batch: 20416 - Loss: 0.021671 - Time:47.19361114501953\n",
      "Epoch: 94 - Batch: 24512 - Loss: 0.066083 - Time:56.60490846633911\n",
      "Epoch: 94 - Batch: 28608 - Loss: 0.022533 - Time:66.0139672756195\n",
      "Epoch: 94 - Batch: 32704 - Loss: 0.051825 - Time:75.42393827438354\n",
      "Epoch: 94 - Batch: 36800 - Loss: 0.070452 - Time:84.83361530303955\n",
      "Epoch: 94 - Batch: 40896 - Loss: 0.016139 - Time:94.24306893348694\n",
      "Epoch: 94 - Batch: 44992 - Loss: 0.217363 - Time:103.66748070716858\n",
      "Epoch: 94 - Batch: 49088 - Loss: 0.067702 - Time:113.07603073120117\n",
      "Epoch: 94 - Batch: 53184 - Loss: 0.019782 - Time:122.48421430587769\n",
      "Epoch: 94 - Batch: 57280 - Loss: 0.071734 - Time:131.8948357105255\n",
      "Epoch: 94 - Batch: 61376 - Loss: 0.029306 - Time:141.32363867759705\n",
      "Epoch: 94 - Batch: 65472 - Loss: 0.128748 - Time:150.73399472236633\n",
      "Epoch: 94 - Batch: 69568 - Loss: 0.080867 - Time:160.14356994628906\n",
      "Epoch: 94 - Batch: 73664 - Loss: 0.136311 - Time:169.5714931488037\n",
      "Epoch: 94 - Batch: 77760 - Loss: 0.181726 - Time:178.98259377479553\n",
      "Epoch: 94 - Batch: 81856 - Loss: 0.055679 - Time:188.3941740989685\n",
      "Epoch: 94 - Batch: 85952 - Loss: 0.026885 - Time:197.80758428573608\n",
      "Epoch: 94 - Batch: 90048 - Loss: 0.218022 - Time:207.21947956085205\n",
      "Epoch: 94 - Batch: 94144 - Loss: 0.024311 - Time:216.6299946308136\n",
      "Epoch: 94 - Batch: 98240 - Loss: 0.055873 - Time:226.04276156425476\n",
      "Epoch: 94 - Batch: 102336 - Loss: 0.067625 - Time:235.45643162727356\n",
      "Epoch: 94 - Batch: 106432 - Loss: 0.102794 - Time:244.88482093811035\n",
      "Epoch: 94 - Batch: 110528 - Loss: 0.224158 - Time:254.29704236984253\n",
      "Epoch: 94 - Batch: 114624 - Loss: 0.104391 - Time:263.7061598300934\n",
      "Epoch: 94 - Batch: 118720 - Loss: 0.107353 - Time:273.11625576019287\n",
      "Epoch: 94 - Batch: 122816 - Loss: 0.074297 - Time:282.54202818870544\n",
      "Epoch: 94 - Batch: 126912 - Loss: 0.071658 - Time:291.9504783153534\n",
      "Epoch: 94 - Batch: 131008 - Loss: 0.157493 - Time:301.3598999977112\n",
      "Epoch: 94 - Batch: 135104 - Loss: 0.156447 - Time:310.78781962394714\n",
      "Epoch: 94 - Batch: 139200 - Loss: 0.045840 - Time:320.1987204551697\n",
      "Epoch: 94 - Batch: 143296 - Loss: 0.023835 - Time:329.6100924015045\n",
      "Epoch: 94 - Batch: 147392 - Loss: 0.092226 - Time:339.0194842815399\n",
      "Epoch: 94 - Batch: 151488 - Loss: 0.237896 - Time:348.4295597076416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 - Batch: 4032 - Loss: 0.041163 - Time:9.55743670463562\n",
      "Epoch: 95 - Batch: 8128 - Loss: 0.102504 - Time:18.99421501159668\n",
      "Epoch: 95 - Batch: 12224 - Loss: 0.064593 - Time:28.40384078025818\n",
      "Epoch: 95 - Batch: 16320 - Loss: 0.027282 - Time:37.81459856033325\n",
      "Epoch: 95 - Batch: 20416 - Loss: 0.027565 - Time:47.22298455238342\n",
      "Epoch: 95 - Batch: 24512 - Loss: 0.019421 - Time:56.63504242897034\n",
      "Epoch: 95 - Batch: 28608 - Loss: 0.031265 - Time:66.04613161087036\n",
      "Epoch: 95 - Batch: 32704 - Loss: 0.111991 - Time:75.45693707466125\n",
      "Epoch: 95 - Batch: 36800 - Loss: 0.112890 - Time:84.86908197402954\n",
      "Epoch: 95 - Batch: 40896 - Loss: 0.280081 - Time:94.29621171951294\n",
      "Epoch: 95 - Batch: 44992 - Loss: 0.029152 - Time:103.7048659324646\n",
      "Epoch: 95 - Batch: 49088 - Loss: 0.034049 - Time:113.11289429664612\n",
      "Epoch: 95 - Batch: 53184 - Loss: 0.027209 - Time:122.52015399932861\n",
      "Epoch: 95 - Batch: 57280 - Loss: 0.125081 - Time:131.9458725452423\n",
      "Epoch: 95 - Batch: 61376 - Loss: 0.076200 - Time:141.3541784286499\n",
      "Epoch: 95 - Batch: 65472 - Loss: 0.046066 - Time:150.7633192539215\n",
      "Epoch: 95 - Batch: 69568 - Loss: 0.033918 - Time:160.18798422813416\n",
      "Epoch: 95 - Batch: 73664 - Loss: 0.037729 - Time:169.59588241577148\n",
      "Epoch: 95 - Batch: 77760 - Loss: 0.447416 - Time:179.0073802471161\n",
      "Epoch: 95 - Batch: 81856 - Loss: 0.167733 - Time:188.4168519973755\n",
      "Epoch: 95 - Batch: 85952 - Loss: 0.075636 - Time:197.82734966278076\n",
      "Epoch: 95 - Batch: 90048 - Loss: 0.034530 - Time:207.23691320419312\n",
      "Epoch: 95 - Batch: 94144 - Loss: 0.301968 - Time:216.64699268341064\n",
      "Epoch: 95 - Batch: 98240 - Loss: 0.033177 - Time:226.05587029457092\n",
      "Epoch: 95 - Batch: 102336 - Loss: 0.265233 - Time:235.47999477386475\n",
      "Epoch: 95 - Batch: 106432 - Loss: 0.064314 - Time:244.88804411888123\n",
      "Epoch: 95 - Batch: 110528 - Loss: 0.024769 - Time:254.29540014266968\n",
      "Epoch: 95 - Batch: 114624 - Loss: 0.040629 - Time:263.7012903690338\n",
      "Epoch: 95 - Batch: 118720 - Loss: 0.023434 - Time:273.1251256465912\n",
      "Epoch: 95 - Batch: 122816 - Loss: 0.154738 - Time:282.5319073200226\n",
      "Epoch: 95 - Batch: 126912 - Loss: 0.017657 - Time:291.9419860839844\n",
      "Epoch: 95 - Batch: 131008 - Loss: 0.502559 - Time:301.3697295188904\n",
      "Epoch: 95 - Batch: 135104 - Loss: 0.026717 - Time:310.77883768081665\n",
      "Epoch: 95 - Batch: 139200 - Loss: 0.053748 - Time:320.18904209136963\n",
      "Epoch: 95 - Batch: 143296 - Loss: 0.043701 - Time:329.59853506088257\n",
      "Epoch: 95 - Batch: 147392 - Loss: 0.389021 - Time:339.0071761608124\n",
      "Epoch: 95 - Batch: 151488 - Loss: 0.042183 - Time:348.4159951210022\n",
      "Epoch: 96 - Batch: 4032 - Loss: 0.038286 - Time:9.553065538406372\n",
      "Epoch: 96 - Batch: 8128 - Loss: 0.183169 - Time:18.991151332855225\n",
      "Epoch: 96 - Batch: 12224 - Loss: 0.049918 - Time:28.40232253074646\n",
      "Epoch: 96 - Batch: 16320 - Loss: 0.095437 - Time:37.813446044921875\n",
      "Epoch: 96 - Batch: 20416 - Loss: 0.018595 - Time:47.22508382797241\n",
      "Epoch: 96 - Batch: 24512 - Loss: 0.031598 - Time:56.63692498207092\n",
      "Epoch: 96 - Batch: 28608 - Loss: 0.185775 - Time:66.06543779373169\n",
      "Epoch: 96 - Batch: 32704 - Loss: 0.052624 - Time:75.47341346740723\n",
      "Epoch: 96 - Batch: 36800 - Loss: 0.068441 - Time:84.88411331176758\n",
      "Epoch: 96 - Batch: 40896 - Loss: 0.033339 - Time:94.31347322463989\n",
      "Epoch: 96 - Batch: 44992 - Loss: 0.021943 - Time:103.72326898574829\n",
      "Epoch: 96 - Batch: 49088 - Loss: 0.260129 - Time:113.13386940956116\n",
      "Epoch: 96 - Batch: 53184 - Loss: 0.048407 - Time:122.5443766117096\n",
      "Epoch: 96 - Batch: 57280 - Loss: 0.063125 - Time:131.95592164993286\n",
      "Epoch: 96 - Batch: 61376 - Loss: 0.043765 - Time:141.36938571929932\n",
      "Epoch: 96 - Batch: 65472 - Loss: 0.028841 - Time:150.7801456451416\n",
      "Epoch: 96 - Batch: 69568 - Loss: 0.023321 - Time:160.19026398658752\n",
      "Epoch: 96 - Batch: 73664 - Loss: 0.096241 - Time:169.61816930770874\n",
      "Epoch: 96 - Batch: 77760 - Loss: 0.024610 - Time:179.03067684173584\n",
      "Epoch: 96 - Batch: 81856 - Loss: 0.424830 - Time:188.4421889781952\n",
      "Epoch: 96 - Batch: 85952 - Loss: 0.039397 - Time:197.85384702682495\n",
      "Epoch: 96 - Batch: 90048 - Loss: 0.017918 - Time:207.28227281570435\n",
      "Epoch: 96 - Batch: 94144 - Loss: 0.171528 - Time:216.69422793388367\n",
      "Epoch: 96 - Batch: 98240 - Loss: 0.084613 - Time:226.10439014434814\n",
      "Epoch: 96 - Batch: 102336 - Loss: 0.405982 - Time:235.5342915058136\n",
      "Epoch: 96 - Batch: 106432 - Loss: 0.042552 - Time:244.9462809562683\n",
      "Epoch: 96 - Batch: 110528 - Loss: 0.039181 - Time:254.35754656791687\n",
      "Epoch: 96 - Batch: 114624 - Loss: 0.090552 - Time:263.7670645713806\n",
      "Epoch: 96 - Batch: 118720 - Loss: 0.130591 - Time:273.17821192741394\n",
      "Epoch: 96 - Batch: 122816 - Loss: 0.016704 - Time:282.58766198158264\n",
      "Epoch: 96 - Batch: 126912 - Loss: 0.020296 - Time:291.99803590774536\n",
      "Epoch: 96 - Batch: 131008 - Loss: 0.103236 - Time:301.409686088562\n",
      "Epoch: 96 - Batch: 135104 - Loss: 0.032329 - Time:310.8382704257965\n",
      "Epoch: 96 - Batch: 139200 - Loss: 0.348413 - Time:320.24884819984436\n",
      "Epoch: 96 - Batch: 143296 - Loss: 0.139909 - Time:329.65907526016235\n",
      "Epoch: 96 - Batch: 147392 - Loss: 0.093952 - Time:339.0695798397064\n",
      "Epoch: 96 - Batch: 151488 - Loss: 0.028114 - Time:348.49591064453125\n",
      "Epoch: 97 - Batch: 4032 - Loss: 0.058900 - Time:9.567402362823486\n",
      "Epoch: 97 - Batch: 8128 - Loss: 0.022780 - Time:18.978225469589233\n",
      "Epoch: 97 - Batch: 12224 - Loss: 0.077681 - Time:28.4150812625885\n",
      "Epoch: 97 - Batch: 16320 - Loss: 0.062329 - Time:37.826417446136475\n",
      "Epoch: 97 - Batch: 20416 - Loss: 0.124528 - Time:47.2362334728241\n",
      "Epoch: 97 - Batch: 24512 - Loss: 0.082292 - Time:56.66416358947754\n",
      "Epoch: 97 - Batch: 28608 - Loss: 0.204620 - Time:66.07519197463989\n",
      "Epoch: 97 - Batch: 32704 - Loss: 0.035828 - Time:75.48532056808472\n",
      "Epoch: 97 - Batch: 36800 - Loss: 0.108485 - Time:84.89589881896973\n",
      "Epoch: 97 - Batch: 40896 - Loss: 0.138595 - Time:94.30392456054688\n",
      "Epoch: 97 - Batch: 44992 - Loss: 0.196026 - Time:103.7136218547821\n",
      "Epoch: 97 - Batch: 49088 - Loss: 0.035833 - Time:113.12310600280762\n",
      "Epoch: 97 - Batch: 53184 - Loss: 0.044147 - Time:122.53462839126587\n",
      "Epoch: 97 - Batch: 57280 - Loss: 0.108424 - Time:131.96312546730042\n",
      "Epoch: 97 - Batch: 61376 - Loss: 0.031161 - Time:141.37028002738953\n",
      "Epoch: 97 - Batch: 65472 - Loss: 0.175027 - Time:150.77909541130066\n",
      "Epoch: 97 - Batch: 69568 - Loss: 0.302001 - Time:160.18917155265808\n",
      "Epoch: 97 - Batch: 73664 - Loss: 0.056177 - Time:169.6019401550293\n",
      "Epoch: 97 - Batch: 77760 - Loss: 0.051131 - Time:179.03024172782898\n",
      "Epoch: 97 - Batch: 81856 - Loss: 0.029612 - Time:188.44134306907654\n",
      "Epoch: 97 - Batch: 85952 - Loss: 0.127133 - Time:197.87050127983093\n",
      "Epoch: 97 - Batch: 90048 - Loss: 0.026894 - Time:207.2818694114685\n",
      "Epoch: 97 - Batch: 94144 - Loss: 0.061516 - Time:216.69612503051758\n",
      "Epoch: 97 - Batch: 98240 - Loss: 0.019903 - Time:226.1076123714447\n",
      "Epoch: 97 - Batch: 102336 - Loss: 0.058028 - Time:235.51929354667664\n",
      "Epoch: 97 - Batch: 106432 - Loss: 0.068305 - Time:244.9296464920044\n",
      "Epoch: 97 - Batch: 110528 - Loss: 0.038408 - Time:254.33978271484375\n",
      "Epoch: 97 - Batch: 114624 - Loss: 0.115354 - Time:263.75176072120667\n",
      "Epoch: 97 - Batch: 118720 - Loss: 0.051480 - Time:273.16348481178284\n",
      "Epoch: 97 - Batch: 122816 - Loss: 0.071022 - Time:282.588166475296\n",
      "Epoch: 97 - Batch: 126912 - Loss: 0.208441 - Time:291.9970669746399\n",
      "Epoch: 97 - Batch: 131008 - Loss: 0.181083 - Time:301.4070792198181\n",
      "Epoch: 97 - Batch: 135104 - Loss: 0.038824 - Time:310.8205499649048\n",
      "Epoch: 97 - Batch: 139200 - Loss: 0.027401 - Time:320.24817085266113\n",
      "Epoch: 97 - Batch: 143296 - Loss: 0.043596 - Time:329.65814185142517\n",
      "Epoch: 97 - Batch: 147392 - Loss: 0.029296 - Time:339.0686876773834\n",
      "Epoch: 97 - Batch: 151488 - Loss: 0.079296 - Time:348.4977514743805\n",
      "Epoch: 98 - Batch: 4032 - Loss: 0.195068 - Time:9.548879623413086\n",
      "Epoch: 98 - Batch: 8128 - Loss: 0.026622 - Time:18.96091651916504\n",
      "Epoch: 98 - Batch: 12224 - Loss: 0.120430 - Time:28.397768259048462\n",
      "Epoch: 98 - Batch: 16320 - Loss: 0.042476 - Time:37.80786633491516\n",
      "Epoch: 98 - Batch: 20416 - Loss: 0.080878 - Time:47.21655750274658\n",
      "Epoch: 98 - Batch: 24512 - Loss: 0.138520 - Time:56.62834119796753\n",
      "Epoch: 98 - Batch: 28608 - Loss: 0.014551 - Time:66.04012656211853\n",
      "Epoch: 98 - Batch: 32704 - Loss: 0.104737 - Time:75.45341873168945\n",
      "Epoch: 98 - Batch: 36800 - Loss: 0.055196 - Time:84.86600351333618\n",
      "Epoch: 98 - Batch: 40896 - Loss: 0.064097 - Time:94.27704739570618\n",
      "Epoch: 98 - Batch: 44992 - Loss: 0.189800 - Time:103.70531558990479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 - Batch: 49088 - Loss: 0.100937 - Time:113.11602783203125\n",
      "Epoch: 98 - Batch: 53184 - Loss: 0.069299 - Time:122.52554821968079\n",
      "Epoch: 98 - Batch: 57280 - Loss: 0.029761 - Time:131.93733048439026\n",
      "Epoch: 98 - Batch: 61376 - Loss: 0.025317 - Time:141.36485862731934\n",
      "Epoch: 98 - Batch: 65472 - Loss: 0.085758 - Time:150.7747769355774\n",
      "Epoch: 98 - Batch: 69568 - Loss: 0.075758 - Time:160.1845464706421\n",
      "Epoch: 98 - Batch: 73664 - Loss: 0.062395 - Time:169.60935044288635\n",
      "Epoch: 98 - Batch: 77760 - Loss: 0.025425 - Time:179.0172302722931\n",
      "Epoch: 98 - Batch: 81856 - Loss: 0.043488 - Time:188.42880058288574\n",
      "Epoch: 98 - Batch: 85952 - Loss: 0.037840 - Time:197.84016180038452\n",
      "Epoch: 98 - Batch: 90048 - Loss: 0.029996 - Time:207.2477638721466\n",
      "Epoch: 98 - Batch: 94144 - Loss: 0.187321 - Time:216.65730166435242\n",
      "Epoch: 98 - Batch: 98240 - Loss: 0.038482 - Time:226.06885528564453\n",
      "Epoch: 98 - Batch: 102336 - Loss: 0.122628 - Time:235.47984552383423\n",
      "Epoch: 98 - Batch: 106432 - Loss: 0.014225 - Time:244.9080867767334\n",
      "Epoch: 98 - Batch: 110528 - Loss: 0.163128 - Time:254.31635189056396\n",
      "Epoch: 98 - Batch: 114624 - Loss: 0.033531 - Time:263.72552037239075\n",
      "Epoch: 98 - Batch: 118720 - Loss: 0.059269 - Time:273.13517689704895\n",
      "Epoch: 98 - Batch: 122816 - Loss: 0.040584 - Time:282.56357979774475\n",
      "Epoch: 98 - Batch: 126912 - Loss: 0.044803 - Time:291.97340989112854\n",
      "Epoch: 98 - Batch: 131008 - Loss: 0.028699 - Time:301.3817312717438\n",
      "Epoch: 98 - Batch: 135104 - Loss: 0.133481 - Time:310.8075752258301\n",
      "Epoch: 98 - Batch: 139200 - Loss: 0.084592 - Time:320.2159378528595\n",
      "Epoch: 98 - Batch: 143296 - Loss: 0.060874 - Time:329.6236310005188\n",
      "Epoch: 98 - Batch: 147392 - Loss: 0.102085 - Time:339.0319595336914\n",
      "Epoch: 98 - Batch: 151488 - Loss: 0.027789 - Time:348.44153118133545\n",
      "Epoch: 99 - Batch: 4032 - Loss: 0.039462 - Time:9.53315544128418\n",
      "Epoch: 99 - Batch: 8128 - Loss: 0.124778 - Time:18.969565391540527\n",
      "Epoch: 99 - Batch: 12224 - Loss: 0.025280 - Time:28.379616260528564\n",
      "Epoch: 99 - Batch: 16320 - Loss: 0.032953 - Time:37.78924822807312\n",
      "Epoch: 99 - Batch: 20416 - Loss: 0.019832 - Time:47.19721579551697\n",
      "Epoch: 99 - Batch: 24512 - Loss: 0.080666 - Time:56.61056447029114\n",
      "Epoch: 99 - Batch: 28608 - Loss: 0.016157 - Time:66.0210428237915\n",
      "Epoch: 99 - Batch: 32704 - Loss: 0.039256 - Time:75.4296543598175\n",
      "Epoch: 99 - Batch: 36800 - Loss: 0.086607 - Time:84.83934998512268\n",
      "Epoch: 99 - Batch: 40896 - Loss: 0.166661 - Time:94.26581501960754\n",
      "Epoch: 99 - Batch: 44992 - Loss: 0.021688 - Time:103.67400431632996\n",
      "Epoch: 99 - Batch: 49088 - Loss: 0.112976 - Time:113.0824031829834\n",
      "Epoch: 99 - Batch: 53184 - Loss: 0.265999 - Time:122.48963594436646\n",
      "Epoch: 99 - Batch: 57280 - Loss: 0.204588 - Time:131.91550755500793\n",
      "Epoch: 99 - Batch: 61376 - Loss: 0.086585 - Time:141.3237645626068\n",
      "Epoch: 99 - Batch: 65472 - Loss: 0.085908 - Time:150.73191809654236\n",
      "Epoch: 99 - Batch: 69568 - Loss: 0.030073 - Time:160.1600649356842\n",
      "Epoch: 99 - Batch: 73664 - Loss: 0.057346 - Time:169.56998586654663\n",
      "Epoch: 99 - Batch: 77760 - Loss: 0.023866 - Time:178.9804289340973\n",
      "Epoch: 99 - Batch: 81856 - Loss: 0.115762 - Time:188.39139580726624\n",
      "Epoch: 99 - Batch: 85952 - Loss: 0.284796 - Time:197.80347156524658\n",
      "Epoch: 99 - Batch: 90048 - Loss: 0.039409 - Time:207.21357488632202\n",
      "Epoch: 99 - Batch: 94144 - Loss: 0.135410 - Time:216.6218934059143\n",
      "Epoch: 99 - Batch: 98240 - Loss: 0.028510 - Time:226.03283596038818\n",
      "Epoch: 99 - Batch: 102336 - Loss: 0.162613 - Time:235.45919919013977\n",
      "Epoch: 99 - Batch: 106432 - Loss: 0.018928 - Time:244.8696949481964\n",
      "Epoch: 99 - Batch: 110528 - Loss: 0.020371 - Time:254.28077054023743\n",
      "Epoch: 99 - Batch: 114624 - Loss: 0.256571 - Time:263.6929545402527\n",
      "Epoch: 99 - Batch: 118720 - Loss: 0.065601 - Time:273.1187582015991\n",
      "Epoch: 99 - Batch: 122816 - Loss: 0.029425 - Time:282.52886056900024\n",
      "Epoch: 99 - Batch: 126912 - Loss: 0.021860 - Time:291.93996810913086\n",
      "Epoch: 99 - Batch: 131008 - Loss: 0.043248 - Time:301.3668360710144\n",
      "Epoch: 99 - Batch: 135104 - Loss: 0.086639 - Time:310.77785086631775\n",
      "Epoch: 99 - Batch: 139200 - Loss: 0.472138 - Time:320.18628120422363\n",
      "Epoch: 99 - Batch: 143296 - Loss: 0.052947 - Time:329.59651350975037\n",
      "Epoch: 99 - Batch: 147392 - Loss: 0.134902 - Time:339.0141603946686\n",
      "Epoch: 99 - Batch: 151488 - Loss: 0.060086 - Time:348.42242431640625\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 512\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "embedder = Embedder(embedding_size=embedding_size)\n",
    "arcface = ArcFaceLoss(num_classes=num_classe, embedding_size=embedding_size,margin=0.3, scale=30.0)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    embedder = nn.DataParallel(embedder)\n",
    "    arcface = nn.DataParallel(arcface)\n",
    "embedder = embedder.to(device)\n",
    "arcface = arcface.to(device)\n",
    "\n",
    "optimizer = optim.Adam(embedder.parameters(), lr=1e-3 ) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.9)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    e_time = time.time()\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        embedder.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embeddings = embedder(images)\n",
    "        \n",
    "        logits = arcface(embeddings, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        if (i+1) % 64 == 0:\n",
    "            print(f'Epoch: {epoch} - Batch: {i*batch_size} - Loss: {loss:.6f} - Time:{time.time() - e_time}')\n",
    "            with open('/home/ielab/project/samples/mosaic_loss.txt', 'a') as file:\n",
    "                file.write(f'{loss:.6f}\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d909c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "822d6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   \n",
    "train_results = []\n",
    "train_labels = []\n",
    "test_results = []\n",
    "test_labels = []\n",
    "\n",
    "embedder.eval()\n",
    "with torch.no_grad():\n",
    "  for img, label in trainloader:\n",
    "    img = img.cuda()\n",
    "    train_results.append(embedder(img).cpu().detach().numpy())\n",
    "    train_labels.append(label)\n",
    "\n",
    "train_results = np.concatenate(train_results)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "embedder.eval()\n",
    "with torch.no_grad():\n",
    "  for img, label in testloader:\n",
    "    img = img.cuda()\n",
    "    test_results.append(embedder(img).cpu().detach().numpy())\n",
    "    test_labels.append(label)\n",
    "\n",
    "test_results = np.concatenate(test_results)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30134ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9777071715730774\n",
      "test acc : 0.3052923923482233\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest acc : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# F1-Score\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241m.\u001b[39mclassification_report(test_labels, test_pred)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ielab/project/samples/gan_test_f1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     23\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# kNN  \n",
    "k = 50\n",
    "model = KNeighborsClassifier(n_neighbors = k)\n",
    "#  \n",
    "model.fit(train_results, train_labels)\n",
    "#knn \n",
    "train_pred = model.predict(train_results)\n",
    "train_acc = (train_pred == train_labels).mean()\n",
    "with open('/home/ielab/project/samples/gan_train_acc.txt', 'a') as file:\n",
    "    file.write(f'{train_acc:.6f}\\n')\n",
    "print(f'train acc : {train_acc}')\n",
    "test_pred = model.predict(test_results)\n",
    "test_acc = (test_pred == test_labels).mean()\n",
    "with open('/home/ielab/project/samples/gan_test_acc.txt', 'a') as file:\n",
    "    file.write(f'{test_acc:.6f}\\n')\n",
    "print(f'test acc : {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a4dbf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# F1-Score\n",
    "f1 = metrics.classification_report(test_labels, test_pred)\n",
    "with open('/home/ielab/project/samples/gan_test_f1.txt', 'a') as file:\n",
    "    file.write(f'{f1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2725ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.23      0.38        13\n",
      "           1       0.00      0.00      0.00        19\n",
      "           2       0.67      0.38      0.48        21\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       1.00      0.33      0.50         6\n",
      "           5       0.00      0.00      0.00        15\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       1.00      0.42      0.59        19\n",
      "           8       1.00      0.17      0.29        24\n",
      "           9       0.13      0.50      0.21        52\n",
      "          10       1.00      0.05      0.10        20\n",
      "          11       0.80      0.24      0.36        17\n",
      "          12       0.89      0.89      0.89         9\n",
      "          13       1.00      0.29      0.44        14\n",
      "          14       0.00      0.00      0.00        10\n",
      "          15       1.00      0.41      0.58        17\n",
      "          16       0.00      0.00      0.00         5\n",
      "          17       0.00      0.00      0.00         7\n",
      "          18       0.75      0.50      0.60         6\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.77      0.71      0.74        14\n",
      "          21       1.00      0.75      0.86        20\n",
      "          22       0.57      0.25      0.35        16\n",
      "          23       0.53      0.56      0.54        18\n",
      "          24       1.00      0.14      0.24        22\n",
      "          25       0.00      0.00      0.00        14\n",
      "          26       1.00      0.71      0.83        14\n",
      "          27       1.00      0.26      0.41        23\n",
      "          28       0.14      0.10      0.12        10\n",
      "          29       0.00      0.00      0.00        10\n",
      "          30       0.06      0.65      0.10        23\n",
      "          31       1.00      0.35      0.52        17\n",
      "          32       1.00      0.74      0.85        19\n",
      "          33       0.90      0.45      0.60        20\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.14      0.25         7\n",
      "          36       1.00      0.35      0.52        23\n",
      "          37       1.00      0.10      0.18        10\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.22      0.36         9\n",
      "          41       0.20      0.12      0.15         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.85      0.52      0.65        21\n",
      "          44       1.00      0.14      0.25         7\n",
      "          45       1.00      0.29      0.44        14\n",
      "          46       1.00      0.33      0.50         6\n",
      "          47       0.00      0.00      0.00        17\n",
      "          48       0.75      0.15      0.25        20\n",
      "          49       1.00      0.33      0.50        12\n",
      "          50       1.00      0.50      0.67        14\n",
      "          51       0.36      0.50      0.42        16\n",
      "          52       1.00      0.28      0.43        18\n",
      "          53       1.00      0.43      0.61        23\n",
      "          54       1.00      0.77      0.87        22\n",
      "          55       0.93      0.76      0.84        17\n",
      "          56       0.50      0.13      0.21        15\n",
      "          57       1.00      0.18      0.31        22\n",
      "          58       1.00      0.43      0.60        14\n",
      "          59       0.00      0.00      0.00         2\n",
      "          60       1.00      0.40      0.57        20\n",
      "          61       1.00      0.55      0.71        20\n",
      "          62       0.40      0.22      0.29         9\n",
      "          63       1.00      0.55      0.71        22\n",
      "          64       0.00      0.00      0.00         2\n",
      "          65       1.00      0.16      0.27        19\n",
      "          66       0.00      0.00      0.00         9\n",
      "          67       1.00      0.08      0.15        12\n",
      "          68       1.00      0.62      0.77        16\n",
      "          69       0.00      0.00      0.00         4\n",
      "          70       0.00      0.00      0.00        15\n",
      "          71       0.45      0.31      0.37        16\n",
      "          72       0.00      0.00      0.00        10\n",
      "          73       1.00      0.65      0.79        20\n",
      "          74       0.00      0.00      0.00        10\n",
      "          75       0.67      0.43      0.52        14\n",
      "          76       0.10      0.20      0.13        10\n",
      "          77       1.00      0.29      0.45        17\n",
      "          78       0.67      0.30      0.41        20\n",
      "          79       0.00      0.00      0.00         8\n",
      "          80       1.00      0.18      0.31        11\n",
      "          81       1.00      0.14      0.25         7\n",
      "          82       1.00      0.50      0.67        20\n",
      "          83       0.33      0.21      0.26        14\n",
      "          84       1.00      0.36      0.53        22\n",
      "          85       0.00      0.00      0.00         7\n",
      "          86       1.00      0.47      0.64        15\n",
      "          87       0.45      0.53      0.49        17\n",
      "          88       0.00      0.00      0.00         7\n",
      "          89       0.00      0.00      0.00        10\n",
      "          90       0.00      0.00      0.00         2\n",
      "          91       0.29      0.25      0.27         8\n",
      "          92       1.00      0.19      0.32        16\n",
      "          93       1.00      0.12      0.22         8\n",
      "          94       1.00      0.13      0.23        23\n",
      "          95       1.00      0.30      0.46        10\n",
      "          96       0.08      0.33      0.13         3\n",
      "          97       1.00      0.27      0.43        22\n",
      "          98       0.00      0.00      0.00         5\n",
      "          99       1.00      0.39      0.56        23\n",
      "         100       1.00      0.11      0.19        19\n",
      "         101       1.00      0.52      0.69        21\n",
      "         102       0.93      0.76      0.84        17\n",
      "         103       0.80      0.36      0.50        11\n",
      "         104       1.00      0.43      0.60        14\n",
      "         105       0.40      0.27      0.32        15\n",
      "         106       0.87      0.76      0.81        17\n",
      "         107       1.00      0.05      0.10        19\n",
      "         108       0.00      0.00      0.00         6\n",
      "         109       1.00      0.08      0.14        13\n",
      "         110       0.00      0.00      0.00         5\n",
      "         111       0.00      0.00      0.00         6\n",
      "         112       1.00      0.14      0.25         7\n",
      "         113       0.79      0.73      0.76        15\n",
      "         114       1.00      0.12      0.22         8\n",
      "         115       0.00      0.00      0.00         9\n",
      "         116       0.00      0.00      0.00         4\n",
      "         117       1.00      0.50      0.67        22\n",
      "         118       1.00      0.17      0.29        12\n",
      "         119       1.00      0.53      0.69        17\n",
      "         120       0.56      0.50      0.53        20\n",
      "         121       0.90      0.41      0.56        22\n",
      "         122       1.00      0.12      0.22         8\n",
      "         123       1.00      0.05      0.09        22\n",
      "         124       1.00      0.72      0.84        18\n",
      "         125       0.00      0.00      0.00         1\n",
      "         126       0.00      0.00      0.00         4\n",
      "         127       0.43      0.40      0.41        15\n",
      "         128       0.00      0.00      0.00         6\n",
      "         129       0.00      0.00      0.00        10\n",
      "         130       1.00      0.44      0.62         9\n",
      "         131       1.00      0.18      0.31        22\n",
      "         132       1.00      0.06      0.11        17\n",
      "         133       0.30      0.67      0.41         9\n",
      "         134       0.80      0.57      0.67         7\n",
      "         135       0.83      0.36      0.50        14\n",
      "         136       1.00      0.56      0.71        18\n",
      "         137       1.00      0.12      0.21        17\n",
      "         138       0.50      0.67      0.57         3\n",
      "         139       0.18      0.67      0.28         9\n",
      "         140       0.80      0.53      0.64        15\n",
      "         141       1.00      0.11      0.20         9\n",
      "         142       0.00      0.00      0.00         7\n",
      "         143       0.00      0.00      0.00         8\n",
      "         144       0.00      0.00      0.00         2\n",
      "         145       1.00      0.11      0.20         9\n",
      "         146       0.78      0.47      0.58        15\n",
      "         147       0.00      0.00      0.00         2\n",
      "         148       0.00      0.00      0.00        12\n",
      "         149       1.00      0.21      0.35        19\n",
      "         150       1.00      0.53      0.69        19\n",
      "         151       0.60      0.32      0.41        19\n",
      "         152       1.00      0.15      0.26        20\n",
      "         153       0.75      0.15      0.25        20\n",
      "         154       1.00      0.43      0.60        21\n",
      "         155       0.86      0.57      0.69        21\n",
      "         156       0.80      0.67      0.73        18\n",
      "         157       1.00      0.20      0.33         5\n",
      "         158       0.64      0.54      0.58        13\n",
      "         159       0.88      0.37      0.52        19\n",
      "         160       1.00      0.12      0.21        17\n",
      "         161       0.80      0.57      0.67        14\n",
      "         162       1.00      0.67      0.80        18\n",
      "         163       1.00      0.09      0.17        11\n",
      "         164       0.81      0.59      0.68        22\n",
      "         165       0.00      0.00      0.00         2\n",
      "         166       1.00      0.24      0.38        17\n",
      "         167       1.00      0.39      0.56        18\n",
      "         168       0.05      0.18      0.08        17\n",
      "         169       1.00      0.50      0.67        18\n",
      "         170       0.00      0.00      0.00        18\n",
      "         171       1.00      0.44      0.62        18\n",
      "         172       1.00      0.25      0.40         4\n",
      "         173       1.00      0.67      0.80        12\n",
      "         174       0.23      0.33      0.27        15\n",
      "         175       0.09      0.50      0.16         6\n",
      "         176       0.64      0.64      0.64        14\n",
      "         177       1.00      0.50      0.67         4\n",
      "         178       0.87      0.62      0.72        21\n",
      "         179       1.00      0.12      0.21        17\n",
      "         180       1.00      0.17      0.29         6\n",
      "         181       0.80      0.67      0.73        18\n",
      "         182       1.00      0.43      0.60         7\n",
      "         183       0.00      0.00      0.00         6\n",
      "         184       1.00      0.10      0.18        20\n",
      "         185       1.00      1.00      1.00         2\n",
      "         186       0.80      0.27      0.40        15\n",
      "         187       1.00      0.44      0.61        16\n",
      "         188       1.00      0.29      0.44         7\n",
      "         189       1.00      0.20      0.33        10\n",
      "         190       0.00      0.00      0.00         1\n",
      "         191       0.80      0.22      0.35        18\n",
      "         192       1.00      0.15      0.26        20\n",
      "         193       0.44      0.53      0.48        15\n",
      "         194       0.00      0.00      0.00         8\n",
      "         195       1.00      0.17      0.30        23\n",
      "         196       1.00      0.35      0.52        23\n",
      "         197       1.00      0.33      0.50         3\n",
      "         198       0.00      0.00      0.00         5\n",
      "         199       0.50      0.35      0.41        17\n",
      "         200       1.00      0.48      0.65        21\n",
      "         201       0.92      0.75      0.83        16\n",
      "         202       1.00      0.11      0.19        19\n",
      "         203       0.00      0.00      0.00         2\n",
      "         204       0.00      0.00      0.00         2\n",
      "         205       1.00      0.45      0.62        11\n",
      "         206       0.00      0.00      0.00         6\n",
      "         207       0.00      0.00      0.00         2\n",
      "         208       0.00      0.00      0.00         6\n",
      "         209       0.00      0.00      0.00        18\n",
      "         210       1.00      0.39      0.56        18\n",
      "         211       1.00      0.31      0.48        16\n",
      "         212       0.00      0.00      0.00         8\n",
      "         213       0.82      0.90      0.86        20\n",
      "         214       0.60      0.33      0.43         9\n",
      "         215       1.00      0.29      0.45        17\n",
      "         216       0.29      0.29      0.29         7\n",
      "         217       0.90      0.75      0.82        12\n",
      "         218       1.00      0.10      0.18        10\n",
      "         219       0.74      0.87      0.80        23\n",
      "         220       0.93      0.70      0.80        20\n",
      "         221       0.36      0.19      0.25        21\n",
      "         222       0.90      0.50      0.64        18\n",
      "         223       0.86      0.43      0.57        14\n",
      "         224       0.00      0.00      0.00        20\n",
      "         225       1.00      0.20      0.33        20\n",
      "         226       1.00      0.21      0.35        19\n",
      "         227       0.00      0.00      0.00         6\n",
      "         228       1.00      0.22      0.36        23\n",
      "         229       0.26      0.39      0.31        18\n",
      "         230       0.93      0.65      0.76        20\n",
      "         231       0.00      0.00      0.00         4\n",
      "         232       0.31      0.31      0.31        13\n",
      "         233       0.00      0.00      0.00         3\n",
      "         234       0.12      0.40      0.19         5\n",
      "         235       1.00      0.17      0.29         6\n",
      "         236       0.50      0.53      0.52        15\n",
      "         237       0.00      0.00      0.00        19\n",
      "         238       1.00      0.32      0.48        19\n",
      "         239       0.00      0.00      0.00        14\n",
      "         240       0.75      0.33      0.46         9\n",
      "         241       0.83      0.53      0.65        19\n",
      "         242       0.82      0.43      0.56        21\n",
      "         243       1.00      0.18      0.31        11\n",
      "         244       0.00      0.00      0.00         1\n",
      "         245       0.00      0.00      0.00         3\n",
      "         246       0.21      0.67      0.32         9\n",
      "         247       0.00      0.00      0.00        19\n",
      "         248       1.00      0.17      0.29        12\n",
      "         249       1.00      0.33      0.50         9\n",
      "         250       0.00      0.00      0.00         4\n",
      "         251       1.00      0.42      0.59        19\n",
      "         252       1.00      0.06      0.11        18\n",
      "         253       1.00      0.83      0.90        23\n",
      "         254       1.00      0.32      0.48        19\n",
      "         255       1.00      0.32      0.48        22\n",
      "         256       0.75      0.60      0.67        20\n",
      "         257       0.00      0.00      0.00         1\n",
      "         258       1.00      0.65      0.79        20\n",
      "         259       0.80      0.36      0.50        11\n",
      "         260       0.92      0.55      0.69        20\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       1.00      0.33      0.50         6\n",
      "         263       0.00      0.00      0.00         2\n",
      "         264       0.00      0.00      0.00         4\n",
      "         265       0.64      0.54      0.58        13\n",
      "         266       0.15      0.40      0.22        15\n",
      "         267       0.88      0.47      0.61        15\n",
      "         268       0.00      0.00      0.00         4\n",
      "         269       0.00      0.00      0.00         6\n",
      "         270       0.67      0.17      0.27        12\n",
      "         271       0.47      0.82      0.60        22\n",
      "         272       0.83      0.62      0.71        16\n",
      "         273       1.00      0.12      0.22        16\n",
      "         274       1.00      0.38      0.55        16\n",
      "         275       1.00      0.36      0.53        11\n",
      "         276       1.00      0.20      0.33        20\n",
      "         277       1.00      0.18      0.31        22\n",
      "         278       0.00      0.00      0.00        22\n",
      "         279       0.12      0.50      0.20         2\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00        16\n",
      "         282       0.47      0.41      0.44        17\n",
      "         283       1.00      0.67      0.80        18\n",
      "         284       1.00      0.33      0.50         3\n",
      "         285       0.86      0.43      0.57        14\n",
      "         286       0.05      0.23      0.08        13\n",
      "         287       1.00      0.33      0.50        24\n",
      "         288       1.00      0.42      0.59        12\n",
      "         289       1.00      0.58      0.74        12\n",
      "         290       0.90      0.64      0.75        14\n",
      "         291       0.00      0.00      0.00        11\n",
      "         292       1.00      0.17      0.29         6\n",
      "         293       0.00      0.00      0.00        12\n",
      "         294       0.22      0.29      0.25         7\n",
      "         295       0.20      0.17      0.18         6\n",
      "         296       0.25      0.06      0.10        17\n",
      "         297       1.00      0.30      0.46        20\n",
      "         298       1.00      0.24      0.38        21\n",
      "         299       1.00      0.57      0.73         7\n",
      "         300       1.00      0.60      0.75         5\n",
      "         301       0.04      0.50      0.08         6\n",
      "         302       1.00      0.28      0.43        18\n",
      "         303       0.67      0.25      0.36        16\n",
      "         304       0.00      0.00      0.00         8\n",
      "         305       0.21      0.72      0.32        18\n",
      "         306       0.00      0.00      0.00        10\n",
      "         307       1.00      0.14      0.25         7\n",
      "         308       0.44      0.27      0.33        15\n",
      "         309       0.00      0.00      0.00         3\n",
      "         310       0.92      0.86      0.89        14\n",
      "         311       1.00      0.29      0.45        17\n",
      "         312       1.00      0.50      0.67        16\n",
      "         313       0.00      0.00      0.00         2\n",
      "         314       0.57      0.17      0.27        23\n",
      "         315       1.00      0.55      0.71        11\n",
      "         316       1.00      0.50      0.67         6\n",
      "         317       0.00      0.00      0.00         1\n",
      "         318       0.00      0.00      0.00         8\n",
      "         319       1.00      0.41      0.58        17\n",
      "         320       0.00      0.00      0.00         7\n",
      "         321       0.00      0.00      0.00        13\n",
      "         322       0.00      0.00      0.00         4\n",
      "         323       0.91      0.50      0.65        20\n",
      "         324       0.00      0.00      0.00         5\n",
      "         325       1.00      0.36      0.53        11\n",
      "         326       0.33      0.44      0.38        16\n",
      "         327       0.00      0.00      0.00         1\n",
      "         328       0.00      0.00      0.00        20\n",
      "         329       0.89      0.42      0.57        19\n",
      "         330       1.00      0.17      0.29        12\n",
      "         331       0.00      0.00      0.00         6\n",
      "         332       0.00      0.00      0.00         4\n",
      "         333       0.80      0.36      0.50        11\n",
      "         334       1.00      0.57      0.73         7\n",
      "         335       0.00      0.00      0.00         1\n",
      "         336       0.60      0.33      0.43        18\n",
      "         337       1.00      0.20      0.33         5\n",
      "         338       1.00      0.35      0.52        17\n",
      "         339       1.00      0.17      0.29         6\n",
      "         340       0.00      0.00      0.00         1\n",
      "         341       0.80      0.29      0.42        14\n",
      "         342       0.50      0.50      0.50        12\n",
      "         343       0.00      0.00      0.00         6\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       1.00      0.48      0.65        21\n",
      "         346       0.31      0.24      0.27        17\n",
      "         347       0.00      0.00      0.00         7\n",
      "         348       0.89      0.36      0.52        22\n",
      "         350       0.00      0.00      0.00         4\n",
      "         351       0.00      0.00      0.00         2\n",
      "         352       0.92      0.52      0.67        23\n",
      "         353       1.00      0.60      0.75        10\n",
      "         354       0.00      0.00      0.00         4\n",
      "         355       0.00      0.00      0.00         1\n",
      "         356       1.00      0.29      0.44         7\n",
      "         357       1.00      0.11      0.20         9\n",
      "         358       1.00      0.30      0.46        20\n",
      "         359       0.00      0.00      0.00        13\n",
      "         360       0.22      0.33      0.27         6\n",
      "         361       0.50      0.61      0.55        18\n",
      "         362       0.00      0.00      0.00         7\n",
      "         363       0.00      0.00      0.00         3\n",
      "         364       0.00      0.00      0.00         4\n",
      "         365       0.00      0.00      0.00        14\n",
      "         366       0.86      0.35      0.50        17\n",
      "         367       0.00      0.00      0.00         4\n",
      "         368       0.00      0.00      0.00         2\n",
      "         369       1.00      0.06      0.11        18\n",
      "         370       1.00      0.77      0.87        13\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         4\n",
      "         373       0.91      0.45      0.61        22\n",
      "         374       1.00      0.48      0.65        21\n",
      "         375       1.00      0.62      0.77         8\n",
      "         376       0.11      0.25      0.15        16\n",
      "         377       0.00      0.00      0.00        14\n",
      "         378       1.00      0.40      0.57        10\n",
      "         379       0.00      0.00      0.00         5\n",
      "         380       1.00      0.32      0.48        19\n",
      "         381       1.00      0.14      0.25         7\n",
      "         382       1.00      0.06      0.11        18\n",
      "         383       0.00      0.00      0.00         7\n",
      "         384       0.80      0.27      0.40        15\n",
      "         385       0.75      0.33      0.46        18\n",
      "         386       1.00      0.27      0.42        15\n",
      "         387       1.00      0.43      0.60         7\n",
      "         388       1.00      0.47      0.64        15\n",
      "         389       1.00      0.27      0.43        11\n",
      "         390       0.57      0.57      0.57         7\n",
      "         391       0.00      0.00      0.00         5\n",
      "         392       0.80      0.67      0.73        18\n",
      "         393       0.00      0.00      0.00         2\n",
      "         394       1.00      0.33      0.50        18\n",
      "         395       0.85      0.69      0.76        16\n",
      "         396       0.00      0.00      0.00        12\n",
      "         397       1.00      0.12      0.21        17\n",
      "         398       0.00      0.00      0.00         6\n",
      "         399       0.00      0.00      0.00         3\n",
      "         400       0.08      0.10      0.09        21\n",
      "         401       1.00      0.50      0.67        14\n",
      "         402       0.00      0.00      0.00        12\n",
      "         403       1.00      0.22      0.36        18\n",
      "         404       0.82      0.53      0.64        17\n",
      "         405       1.00      0.20      0.33        20\n",
      "         406       1.00      0.50      0.67        12\n",
      "         407       1.00      0.37      0.54        19\n",
      "         408       1.00      0.06      0.12        16\n",
      "         409       1.00      0.53      0.69        19\n",
      "         410       0.89      0.36      0.52        22\n",
      "         411       0.00      0.00      0.00         2\n",
      "         412       0.00      0.00      0.00        20\n",
      "         413       1.00      0.50      0.67        22\n",
      "         414       0.00      0.00      0.00         2\n",
      "         415       0.62      0.42      0.50        12\n",
      "         416       1.00      0.20      0.33        20\n",
      "         417       0.17      0.11      0.13         9\n",
      "         418       1.00      0.17      0.29        24\n",
      "         419       0.75      0.40      0.52        15\n",
      "         420       0.29      0.71      0.41        14\n",
      "         421       1.00      0.35      0.52        20\n",
      "         422       1.00      0.27      0.43        22\n",
      "         423       0.00      0.00      0.00        18\n",
      "         424       0.16      0.59      0.25        17\n",
      "         425       0.86      0.71      0.77        17\n",
      "         426       0.00      0.00      0.00         2\n",
      "         427       0.69      0.61      0.65        18\n",
      "         428       0.64      0.78      0.70         9\n",
      "         429       0.75      0.55      0.63        11\n",
      "         430       1.00      0.11      0.20         9\n",
      "         431       1.00      0.69      0.81        16\n",
      "         432       1.00      0.53      0.69        17\n",
      "         433       0.79      0.68      0.73        22\n",
      "         434       0.00      0.00      0.00         5\n",
      "         435       0.06      0.43      0.11         7\n",
      "         436       0.88      0.82      0.85        17\n",
      "         437       0.00      0.00      0.00        17\n",
      "         438       0.60      0.15      0.24        20\n",
      "         439       0.00      0.00      0.00         9\n",
      "         440       1.00      0.50      0.67         6\n",
      "         441       0.38      0.52      0.44        21\n",
      "         442       1.00      0.08      0.14        13\n",
      "         443       0.00      0.00      0.00        18\n",
      "         444       0.00      0.00      0.00         4\n",
      "         445       0.00      0.00      0.00         3\n",
      "         446       0.40      0.18      0.25        22\n",
      "         447       1.00      0.06      0.11        17\n",
      "         448       1.00      0.30      0.46        10\n",
      "         449       1.00      0.89      0.94        19\n",
      "         450       0.88      0.44      0.58        16\n",
      "         451       0.91      0.43      0.59        23\n",
      "         452       1.00      0.50      0.67        16\n",
      "         453       1.00      0.65      0.79        26\n",
      "         454       0.00      0.00      0.00         3\n",
      "         455       1.00      0.15      0.26        20\n",
      "         456       1.00      0.14      0.25        21\n",
      "         457       0.36      0.76      0.48        21\n",
      "         458       1.00      0.75      0.86         8\n",
      "         459       0.00      0.00      0.00         4\n",
      "         460       1.00      0.30      0.47        23\n",
      "         461       0.00      0.00      0.00         1\n",
      "         462       0.40      0.67      0.50        12\n",
      "         463       1.00      0.33      0.50        21\n",
      "         464       1.00      0.44      0.62        18\n",
      "         465       0.40      0.33      0.36        18\n",
      "         466       0.30      0.50      0.37        18\n",
      "         467       1.00      0.35      0.52        17\n",
      "         468       0.75      0.75      0.75         4\n",
      "         469       0.00      0.00      0.00         6\n",
      "         470       1.00      0.30      0.46        10\n",
      "         471       0.00      0.00      0.00         4\n",
      "         472       1.00      0.09      0.17        11\n",
      "         473       0.00      0.00      0.00         4\n",
      "         474       1.00      0.23      0.38        13\n",
      "         475       0.75      0.35      0.48        17\n",
      "         476       0.80      0.31      0.44        13\n",
      "         477       1.00      0.37      0.54        19\n",
      "         478       0.50      0.07      0.12        14\n",
      "         479       0.00      0.00      0.00         2\n",
      "         480       1.00      0.16      0.27        19\n",
      "         481       1.00      0.44      0.61        16\n",
      "         482       1.00      0.68      0.81        19\n",
      "         483       0.00      0.00      0.00        19\n",
      "         484       1.00      0.18      0.30        17\n",
      "         485       0.00      0.00      0.00         3\n",
      "         486       0.88      0.44      0.58        16\n",
      "         487       1.00      0.45      0.62        22\n",
      "         488       0.20      0.05      0.08        20\n",
      "         489       1.00      0.09      0.17        22\n",
      "         490       0.00      0.00      0.00         8\n",
      "         491       0.00      0.00      0.00         4\n",
      "         492       1.00      0.33      0.50         3\n",
      "         493       0.00      0.00      0.00         3\n",
      "         494       0.82      0.47      0.60        19\n",
      "         495       1.00      0.56      0.71        18\n",
      "         496       0.00      0.00      0.00         5\n",
      "         497       0.00      0.00      0.00        22\n",
      "         498       0.60      0.84      0.70        25\n",
      "         499       0.00      0.00      0.00        11\n",
      "         500       0.00      0.00      0.00        10\n",
      "         501       1.00      0.50      0.67        18\n",
      "         502       0.00      0.00      0.00         2\n",
      "         503       1.00      0.29      0.44        14\n",
      "         504       0.00      0.00      0.00        16\n",
      "         505       0.71      0.38      0.50        13\n",
      "         506       1.00      0.64      0.78        22\n",
      "         507       0.00      0.00      0.00        10\n",
      "         508       0.00      0.00      0.00        11\n",
      "         509       0.00      0.00      0.00        11\n",
      "         510       0.00      0.00      0.00         4\n",
      "         511       0.31      0.50      0.38        10\n",
      "         512       0.06      0.60      0.10         5\n",
      "         513       0.21      0.23      0.22        13\n",
      "         514       0.03      0.38      0.05         8\n",
      "         515       1.00      0.10      0.18        10\n",
      "         516       1.00      0.07      0.13        14\n",
      "         517       1.00      0.25      0.40        20\n",
      "         518       1.00      0.55      0.71        20\n",
      "         519       1.00      0.36      0.53        14\n",
      "         520       1.00      0.44      0.61        16\n",
      "         521       1.00      0.26      0.42        19\n",
      "         522       0.00      0.00      0.00         4\n",
      "         523       0.18      0.50      0.27         4\n",
      "         524       0.90      0.64      0.75        14\n",
      "         525       1.00      0.17      0.29         6\n",
      "         526       1.00      0.33      0.50         9\n",
      "         527       1.00      0.28      0.43        18\n",
      "         528       1.00      0.65      0.79        20\n",
      "         529       1.00      0.21      0.35        19\n",
      "         530       0.00      0.00      0.00        17\n",
      "         531       1.00      0.56      0.71        18\n",
      "         532       1.00      0.45      0.62        20\n",
      "         533       1.00      0.20      0.33        20\n",
      "         534       1.00      0.47      0.64        19\n",
      "         535       0.00      0.00      0.00         6\n",
      "         536       0.50      0.14      0.22         7\n",
      "         537       1.00      0.15      0.27        13\n",
      "         538       1.00      0.05      0.10        19\n",
      "         539       0.46      0.60      0.52        20\n",
      "         540       0.00      0.00      0.00         8\n",
      "         541       0.83      0.71      0.77        14\n",
      "         542       0.50      0.33      0.40         9\n",
      "         543       0.00      0.00      0.00         8\n",
      "         544       0.00      0.00      0.00        20\n",
      "         545       0.00      0.00      0.00         1\n",
      "         546       0.00      0.00      0.00        18\n",
      "         547       0.00      0.00      0.00         4\n",
      "         548       0.00      0.00      0.00         1\n",
      "         549       0.00      0.00      0.00         7\n",
      "         550       0.50      0.11      0.18         9\n",
      "         551       0.00      0.00      0.00        23\n",
      "         552       1.00      0.14      0.25         7\n",
      "         553       0.00      0.00      0.00        11\n",
      "         554       0.55      0.95      0.70        22\n",
      "         555       0.20      0.10      0.13        10\n",
      "         556       0.00      0.00      0.00         8\n",
      "         557       1.00      0.47      0.64        19\n",
      "         558       0.69      0.73      0.71        15\n",
      "         559       0.75      0.16      0.26        19\n",
      "         560       0.82      0.60      0.69        15\n",
      "         561       0.50      0.21      0.30        19\n",
      "         562       0.00      0.00      0.00        20\n",
      "         563       0.81      0.65      0.72        20\n",
      "         564       1.00      0.59      0.74        22\n",
      "         565       0.80      0.80      0.80        15\n",
      "         566       1.00      0.08      0.14        13\n",
      "         567       0.00      0.00      0.00         1\n",
      "         568       0.75      0.27      0.40        11\n",
      "         569       0.86      0.63      0.73        19\n",
      "         570       1.00      0.25      0.40         4\n",
      "         571       0.00      0.00      0.00        19\n",
      "         572       1.00      0.62      0.77         8\n",
      "         573       1.00      0.33      0.50        18\n",
      "         574       0.92      0.65      0.76        17\n",
      "         575       0.50      0.14      0.22         7\n",
      "         576       0.00      0.00      0.00        11\n",
      "         577       0.15      0.29      0.20         7\n",
      "         578       0.00      0.00      0.00         4\n",
      "         579       0.00      0.00      0.00        10\n",
      "         580       0.00      0.00      0.00         4\n",
      "         581       1.00      0.17      0.29        18\n",
      "         582       1.00      0.17      0.29         6\n",
      "         583       1.00      0.35      0.52        23\n",
      "         584       1.00      0.29      0.45        17\n",
      "         585       0.00      0.00      0.00         5\n",
      "         586       0.50      0.43      0.46         7\n",
      "         587       0.33      0.06      0.10        18\n",
      "         588       1.00      0.15      0.26        20\n",
      "         589       0.00      0.00      0.00        11\n",
      "         590       0.38      0.33      0.35         9\n",
      "         591       0.00      0.00      0.00        20\n",
      "         592       0.00      0.00      0.00         3\n",
      "         593       0.00      0.00      0.00         9\n",
      "         594       0.00      0.00      0.00         9\n",
      "         595       1.00      0.55      0.71        22\n",
      "         596       1.00      0.48      0.65        21\n",
      "         597       0.18      0.82      0.30        17\n",
      "         598       1.00      0.25      0.40         4\n",
      "         599       1.00      0.25      0.40        16\n",
      "         600       1.00      0.36      0.53        11\n",
      "         601       1.00      0.58      0.73        19\n",
      "         602       1.00      0.11      0.20         9\n",
      "         603       0.29      0.40      0.33         5\n",
      "         604       0.73      0.53      0.62        15\n",
      "         605       0.31      0.27      0.29        15\n",
      "         606       1.00      0.43      0.60        21\n",
      "         607       0.00      0.00      0.00         4\n",
      "         608       1.00      0.14      0.25         7\n",
      "         609       1.00      0.50      0.67        16\n",
      "         610       1.00      0.50      0.67         8\n",
      "         611       0.71      0.36      0.48        14\n",
      "         612       1.00      0.39      0.56        18\n",
      "         613       1.00      0.30      0.46        10\n",
      "         614       1.00      0.33      0.50        12\n",
      "         615       0.70      0.58      0.64        12\n",
      "         616       0.00      0.00      0.00         4\n",
      "         617       1.00      0.23      0.38        13\n",
      "         618       0.00      0.00      0.00         5\n",
      "         619       1.00      0.22      0.36        18\n",
      "         620       1.00      0.08      0.15        12\n",
      "         621       0.00      0.00      0.00         2\n",
      "         622       0.00      0.00      0.00        16\n",
      "         623       0.89      0.38      0.53        21\n",
      "         624       0.00      0.00      0.00        17\n",
      "         625       1.00      0.33      0.50        18\n",
      "         626       0.00      0.00      0.00         2\n",
      "         627       0.00      0.00      0.00         1\n",
      "         628       0.38      0.29      0.33        17\n",
      "         629       1.00      0.13      0.23        23\n",
      "         630       0.33      0.22      0.27         9\n",
      "         631       0.00      0.00      0.00         5\n",
      "         632       0.92      0.55      0.69        20\n",
      "         633       1.00      0.82      0.90        22\n",
      "         634       0.00      0.00      0.00        10\n",
      "         635       0.00      0.00      0.00         4\n",
      "         636       1.00      0.57      0.73        21\n",
      "         637       1.00      0.56      0.71        18\n",
      "         638       0.00      0.00      0.00         1\n",
      "         639       0.20      0.14      0.17         7\n",
      "         640       0.43      0.15      0.22        20\n",
      "         641       1.00      0.56      0.71         9\n",
      "         642       1.00      0.07      0.13        14\n",
      "         643       1.00      0.07      0.13        14\n",
      "         644       0.00      0.00      0.00        19\n",
      "         645       0.00      0.00      0.00        10\n",
      "         646       0.60      0.16      0.25        19\n",
      "         647       1.00      0.40      0.57        20\n",
      "         648       0.80      0.73      0.76        11\n",
      "         649       0.33      0.50      0.40        14\n",
      "         650       0.00      0.00      0.00         5\n",
      "         651       0.23      0.31      0.26        16\n",
      "         652       0.00      0.00      0.00        20\n",
      "         653       0.00      0.00      0.00         1\n",
      "         654       0.00      0.00      0.00         2\n",
      "         655       1.00      0.20      0.33         5\n",
      "         656       1.00      0.43      0.60         7\n",
      "         657       0.13      0.40      0.20         5\n",
      "         658       0.00      0.00      0.00        15\n",
      "         659       0.00      0.00      0.00         5\n",
      "         660       1.00      0.59      0.74        17\n",
      "         661       0.88      0.39      0.54        18\n",
      "         662       1.00      0.75      0.86        16\n",
      "         663       1.00      0.38      0.55         8\n",
      "         664       0.00      0.00      0.00         8\n",
      "         665       0.90      0.43      0.58        21\n",
      "         666       0.50      0.15      0.24        13\n",
      "         667       0.00      0.00      0.00         8\n",
      "         668       1.00      0.35      0.52        20\n",
      "         669       0.00      0.00      0.00         2\n",
      "         670       1.00      0.14      0.25         7\n",
      "         671       0.00      0.00      0.00         6\n",
      "         672       0.00      0.00      0.00         3\n",
      "         673       0.62      0.42      0.50        12\n",
      "         674       0.05      0.56      0.09        18\n",
      "         675       1.00      0.44      0.62        18\n",
      "         676       1.00      0.41      0.58        22\n",
      "         677       1.00      0.14      0.25        14\n",
      "         678       0.00      0.00      0.00         3\n",
      "         679       1.00      0.33      0.50        18\n",
      "         680       1.00      0.10      0.18        10\n",
      "         681       0.82      0.67      0.74        21\n",
      "         682       0.03      0.10      0.05        10\n",
      "         683       1.00      0.11      0.20        18\n",
      "         684       1.00      0.63      0.77        19\n",
      "         685       1.00      0.77      0.87        13\n",
      "         686       1.00      0.20      0.33         5\n",
      "         687       0.00      0.00      0.00         3\n",
      "         688       0.00      0.00      0.00         1\n",
      "         689       1.00      0.07      0.12        15\n",
      "         690       1.00      0.04      0.08        25\n",
      "         691       0.00      0.00      0.00         8\n",
      "         692       1.00      0.59      0.74        17\n",
      "         693       0.43      0.25      0.32        12\n",
      "         694       1.00      0.32      0.48        22\n",
      "         695       1.00      0.10      0.17        21\n",
      "         696       1.00      0.14      0.25        21\n",
      "         697       0.49      0.78      0.60        23\n",
      "         698       1.00      0.18      0.31        22\n",
      "         699       1.00      0.28      0.43        18\n",
      "         700       1.00      0.67      0.80        21\n",
      "         701       1.00      0.43      0.60         7\n",
      "         702       0.50      0.33      0.40        15\n",
      "         703       0.00      0.00      0.00         4\n",
      "         704       0.00      0.00      0.00         3\n",
      "         705       1.00      0.24      0.38        21\n",
      "         706       1.00      0.29      0.45        24\n",
      "         707       0.67      0.11      0.19        18\n",
      "         708       0.67      0.33      0.44         6\n",
      "         709       0.92      0.65      0.76        17\n",
      "         710       0.00      0.00      0.00        10\n",
      "         711       0.50      0.33      0.40         3\n",
      "         712       0.00      0.00      0.00         6\n",
      "         713       0.00      0.00      0.00         6\n",
      "         714       1.00      0.20      0.33        20\n",
      "         715       1.00      0.86      0.93        22\n",
      "         716       0.73      0.36      0.48        22\n",
      "         717       1.00      0.47      0.64        19\n",
      "         718       1.00      0.22      0.36         9\n",
      "         719       0.86      0.67      0.75         9\n",
      "         720       0.67      0.25      0.36         8\n",
      "         721       0.00      0.00      0.00         2\n",
      "         722       1.00      0.06      0.11        17\n",
      "         723       1.00      0.12      0.22         8\n",
      "         724       1.00      0.57      0.73         7\n",
      "         725       1.00      0.50      0.67         4\n",
      "         726       1.00      0.57      0.73        14\n",
      "         727       0.00      0.00      0.00         9\n",
      "         728       0.00      0.00      0.00         1\n",
      "         729       0.00      0.00      0.00         5\n",
      "         730       0.00      0.00      0.00         3\n",
      "         731       0.80      0.57      0.67         7\n",
      "         732       1.00      0.56      0.72        16\n",
      "         733       0.67      0.29      0.40         7\n",
      "         734       0.12      0.36      0.18        14\n",
      "         735       1.00      0.50      0.67        18\n",
      "         736       1.00      0.06      0.11        17\n",
      "         737       1.00      0.20      0.33         5\n",
      "         738       1.00      0.38      0.55         8\n",
      "         739       0.29      0.40      0.33         5\n",
      "         740       0.00      0.00      0.00         3\n",
      "         741       0.00      0.00      0.00         2\n",
      "         742       0.00      0.00      0.00        12\n",
      "         743       1.00      0.41      0.58        22\n",
      "         744       1.00      0.65      0.79        17\n",
      "         745       0.00      0.00      0.00         9\n",
      "         746       1.00      0.12      0.21        25\n",
      "         747       1.00      0.62      0.76        21\n",
      "         748       1.00      0.06      0.12        16\n",
      "         749       1.00      0.11      0.20        18\n",
      "         750       0.88      0.32      0.47        22\n",
      "         751       0.00      0.00      0.00         3\n",
      "         752       1.00      0.33      0.50        15\n",
      "         753       1.00      0.56      0.71         9\n",
      "         754       0.88      0.47      0.61        15\n",
      "         755       1.00      0.08      0.15        25\n",
      "         756       0.50      0.30      0.37        10\n",
      "         757       1.00      0.60      0.75        15\n",
      "         758       0.88      0.44      0.58        16\n",
      "         759       0.00      0.00      0.00         6\n",
      "         760       1.00      0.54      0.70        13\n",
      "         761       1.00      0.48      0.65        23\n",
      "         762       1.00      0.75      0.86        24\n",
      "         763       1.00      0.20      0.33        10\n",
      "         764       1.00      0.75      0.86         4\n",
      "         765       1.00      0.60      0.75        20\n",
      "         766       1.00      0.25      0.40        24\n",
      "         767       0.67      0.53      0.59        19\n",
      "         768       1.00      0.50      0.67         2\n",
      "         769       1.00      0.33      0.50        15\n",
      "         770       0.00      0.00      0.00         8\n",
      "         771       0.50      0.50      0.50        16\n",
      "         772       0.80      0.44      0.57        18\n",
      "         773       0.00      0.00      0.00         4\n",
      "         774       1.00      0.85      0.92        13\n",
      "         775       0.00      0.00      0.00         6\n",
      "         776       0.00      0.00      0.00         5\n",
      "         777       0.00      0.00      0.00         5\n",
      "         778       0.00      0.00      0.00         2\n",
      "         779       1.00      0.58      0.74        24\n",
      "         780       1.00      0.13      0.24        15\n",
      "         781       1.00      0.35      0.52        17\n",
      "         782       0.74      0.58      0.65        24\n",
      "         783       0.00      0.00      0.00        17\n",
      "         784       1.00      0.36      0.53        14\n",
      "         785       1.00      0.40      0.57        10\n",
      "         786       0.00      0.00      0.00         3\n",
      "         787       0.00      0.00      0.00         5\n",
      "         788       0.00      0.00      0.00        19\n",
      "         789       0.17      0.11      0.13         9\n",
      "         790       0.00      0.00      0.00         9\n",
      "         791       1.00      0.10      0.18        20\n",
      "         792       0.00      0.00      0.00         4\n",
      "         793       0.00      0.00      0.00         5\n",
      "         794       1.00      0.22      0.36         9\n",
      "         795       1.00      0.05      0.09        21\n",
      "         796       0.00      0.00      0.00        10\n",
      "         797       1.00      0.47      0.64        19\n",
      "         798       0.00      0.00      0.00         4\n",
      "         799       1.00      0.25      0.40        20\n",
      "         800       1.00      0.33      0.50         3\n",
      "         801       0.00      0.00      0.00         2\n",
      "         802       1.00      0.10      0.18        20\n",
      "         803       0.00      0.00      0.00        17\n",
      "         804       1.00      0.30      0.46        20\n",
      "         805       0.57      0.33      0.42        12\n",
      "         806       1.00      0.13      0.24        15\n",
      "         807       1.00      0.08      0.14        13\n",
      "         808       0.00      0.00      0.00         9\n",
      "         809       0.86      0.43      0.57        14\n",
      "         810       1.00      0.14      0.25         7\n",
      "         811       1.00      0.12      0.22        16\n",
      "         812       0.50      0.08      0.13        13\n",
      "         813       1.00      0.63      0.77        19\n",
      "         814       0.00      0.00      0.00         1\n",
      "         815       0.75      0.14      0.23        22\n",
      "         816       0.00      0.00      0.00        21\n",
      "         817       1.00      0.22      0.36        18\n",
      "         818       0.00      0.00      0.00         2\n",
      "         819       0.00      0.00      0.00         9\n",
      "         820       0.86      0.43      0.57        14\n",
      "         821       1.00      0.38      0.55        16\n",
      "         822       1.00      0.48      0.65        21\n",
      "         823       0.08      0.71      0.14        17\n",
      "         824       0.00      0.00      0.00         9\n",
      "         825       0.00      0.00      0.00         7\n",
      "         826       0.55      0.61      0.58        18\n",
      "         827       0.00      0.00      0.00         3\n",
      "         828       1.00      0.29      0.44         7\n",
      "         829       0.71      0.36      0.48        14\n",
      "         830       1.00      0.07      0.12        15\n",
      "         831       0.00      0.00      0.00         1\n",
      "         832       0.75      0.53      0.62        17\n",
      "         833       0.08      0.25      0.12        16\n",
      "         834       1.00      0.24      0.38        21\n",
      "         835       0.00      0.00      0.00        10\n",
      "         836       0.39      0.64      0.48        11\n",
      "         837       0.91      0.45      0.61        22\n",
      "         838       1.00      0.40      0.57        10\n",
      "         839       0.00      0.00      0.00        10\n",
      "         840       0.19      0.43      0.26         7\n",
      "         841       0.00      0.00      0.00        20\n",
      "         842       1.00      0.55      0.71        11\n",
      "         843       0.00      0.00      0.00         4\n",
      "         844       0.00      0.00      0.00         9\n",
      "         845       0.00      0.00      0.00         1\n",
      "         846       0.00      0.00      0.00        12\n",
      "         847       1.00      0.16      0.27        19\n",
      "         848       0.00      0.00      0.00         3\n",
      "         849       0.00      0.00      0.00        13\n",
      "         850       1.00      0.25      0.40        20\n",
      "         851       1.00      0.75      0.86         8\n",
      "         852       1.00      0.40      0.57        10\n",
      "         853       1.00      0.33      0.50         3\n",
      "         854       0.50      0.70      0.58        10\n",
      "         855       1.00      0.33      0.50        12\n",
      "         856       1.00      0.52      0.69        23\n",
      "         857       1.00      0.53      0.69        19\n",
      "         858       0.88      0.64      0.74        22\n",
      "         859       0.00      0.00      0.00        23\n",
      "         860       0.00      0.00      0.00        15\n",
      "         861       0.00      0.00      0.00         4\n",
      "         862       1.00      0.68      0.81        19\n",
      "         863       0.00      0.00      0.00         1\n",
      "         864       1.00      0.33      0.50         6\n",
      "         865       1.00      0.45      0.62        11\n",
      "         866       0.00      0.00      0.00        15\n",
      "         867       0.00      0.00      0.00         7\n",
      "         868       1.00      0.11      0.20         9\n",
      "         869       0.00      0.65      0.00        17\n",
      "         870       0.50      0.33      0.40        15\n",
      "         871       0.71      0.45      0.56        11\n",
      "         872       1.00      0.33      0.50        15\n",
      "         873       1.00      0.17      0.29        12\n",
      "         874       1.00      0.28      0.43        18\n",
      "         875       1.00      0.20      0.33        20\n",
      "         876       1.00      0.18      0.31        22\n",
      "         877       1.00      0.24      0.38        17\n",
      "         878       0.00      0.00      0.00         1\n",
      "         879       0.00      0.00      0.00         1\n",
      "         880       0.00      0.00      0.00         9\n",
      "         881       1.00      0.30      0.46        10\n",
      "         882       1.00      0.56      0.71        18\n",
      "         883       0.00      0.00      0.00         6\n",
      "         884       1.00      0.64      0.78        14\n",
      "         885       0.50      0.12      0.20         8\n",
      "         886       1.00      0.11      0.20         9\n",
      "         887       1.00      0.30      0.46        10\n",
      "         888       0.35      0.47      0.40        17\n",
      "         889       0.88      0.50      0.64        14\n",
      "         890       0.00      0.00      0.00         9\n",
      "         891       0.80      0.18      0.30        22\n",
      "         892       1.00      0.33      0.50         9\n",
      "         893       0.75      0.75      0.75        24\n",
      "         894       0.57      0.50      0.53         8\n",
      "         895       0.00      0.00      0.00         5\n",
      "         896       0.00      0.00      0.00         1\n",
      "         897       0.50      0.08      0.14        12\n",
      "         898       0.81      0.81      0.81        16\n",
      "         899       0.00      0.00      0.00         6\n",
      "         900       0.00      0.00      0.00         5\n",
      "         901       1.00      0.12      0.22        16\n",
      "         902       0.76      0.67      0.71        24\n",
      "         903       1.00      0.05      0.09        22\n",
      "         904       1.00      0.52      0.69        23\n",
      "         905       1.00      0.29      0.44        21\n",
      "         906       1.00      0.44      0.61        16\n",
      "         907       1.00      0.29      0.45        17\n",
      "         908       1.00      0.40      0.57        20\n",
      "         909       0.00      0.00      0.00        19\n",
      "         910       1.00      0.33      0.50         6\n",
      "         911       0.00      0.00      0.00         4\n",
      "         912       0.67      0.12      0.21        16\n",
      "         913       0.00      0.00      0.00        10\n",
      "         914       0.75      0.64      0.69        14\n",
      "         915       1.00      0.15      0.26        20\n",
      "         916       0.50      0.14      0.22         7\n",
      "         917       0.00      0.00      0.00        16\n",
      "         918       0.86      0.75      0.80         8\n",
      "         919       1.00      0.33      0.50        15\n",
      "         920       0.50      0.27      0.35        11\n",
      "         921       0.57      0.44      0.50         9\n",
      "         922       1.00      0.09      0.17        11\n",
      "         923       0.00      0.00      0.00         9\n",
      "         924       0.00      0.00      0.00         8\n",
      "         925       1.00      0.22      0.36         9\n",
      "         926       0.45      0.64      0.53        22\n",
      "         927       1.00      0.11      0.20         9\n",
      "         928       0.00      0.00      0.00         8\n",
      "         929       0.44      0.33      0.38        12\n",
      "         930       0.38      0.73      0.50        15\n",
      "         931       1.00      0.57      0.73         7\n",
      "         932       1.00      0.44      0.62        18\n",
      "         933       0.83      0.59      0.69        17\n",
      "         934       0.89      0.42      0.57        19\n",
      "         935       0.00      0.00      0.00         9\n",
      "         936       0.40      0.20      0.27        10\n",
      "         937       1.00      0.23      0.37        22\n",
      "         938       0.00      0.00      0.00         2\n",
      "         939       0.57      0.17      0.27        23\n",
      "         940       1.00      0.33      0.50         6\n",
      "         941       1.00      0.14      0.25        14\n",
      "         942       1.00      0.07      0.13        14\n",
      "         943       0.00      0.00      0.00         2\n",
      "         944       1.00      0.10      0.17        21\n",
      "         945       1.00      0.27      0.42        15\n",
      "         946       0.00      0.00      0.00         7\n",
      "         947       1.00      0.60      0.75        20\n",
      "         948       0.00      0.00      0.00        14\n",
      "         949       0.50      0.17      0.25         6\n",
      "         950       0.00      0.00      0.00         1\n",
      "         951       0.00      0.00      0.00         2\n",
      "         952       0.00      0.00      0.00         2\n",
      "         953       0.75      0.60      0.67        10\n",
      "         954       0.00      0.00      0.00         2\n",
      "         955       0.00      0.00      0.00         4\n",
      "         956       1.00      0.29      0.44        21\n",
      "         957       1.00      0.40      0.57         5\n",
      "         958       0.08      0.12      0.10         8\n",
      "         959       1.00      0.18      0.31        11\n",
      "         960       1.00      0.54      0.70        24\n",
      "         961       0.00      0.00      0.00         2\n",
      "         962       0.67      0.18      0.29        11\n",
      "         963       1.00      0.08      0.15        12\n",
      "         964       1.00      0.14      0.25         7\n",
      "         965       0.89      0.73      0.80        11\n",
      "         966       0.88      0.33      0.48        21\n",
      "         967       0.00      0.00      0.00        10\n",
      "         968       1.00      0.37      0.54        19\n",
      "         969       1.00      0.33      0.50         9\n",
      "         970       0.00      0.00      0.00         1\n",
      "         971       1.00      0.35      0.52        20\n",
      "         972       0.42      0.62      0.50        21\n",
      "         973       1.00      0.32      0.48        25\n",
      "         974       0.00      0.00      0.00         9\n",
      "         975       1.00      0.33      0.50         6\n",
      "         976       1.00      0.21      0.35        14\n",
      "         977       1.00      0.42      0.59        19\n",
      "         978       0.50      0.18      0.26        17\n",
      "         979       0.75      0.50      0.60         6\n",
      "         980       0.33      0.11      0.17         9\n",
      "         981       0.30      0.70      0.42        23\n",
      "         982       0.61      0.83      0.70        24\n",
      "         983       0.00      0.00      0.00         6\n",
      "         984       0.59      0.80      0.68        20\n",
      "         985       1.00      0.48      0.65        25\n",
      "         986       1.00      0.14      0.24        22\n",
      "         987       0.00      0.00      0.00         9\n",
      "         988       1.00      0.06      0.12        16\n",
      "         989       0.58      0.58      0.58        12\n",
      "         990       0.00      0.00      0.00         6\n",
      "         991       1.00      0.18      0.30        17\n",
      "         992       0.00      0.00      0.00         6\n",
      "         993       0.00      0.00      0.00         7\n",
      "         994       1.00      0.40      0.57        15\n",
      "         995       1.00      0.07      0.13        14\n",
      "         996       0.14      0.10      0.12        10\n",
      "         997       1.00      0.40      0.57        10\n",
      "         998       0.00      0.00      0.00         2\n",
      "         999       1.00      0.23      0.37        22\n",
      "        1000       1.00      0.15      0.26        20\n",
      "        1001       1.00      0.13      0.23        23\n",
      "        1002       0.00      0.00      0.00         6\n",
      "        1003       1.00      0.55      0.71        11\n",
      "        1004       0.80      0.63      0.71        19\n",
      "        1005       0.00      0.00      0.00         4\n",
      "        1006       0.00      0.00      0.00         6\n",
      "        1007       1.00      0.25      0.40        16\n",
      "        1008       0.00      0.00      0.00        10\n",
      "        1009       0.00      0.00      0.00         2\n",
      "        1010       0.70      0.32      0.44        22\n",
      "        1011       0.00      0.00      0.00         4\n",
      "        1012       0.00      0.00      0.00        10\n",
      "        1013       1.00      0.50      0.67         8\n",
      "        1014       1.00      0.38      0.55         8\n",
      "        1015       0.00      0.00      0.00         2\n",
      "        1016       0.00      0.00      0.00         8\n",
      "        1017       0.00      0.00      0.00         6\n",
      "        1018       0.89      0.40      0.55        20\n",
      "        1019       0.33      0.33      0.33        12\n",
      "        1020       0.00      0.00      0.00         5\n",
      "        1021       1.00      0.71      0.83        21\n",
      "        1022       1.00      0.26      0.42        19\n",
      "        1023       1.00      0.43      0.60         7\n",
      "        1024       0.00      0.00      0.00        11\n",
      "        1025       1.00      0.25      0.40         4\n",
      "        1026       0.00      0.00      0.00         4\n",
      "        1027       1.00      0.82      0.90        22\n",
      "        1028       0.43      0.56      0.49        16\n",
      "        1029       0.00      0.00      0.00         7\n",
      "        1030       0.00      0.00      0.00         8\n",
      "        1031       1.00      0.59      0.74        22\n",
      "        1032       0.53      0.71      0.61        14\n",
      "        1033       1.00      0.50      0.67        12\n",
      "        1034       0.00      0.00      0.00         3\n",
      "        1035       0.00      0.00      0.00        14\n",
      "        1036       0.00      0.00      0.00         2\n",
      "        1037       1.00      0.31      0.47        13\n",
      "        1038       1.00      0.12      0.22         8\n",
      "        1039       1.00      0.57      0.73         7\n",
      "        1040       0.00      0.00      0.00        19\n",
      "        1041       1.00      0.56      0.72        16\n",
      "        1042       1.00      0.28      0.43        18\n",
      "        1043       1.00      0.26      0.41        23\n",
      "        1044       1.00      0.22      0.36         9\n",
      "        1045       1.00      0.38      0.55        21\n",
      "        1046       0.50      0.14      0.22         7\n",
      "        1047       0.93      0.72      0.81        18\n",
      "        1048       1.00      0.33      0.50        18\n",
      "        1049       1.00      0.05      0.10        20\n",
      "        1050       0.00      0.00      0.00         3\n",
      "        1051       1.00      0.25      0.40         8\n",
      "        1052       1.00      0.40      0.57        20\n",
      "        1053       1.00      0.21      0.34        24\n",
      "        1054       1.00      0.12      0.21        17\n",
      "        1055       1.00      0.08      0.15        12\n",
      "        1056       0.00      0.00      0.00         4\n",
      "        1057       0.00      0.00      0.00         4\n",
      "        1058       1.00      0.42      0.59        12\n",
      "        1059       1.00      0.59      0.74        17\n",
      "        1060       1.00      0.29      0.44        21\n",
      "        1061       1.00      0.57      0.73        14\n",
      "        1062       0.45      0.79      0.58        19\n",
      "        1063       0.00      0.00      0.00        23\n",
      "        1064       1.00      0.46      0.63        13\n",
      "        1065       0.90      0.69      0.78        13\n",
      "        1066       1.00      0.21      0.35        19\n",
      "        1067       1.00      0.29      0.45        17\n",
      "        1068       0.00      0.00      0.00         6\n",
      "        1069       0.00      0.00      0.00        20\n",
      "        1070       0.50      0.40      0.44        15\n",
      "        1071       0.26      0.38      0.31        13\n",
      "        1072       0.50      0.25      0.33         8\n",
      "        1073       0.29      0.36      0.32        14\n",
      "        1074       0.91      0.42      0.57        24\n",
      "        1075       0.00      0.00      0.00        22\n",
      "        1076       1.00      0.29      0.45        17\n",
      "        1077       0.00      0.00      0.00         7\n",
      "        1078       1.00      0.33      0.50         3\n",
      "        1079       0.79      0.69      0.73        16\n",
      "        1080       0.00      0.00      0.00         2\n",
      "        1081       0.67      0.24      0.35        17\n",
      "        1082       0.00      0.00      0.00        17\n",
      "        1083       0.00      0.00      0.00         2\n",
      "        1084       0.50      0.12      0.20        16\n",
      "        1085       0.00      0.00      0.00        10\n",
      "        1086       1.00      0.07      0.12        15\n",
      "        1087       1.00      0.06      0.11        18\n",
      "        1088       1.00      0.75      0.86        16\n",
      "        1089       1.00      0.59      0.74        17\n",
      "        1090       0.00      0.00      0.00        23\n",
      "        1091       0.88      0.88      0.88         8\n",
      "        1092       0.00      0.58      0.00        12\n",
      "        1093       0.67      0.75      0.71        16\n",
      "        1094       0.00      0.00      0.00        19\n",
      "        1095       0.00      0.00      0.00         1\n",
      "        1096       0.75      0.46      0.57        13\n",
      "        1097       1.00      0.76      0.87        17\n",
      "        1098       1.00      0.70      0.82        20\n",
      "        1099       0.00      0.00      0.00         1\n",
      "        1100       0.92      0.71      0.80        17\n",
      "        1101       0.75      0.43      0.55        14\n",
      "        1102       0.86      0.43      0.57        14\n",
      "        1103       1.00      0.20      0.33        15\n",
      "        1104       0.00      0.00      0.00         4\n",
      "        1105       1.00      0.25      0.40        12\n",
      "        1106       0.00      0.00      0.00         4\n",
      "        1107       0.00      0.00      0.00         9\n",
      "        1108       1.00      0.30      0.46        20\n",
      "        1109       0.00      0.00      0.00         6\n",
      "        1110       1.00      0.19      0.32        21\n",
      "        1111       0.00      0.00      0.00         3\n",
      "        1112       1.00      0.69      0.81        16\n",
      "        1113       0.55      0.35      0.43        17\n",
      "        1114       0.62      0.28      0.38        18\n",
      "        1115       0.00      0.00      0.00         4\n",
      "        1116       0.00      0.00      0.00         5\n",
      "        1117       0.23      0.46      0.31        13\n",
      "        1118       1.00      0.37      0.54        19\n",
      "        1119       0.00      0.00      0.00         3\n",
      "        1120       0.10      0.60      0.17         5\n",
      "        1121       0.04      0.50      0.07         8\n",
      "        1122       1.00      0.47      0.64        19\n",
      "        1123       1.00      0.29      0.44         7\n",
      "        1124       0.80      0.25      0.38        16\n",
      "        1125       0.00      0.00      0.00        10\n",
      "        1126       1.00      0.32      0.48        19\n",
      "        1127       1.00      0.12      0.22        16\n",
      "        1128       1.00      0.32      0.48        22\n",
      "        1129       0.61      0.50      0.55        22\n",
      "        1130       0.22      0.29      0.25         7\n",
      "        1131       0.00      0.00      0.00         5\n",
      "        1132       1.00      0.20      0.33         5\n",
      "        1133       1.00      0.05      0.10        19\n",
      "        1134       0.00      0.00      0.00         1\n",
      "        1135       0.00      0.00      0.00        18\n",
      "        1136       0.00      0.00      0.00        13\n",
      "        1137       0.80      0.36      0.50        11\n",
      "        1138       1.00      0.29      0.44         7\n",
      "        1139       0.70      0.50      0.58        14\n",
      "        1140       0.73      0.55      0.63        20\n",
      "        1141       0.00      0.00      0.00        14\n",
      "        1142       1.00      0.24      0.38        17\n",
      "        1143       0.19      0.50      0.27         6\n",
      "        1144       0.88      0.39      0.54        18\n",
      "        1145       0.00      0.00      0.00         2\n",
      "        1146       1.00      0.08      0.15        12\n",
      "        1147       1.00      0.67      0.80        18\n",
      "        1148       0.00      0.00      0.00         2\n",
      "        1149       0.90      0.56      0.69        16\n",
      "        1150       0.00      0.00      0.00         2\n",
      "        1151       0.00      0.00      0.00         1\n",
      "        1152       0.50      0.36      0.42        14\n",
      "        1153       0.00      0.00      0.00         3\n",
      "        1154       1.00      0.14      0.25         7\n",
      "        1155       0.00      0.00      0.00         4\n",
      "        1156       1.00      0.11      0.20        18\n",
      "        1157       0.00      0.00      0.00         8\n",
      "        1158       0.92      0.60      0.73        20\n",
      "        1159       1.00      0.25      0.40        16\n",
      "        1160       1.00      0.44      0.61        16\n",
      "        1161       0.00      0.00      0.00         4\n",
      "        1162       0.00      0.00      0.00        12\n",
      "        1163       0.67      0.17      0.27        12\n",
      "        1164       1.00      0.12      0.22        16\n",
      "        1165       1.00      0.50      0.67         2\n",
      "        1166       0.00      0.00      0.00         6\n",
      "        1167       0.00      0.00      0.00         3\n",
      "        1168       1.00      0.36      0.53        14\n",
      "        1169       0.00      0.00      0.00         4\n",
      "        1170       0.50      0.20      0.29         5\n",
      "        1171       1.00      0.36      0.53        14\n",
      "        1172       1.00      0.79      0.88        24\n",
      "        1173       0.60      0.38      0.46         8\n",
      "        1174       1.00      0.44      0.62         9\n",
      "        1175       1.00      0.30      0.46        10\n",
      "        1176       1.00      0.17      0.29         6\n",
      "        1177       0.00      0.00      0.00         1\n",
      "        1178       0.22      0.60      0.32        10\n",
      "        1179       0.00      0.00      0.00         5\n",
      "        1180       0.00      0.00      0.00         9\n",
      "        1181       1.00      0.12      0.22        16\n",
      "        1182       0.00      0.00      0.00         2\n",
      "        1183       0.00      0.00      0.00        27\n",
      "        1184       0.00      0.00      0.00        21\n",
      "        1185       1.00      0.04      0.08        23\n",
      "        1186       0.83      0.33      0.48        15\n",
      "        1187       0.94      0.85      0.89        20\n",
      "        1188       1.00      0.10      0.18        10\n",
      "        1189       1.00      0.33      0.50        24\n",
      "        1190       1.00      0.30      0.47        23\n",
      "        1191       1.00      0.12      0.21        17\n",
      "        1192       1.00      0.22      0.36        18\n",
      "        1193       0.00      0.00      0.00        16\n",
      "        1194       1.00      0.10      0.18        10\n",
      "        1195       0.33      0.20      0.25         5\n",
      "        1196       0.00      0.00      0.00         9\n",
      "        1197       1.00      0.18      0.30        17\n",
      "        1198       0.67      0.59      0.62        17\n",
      "        1199       1.00      0.24      0.38        21\n",
      "        1200       1.00      0.82      0.90        11\n",
      "        1201       1.00      0.29      0.45        24\n",
      "        1202       1.00      0.29      0.44         7\n",
      "        1203       0.82      0.43      0.56        21\n",
      "        1204       1.00      0.38      0.55        16\n",
      "        1205       1.00      0.54      0.70        13\n",
      "        1206       0.25      0.14      0.18         7\n",
      "        1207       0.00      0.00      0.00         2\n",
      "        1208       0.00      0.00      0.00        16\n",
      "        1209       1.00      0.22      0.36        18\n",
      "        1210       0.71      0.42      0.53        12\n",
      "        1211       1.00      0.44      0.62         9\n",
      "        1212       1.00      0.12      0.21        17\n",
      "        1213       0.00      0.00      0.00         8\n",
      "        1214       0.67      0.38      0.48        16\n",
      "        1215       0.00      0.00      0.00         2\n",
      "        1216       0.33      0.33      0.33        12\n",
      "        1217       0.00      0.00      0.00         2\n",
      "        1218       0.57      0.50      0.53         8\n",
      "        1219       0.00      0.00      0.00         6\n",
      "        1220       0.27      0.30      0.29        10\n",
      "        1221       0.00      0.00      0.00        16\n",
      "        1222       1.00      0.04      0.08        23\n",
      "        1223       1.00      0.20      0.33         5\n",
      "        1224       0.50      0.31      0.38        16\n",
      "        1225       1.00      0.18      0.31        22\n",
      "        1226       0.92      0.46      0.62        26\n",
      "        1227       1.00      0.12      0.22         8\n",
      "        1228       1.00      0.12      0.22        16\n",
      "        1229       1.00      0.56      0.71        18\n",
      "        1230       0.89      0.44      0.59        18\n",
      "        1231       0.00      0.00      0.00         9\n",
      "        1232       1.00      0.45      0.62        11\n",
      "        1233       0.00      0.00      0.00        14\n",
      "        1234       0.93      0.59      0.72        22\n",
      "        1235       0.15      0.38      0.21        16\n",
      "        1236       0.00      0.00      0.00         7\n",
      "        1237       0.00      0.00      0.00         3\n",
      "        1238       0.17      0.20      0.18         5\n",
      "        1239       0.00      0.00      0.00         3\n",
      "        1240       0.83      0.33      0.48        15\n",
      "        1241       1.00      0.18      0.31        11\n",
      "        1242       1.00      0.46      0.63        24\n",
      "        1243       0.00      0.00      0.00         2\n",
      "        1244       1.00      0.63      0.77        19\n",
      "        1245       0.00      0.00      0.00         2\n",
      "        1246       0.92      0.60      0.73        20\n",
      "        1247       1.00      0.19      0.32        21\n",
      "        1248       0.33      0.10      0.15        10\n",
      "        1249       0.00      0.00      0.00         4\n",
      "        1250       0.00      0.00      0.00         3\n",
      "        1251       0.36      0.19      0.25        21\n",
      "        1252       0.88      0.79      0.83        19\n",
      "        1253       0.00      0.00      0.00         4\n",
      "        1254       1.00      0.29      0.44        14\n",
      "        1255       0.33      0.12      0.18         8\n",
      "        1256       0.70      0.79      0.75        24\n",
      "        1257       1.00      0.33      0.50        12\n",
      "        1258       1.00      0.27      0.43        22\n",
      "        1259       0.00      0.00      0.00         3\n",
      "        1260       0.57      0.50      0.53         8\n",
      "        1261       0.67      0.18      0.29        11\n",
      "        1262       1.00      0.64      0.78        14\n",
      "        1263       0.22      0.64      0.33        14\n",
      "        1264       0.90      0.60      0.72        15\n",
      "        1265       0.00      0.00      0.00         8\n",
      "        1266       1.00      0.45      0.62        22\n",
      "        1267       1.00      0.30      0.47        23\n",
      "        1268       1.00      0.07      0.12        15\n",
      "        1269       0.00      0.00      0.00         3\n",
      "        1270       1.00      0.17      0.29         6\n",
      "        1271       0.00      0.00      0.00         8\n",
      "        1272       0.00      0.00      0.00        12\n",
      "        1273       1.00      0.55      0.71        20\n",
      "        1274       0.57      0.31      0.40        13\n",
      "        1275       0.54      0.47      0.50        15\n",
      "        1276       1.00      0.22      0.36         9\n",
      "        1277       0.17      0.06      0.09        16\n",
      "        1278       1.00      0.04      0.08        23\n",
      "        1279       1.00      0.11      0.20         9\n",
      "        1280       1.00      0.50      0.67        10\n",
      "        1281       0.00      0.00      0.00         5\n",
      "        1282       0.00      0.00      0.00         9\n",
      "        1283       0.00      0.00      0.00        14\n",
      "        1284       0.00      0.00      0.00         3\n",
      "        1285       1.00      0.30      0.47        23\n",
      "        1286       0.86      0.38      0.52        16\n",
      "        1287       1.00      0.32      0.48        22\n",
      "        1288       0.89      0.44      0.59        18\n",
      "        1289       0.00      0.00      0.00        26\n",
      "        1290       1.00      0.25      0.40        16\n",
      "        1291       0.20      0.19      0.19        16\n",
      "        1292       0.00      0.00      0.00         3\n",
      "        1293       0.00      0.00      0.00        23\n",
      "        1294       0.00      0.00      0.00         3\n",
      "        1295       0.00      0.00      0.00        14\n",
      "        1296       1.00      0.76      0.86        21\n",
      "        1297       0.00      0.00      0.00        19\n",
      "        1298       0.36      0.64      0.46        14\n",
      "        1299       0.00      0.00      0.00         6\n",
      "        1300       1.00      0.15      0.27        13\n",
      "        1301       0.00      0.00      0.00         4\n",
      "        1302       0.50      0.05      0.09        20\n",
      "        1303       1.00      0.11      0.20         9\n",
      "        1304       0.18      0.18      0.18        11\n",
      "        1305       0.00      0.00      0.00        23\n",
      "        1306       0.62      0.40      0.48        20\n",
      "        1307       0.00      0.00      0.00         4\n",
      "        1308       0.60      0.63      0.62        19\n",
      "        1309       1.00      0.20      0.33        20\n",
      "        1310       0.00      0.00      0.00         7\n",
      "        1311       1.00      0.62      0.77        24\n",
      "        1312       0.87      0.76      0.81        17\n",
      "        1313       1.00      0.33      0.50         9\n",
      "        1314       0.00      0.00      0.00         7\n",
      "        1315       1.00      0.44      0.62        18\n",
      "        1316       0.80      0.40      0.53        10\n",
      "        1317       1.00      0.71      0.83        21\n",
      "        1318       1.00      0.46      0.63        24\n",
      "        1319       1.00      0.25      0.40         8\n",
      "        1320       1.00      0.75      0.86         4\n",
      "        1321       0.00      0.00      0.00        11\n",
      "        1322       1.00      0.43      0.60         7\n",
      "        1323       0.50      0.09      0.15        22\n",
      "        1324       1.00      0.56      0.72        16\n",
      "        1325       1.00      0.13      0.23        23\n",
      "        1326       1.00      0.50      0.67        14\n",
      "        1327       0.75      0.19      0.30        16\n",
      "        1328       0.65      0.74      0.69        23\n",
      "        1329       1.00      0.11      0.20         9\n",
      "        1330       0.00      0.00      0.00         2\n",
      "        1331       1.00      0.08      0.15        24\n",
      "        1332       1.00      0.76      0.86        21\n",
      "        1333       0.78      0.58      0.67        12\n",
      "        1334       0.00      0.00      0.00        15\n",
      "        1335       1.00      0.71      0.83        17\n",
      "        1336       1.00      0.59      0.74        22\n",
      "        1337       0.00      0.00      0.00         9\n",
      "        1338       1.00      0.43      0.61        23\n",
      "        1339       0.00      0.00      0.00         8\n",
      "        1340       1.00      0.55      0.71        11\n",
      "        1341       1.00      0.52      0.69        21\n",
      "        1342       1.00      0.50      0.67        10\n",
      "        1343       0.00      0.00      0.00         6\n",
      "        1344       1.00      0.67      0.80        24\n",
      "        1345       1.00      0.42      0.59        19\n",
      "        1346       1.00      0.42      0.59        19\n",
      "        1347       1.00      0.07      0.13        14\n",
      "        1348       0.00      0.00      0.00         1\n",
      "        1349       0.00      0.00      0.00         8\n",
      "        1350       0.33      0.40      0.36         5\n",
      "        1351       1.00      0.17      0.29        18\n",
      "        1352       1.00      0.35      0.52        17\n",
      "        1353       1.00      0.08      0.15        12\n",
      "        1354       0.00      0.00      0.00        16\n",
      "        1355       0.92      0.52      0.67        21\n",
      "        1356       0.09      0.65      0.16        20\n",
      "        1357       0.00      0.00      0.00         4\n",
      "        1358       1.00      0.62      0.77         8\n",
      "        1359       0.62      0.38      0.48        13\n",
      "        1360       1.00      0.45      0.62        20\n",
      "        1361       1.00      0.17      0.29         6\n",
      "        1362       0.00      0.00      0.00         6\n",
      "        1363       0.65      0.87      0.74        15\n",
      "        1364       0.00      0.00      0.00         5\n",
      "        1365       0.00      0.00      0.00         1\n",
      "        1366       0.50      0.67      0.57        18\n",
      "        1367       0.89      0.47      0.62        17\n",
      "        1368       0.26      0.40      0.32        15\n",
      "        1369       0.94      0.77      0.85        22\n",
      "        1370       0.00      0.00      0.00        16\n",
      "        1371       0.29      0.44      0.35         9\n",
      "        1372       0.06      0.65      0.10        20\n",
      "        1373       0.80      0.44      0.57         9\n",
      "        1374       1.00      0.25      0.40        20\n",
      "        1375       1.00      0.20      0.33         5\n",
      "        1376       1.00      0.53      0.70        15\n",
      "        1377       0.00      0.00      0.00        10\n",
      "        1378       0.69      0.45      0.55        20\n",
      "        1379       0.00      0.00      0.00         5\n",
      "        1380       1.00      0.39      0.56        18\n",
      "        1381       1.00      0.29      0.44        14\n",
      "        1382       0.60      0.18      0.27        17\n",
      "        1383       0.00      0.00      0.00        19\n",
      "        1384       1.00      0.05      0.09        22\n",
      "        1385       1.00      0.60      0.75         5\n",
      "        1386       1.00      0.44      0.62        18\n",
      "        1387       1.00      0.48      0.65        21\n",
      "        1388       0.00      0.00      0.00         3\n",
      "        1389       0.00      0.00      0.00         7\n",
      "        1390       0.20      0.25      0.22         4\n",
      "        1391       0.65      0.68      0.67        19\n",
      "        1392       1.00      0.78      0.88        23\n",
      "        1393       0.00      0.00      0.00         7\n",
      "        1394       1.00      0.53      0.69        17\n",
      "        1395       0.00      0.00      0.00         2\n",
      "        1396       0.60      0.27      0.37        11\n",
      "        1397       0.00      0.00      0.00         3\n",
      "        1398       0.00      0.00      0.00         5\n",
      "        1399       1.00      0.37      0.54        19\n",
      "        1400       1.00      0.44      0.61        16\n",
      "        1401       1.00      0.12      0.22         8\n",
      "        1402       0.00      0.43      0.00        21\n",
      "        1403       0.00      0.00      0.00         5\n",
      "        1404       0.00      0.00      0.00         2\n",
      "        1405       0.00      0.00      0.00         6\n",
      "        1406       1.00      0.24      0.38        17\n",
      "        1407       0.88      0.41      0.56        17\n",
      "        1408       1.00      0.11      0.19        19\n",
      "        1409       1.00      0.40      0.57        10\n",
      "        1410       0.67      0.14      0.24        14\n",
      "        1411       1.00      0.14      0.25        21\n",
      "        1412       0.07      0.23      0.11        13\n",
      "        1414       0.88      0.30      0.45        23\n",
      "        1415       0.00      0.00      0.00        25\n",
      "        1416       0.01      0.07      0.02        14\n",
      "        1417       1.00      0.57      0.73        21\n",
      "        1418       0.00      0.00      0.00         8\n",
      "        1419       0.00      0.00      0.00        23\n",
      "        1420       0.52      0.67      0.58        21\n",
      "        1421       1.00      0.56      0.71         9\n",
      "        1422       1.00      0.19      0.32        21\n",
      "        1423       1.00      0.24      0.38        17\n",
      "        1424       1.00      0.60      0.75        25\n",
      "        1425       0.00      0.00      0.00        20\n",
      "        1426       0.00      0.00      0.00         1\n",
      "        1427       0.33      0.10      0.15        10\n",
      "        1428       1.00      0.40      0.57        10\n",
      "        1429       1.00      0.08      0.15        12\n",
      "        1430       0.00      0.00      0.00         7\n",
      "        1431       0.00      0.00      0.00        10\n",
      "        1432       0.00      0.00      0.00         4\n",
      "        1433       0.60      0.38      0.46        16\n",
      "        1434       0.00      0.00      0.00         3\n",
      "        1435       0.62      0.56      0.59         9\n",
      "        1436       0.36      0.47      0.41        17\n",
      "        1437       1.00      0.78      0.88        18\n",
      "        1438       0.00      0.00      0.00         4\n",
      "        1439       0.00      0.00      0.00         2\n",
      "        1440       0.00      0.00      0.00        10\n",
      "        1441       0.00      0.00      0.00         2\n",
      "        1442       0.00      0.00      0.00         6\n",
      "        1443       1.00      0.06      0.11        18\n",
      "        1444       1.00      0.35      0.52        20\n",
      "        1445       1.00      0.08      0.15        12\n",
      "        1446       0.87      0.76      0.81        17\n",
      "        1447       0.00      0.00      0.00         8\n",
      "        1448       0.00      0.00      0.00        10\n",
      "        1449       0.00      0.00      0.00        16\n",
      "        1450       1.00      0.09      0.17        22\n",
      "        1451       0.60      0.75      0.67         4\n",
      "        1452       1.00      0.65      0.79        23\n",
      "        1453       0.86      0.43      0.57        14\n",
      "        1454       1.00      0.14      0.25         7\n",
      "        1455       0.71      0.28      0.40        18\n",
      "        1456       1.00      0.12      0.22         8\n",
      "        1457       1.00      0.29      0.44        14\n",
      "        1458       1.00      0.38      0.55         8\n",
      "        1459       0.00      0.00      0.00        14\n",
      "        1460       0.00      0.00      0.00         4\n",
      "        1461       1.00      0.04      0.08        23\n",
      "        1462       1.00      0.22      0.36         9\n",
      "        1463       1.00      0.27      0.43        11\n",
      "        1464       1.00      0.14      0.25         7\n",
      "        1465       0.00      0.00      0.00         2\n",
      "        1466       0.00      0.00      0.00         5\n",
      "        1467       1.00      0.45      0.62        11\n",
      "        1468       0.00      0.00      0.00         2\n",
      "        1469       0.00      0.00      0.00         2\n",
      "        1470       0.00      0.00      0.00         4\n",
      "        1471       1.00      0.17      0.29         6\n",
      "        1472       1.00      0.59      0.74        17\n",
      "        1473       1.00      0.09      0.17        11\n",
      "        1474       1.00      0.17      0.29        18\n",
      "        1475       0.00      0.00      0.00         2\n",
      "        1476       0.00      0.00      0.00         4\n",
      "        1477       1.00      0.56      0.71         9\n",
      "        1478       1.00      0.47      0.64        17\n",
      "        1479       1.00      0.43      0.60        21\n",
      "        1480       0.50      0.33      0.40         6\n",
      "        1481       0.86      0.55      0.67        11\n",
      "        1482       0.46      0.71      0.56        17\n",
      "        1483       0.00      0.00      0.00         2\n",
      "        1484       0.50      0.50      0.50         2\n",
      "        1485       0.00      0.00      0.00         3\n",
      "        1486       0.91      0.83      0.87        12\n",
      "        1487       0.00      0.00      0.00         6\n",
      "        1488       0.40      0.14      0.21        14\n",
      "        1489       1.00      0.22      0.36         9\n",
      "        1490       1.00      0.26      0.42        19\n",
      "        1491       1.00      0.38      0.56        13\n",
      "        1492       0.59      0.72      0.65        18\n",
      "        1493       1.00      0.18      0.31        22\n",
      "        1494       0.00      0.00      0.00        18\n",
      "        1495       0.56      0.56      0.56        18\n",
      "        1496       1.00      0.09      0.17        11\n",
      "        1497       0.00      0.00      0.00         3\n",
      "        1498       1.00      0.14      0.25        14\n",
      "        1499       0.47      0.67      0.55        12\n",
      "        1500       1.00      0.57      0.73         7\n",
      "        1501       1.00      0.59      0.74        22\n",
      "        1502       0.00      0.00      0.00         1\n",
      "        1503       1.00      0.23      0.37        22\n",
      "        1504       0.00      0.00      0.00         3\n",
      "        1505       1.00      0.20      0.33         5\n",
      "        1506       1.00      0.55      0.71        20\n",
      "        1507       1.00      0.12      0.22         8\n",
      "        1508       0.47      0.44      0.46        18\n",
      "        1509       0.71      0.77      0.74        13\n",
      "        1510       0.92      0.57      0.71        21\n",
      "        1511       1.00      0.54      0.70        26\n",
      "        1512       1.00      0.42      0.59        19\n",
      "        1513       0.00      0.00      0.00         4\n",
      "        1514       1.00      0.14      0.25        14\n",
      "        1515       1.00      0.42      0.59        19\n",
      "        1516       1.00      0.36      0.53        22\n",
      "        1517       0.00      0.00      0.00        19\n",
      "        1518       1.00      0.17      0.29         6\n",
      "        1519       0.00      0.00      0.00        12\n",
      "        1520       1.00      0.29      0.44        21\n",
      "        1521       0.00      0.00      0.00         4\n",
      "        1522       1.00      0.08      0.14        13\n",
      "        1523       0.00      0.00      0.00         3\n",
      "        1524       0.00      0.00      0.00         6\n",
      "        1525       0.00      0.00      0.00         2\n",
      "        1526       1.00      0.55      0.71        20\n",
      "        1527       0.80      0.20      0.32        20\n",
      "        1528       1.00      0.43      0.60        14\n",
      "        1529       0.24      0.57      0.34        21\n",
      "        1530       1.00      0.10      0.18        20\n",
      "        1531       0.19      0.55      0.28        11\n",
      "        1532       0.60      0.50      0.55         6\n",
      "        1533       0.00      0.00      0.00         8\n",
      "        1534       0.00      0.00      0.00        21\n",
      "        1535       1.00      0.42      0.59        19\n",
      "        1536       0.67      0.12      0.20        17\n",
      "        1537       0.86      0.83      0.84        23\n",
      "        1538       1.00      0.19      0.32        21\n",
      "        1539       1.00      0.18      0.31        11\n",
      "        1540       1.00      0.08      0.15        25\n",
      "        1541       0.91      0.91      0.91        22\n",
      "        1542       1.00      0.08      0.14        13\n",
      "        1543       0.00      0.00      0.00         1\n",
      "        1544       0.00      0.00      0.00         2\n",
      "        1545       1.00      0.32      0.48        22\n",
      "        1546       0.00      0.00      0.00        11\n",
      "        1547       0.00      0.00      0.00        19\n",
      "        1548       0.00      0.00      0.00         4\n",
      "        1549       1.00      0.50      0.67        10\n",
      "        1550       0.00      0.00      0.00         2\n",
      "        1551       0.00      0.00      0.00        13\n",
      "        1552       0.90      0.64      0.75        14\n",
      "        1553       1.00      0.14      0.25        14\n",
      "        1554       1.00      0.11      0.19        19\n",
      "        1555       1.00      0.25      0.40         4\n",
      "        1556       1.00      0.53      0.69        19\n",
      "        1557       1.00      0.29      0.45        17\n",
      "        1558       1.00      0.47      0.64        19\n",
      "        1559       0.00      0.00      0.00        14\n",
      "        1560       1.00      0.44      0.62         9\n",
      "        1561       1.00      0.12      0.21        17\n",
      "        1562       0.74      0.74      0.74        19\n",
      "        1563       0.00      0.00      0.00        20\n",
      "        1564       0.00      0.00      0.00         3\n",
      "        1565       1.00      0.60      0.75         5\n",
      "        1566       1.00      0.14      0.24        22\n",
      "        1567       0.22      0.47      0.30        17\n",
      "        1568       1.00      0.09      0.17        11\n",
      "        1569       0.67      0.57      0.62         7\n",
      "        1570       1.00      0.46      0.63        24\n",
      "        1571       1.00      0.46      0.63        13\n",
      "        1572       0.00      0.00      0.00         7\n",
      "        1573       0.33      0.06      0.10        17\n",
      "        1574       0.75      0.27      0.40        22\n",
      "        1575       0.71      0.38      0.50        13\n",
      "        1576       0.50      0.12      0.20         8\n",
      "        1577       1.00      0.20      0.33        10\n",
      "        1578       1.00      0.11      0.20        18\n",
      "        1579       0.00      0.00      0.00        23\n",
      "        1580       1.00      0.53      0.69        19\n",
      "        1581       0.00      0.54      0.00        13\n",
      "        1582       1.00      0.27      0.43        11\n",
      "        1583       0.00      0.00      0.00         7\n",
      "        1584       0.17      0.50      0.25        10\n",
      "        1585       0.00      0.00      0.00        20\n",
      "        1586       0.00      0.00      0.00         9\n",
      "        1587       1.00      0.08      0.14        13\n",
      "        1588       0.78      0.54      0.64        13\n",
      "        1589       1.00      0.09      0.17        22\n",
      "        1590       0.00      0.00      0.00        21\n",
      "        1591       0.83      0.43      0.57        23\n",
      "        1592       1.00      0.17      0.29        12\n",
      "        1593       0.70      0.30      0.42        23\n",
      "        1594       0.40      0.29      0.33         7\n",
      "        1595       0.00      0.00      0.00         8\n",
      "        1596       0.17      0.22      0.19         9\n",
      "        1597       1.00      0.09      0.17        11\n",
      "        1598       1.00      0.20      0.33         5\n",
      "        1599       1.00      0.19      0.32        16\n",
      "        1600       0.00      0.00      0.00        17\n",
      "        1601       1.00      0.12      0.22         8\n",
      "        1602       1.00      0.17      0.29        18\n",
      "        1603       0.78      0.41      0.54        17\n",
      "        1604       1.00      0.47      0.64        17\n",
      "        1605       0.50      0.50      0.50         2\n",
      "        1606       1.00      0.11      0.20         9\n",
      "        1607       1.00      0.33      0.50        12\n",
      "        1608       1.00      0.12      0.21        17\n",
      "        1609       0.86      0.67      0.75        18\n",
      "        1610       1.00      0.47      0.64        17\n",
      "        1611       0.00      0.00      0.00         2\n",
      "        1612       0.00      0.00      0.00         8\n",
      "        1613       1.00      0.56      0.71         9\n",
      "        1614       1.00      0.33      0.50         6\n",
      "        1615       0.00      0.00      0.00        10\n",
      "        1616       0.00      0.00      0.00         2\n",
      "        1617       0.70      0.33      0.45        21\n",
      "        1618       0.00      0.00      0.00        11\n",
      "        1619       1.00      0.20      0.33        10\n",
      "        1620       0.00      0.00      0.00         3\n",
      "        1621       0.83      0.62      0.71        16\n",
      "        1622       1.00      0.46      0.63        13\n",
      "        1623       1.00      0.17      0.29         6\n",
      "        1624       1.00      0.50      0.67        22\n",
      "        1625       1.00      0.50      0.67        14\n",
      "        1626       0.89      0.47      0.62        17\n",
      "        1627       0.57      0.50      0.53         8\n",
      "        1628       0.00      0.00      0.00         5\n",
      "        1629       1.00      0.12      0.22         8\n",
      "        1630       1.00      0.33      0.50        15\n",
      "        1631       0.28      0.64      0.39        11\n",
      "        1632       1.00      0.45      0.62        20\n",
      "        1633       1.00      0.15      0.27        13\n",
      "        1634       0.50      0.50      0.50         2\n",
      "        1635       0.00      0.00      0.00         7\n",
      "        1636       0.00      0.00      0.00        15\n",
      "        1637       1.00      0.19      0.32        16\n",
      "        1638       0.00      0.00      0.00        14\n",
      "        1639       1.00      0.33      0.50         9\n",
      "        1640       0.71      0.83      0.77        12\n",
      "        1641       1.00      0.64      0.78        22\n",
      "        1642       1.00      0.62      0.76        21\n",
      "        1643       0.00      0.00      0.00         9\n",
      "        1644       0.00      0.00      0.00         4\n",
      "        1645       0.00      0.00      0.00        14\n",
      "        1646       1.00      0.44      0.61        16\n",
      "        1647       0.82      0.38      0.51        24\n",
      "        1648       0.88      0.50      0.64        14\n",
      "        1649       1.00      0.25      0.40         8\n",
      "        1650       0.00      0.00      0.00        12\n",
      "        1651       1.00      0.23      0.37        22\n",
      "        1652       1.00      0.48      0.65        21\n",
      "        1653       0.00      0.00      0.00         6\n",
      "        1654       1.00      0.07      0.13        14\n",
      "        1655       1.00      0.08      0.15        24\n",
      "        1656       1.00      0.06      0.12        16\n",
      "        1657       0.85      0.58      0.69        19\n",
      "        1658       1.00      0.22      0.36         9\n",
      "        1659       1.00      0.38      0.55        16\n",
      "        1660       1.00      0.29      0.44        14\n",
      "        1661       1.00      0.25      0.40        24\n",
      "        1662       0.00      0.00      0.00         3\n",
      "        1663       1.00      0.38      0.55         8\n",
      "        1664       0.80      0.36      0.50        11\n",
      "        1665       1.00      0.17      0.29         6\n",
      "        1666       0.00      0.00      0.00         2\n",
      "        1667       0.00      0.00      0.00         3\n",
      "        1668       0.00      0.00      0.00        16\n",
      "        1669       0.00      0.00      0.00        10\n",
      "        1670       0.00      0.00      0.00         2\n",
      "        1671       1.00      0.47      0.64        19\n",
      "        1672       1.00      0.67      0.80         3\n",
      "        1673       0.00      0.00      0.00         6\n",
      "        1674       1.00      0.62      0.77        16\n",
      "        1675       0.00      0.00      0.00         8\n",
      "        1676       0.00      0.00      0.00        14\n",
      "        1677       0.00      0.00      0.00         9\n",
      "        1678       0.00      0.00      0.00         3\n",
      "        1679       1.00      0.29      0.44        21\n",
      "        1680       1.00      0.33      0.50        15\n",
      "        1681       0.32      0.62      0.43        16\n",
      "        1682       1.00      0.36      0.53        11\n",
      "        1683       1.00      0.09      0.17        11\n",
      "        1684       0.43      0.46      0.44        13\n",
      "        1685       0.80      0.35      0.48        23\n",
      "        1686       1.00      0.09      0.16        23\n",
      "        1687       0.00      0.00      0.00        13\n",
      "        1688       0.00      0.00      0.00         4\n",
      "        1689       0.71      0.26      0.38        19\n",
      "        1690       0.89      0.57      0.70        14\n",
      "        1691       0.00      0.00      0.00        18\n",
      "        1692       1.00      0.12      0.22         8\n",
      "        1693       1.00      0.06      0.11        18\n",
      "        1694       1.00      0.09      0.17        11\n",
      "        1695       1.00      0.12      0.21        17\n",
      "        1696       0.00      0.00      0.00        15\n",
      "        1697       0.00      0.00      0.00         2\n",
      "        1698       0.00      0.00      0.00        10\n",
      "        1699       0.00      0.00      0.00         4\n",
      "        1700       1.00      0.27      0.42        15\n",
      "        1701       0.00      0.00      0.00         3\n",
      "        1702       1.00      0.12      0.22         8\n",
      "        1703       0.00      0.00      0.00         5\n",
      "        1704       1.00      0.53      0.69        17\n",
      "        1705       0.00      0.00      0.00         2\n",
      "        1706       0.00      0.00      0.00         3\n",
      "        1707       1.00      0.30      0.46        10\n",
      "        1708       0.33      0.27      0.30        15\n",
      "        1709       0.31      0.45      0.37        11\n",
      "        1710       0.00      0.00      0.00         2\n",
      "        1711       1.00      0.26      0.41        23\n",
      "        1712       1.00      0.54      0.70        13\n",
      "        1713       0.80      0.35      0.48        23\n",
      "        1714       0.86      0.55      0.67        22\n",
      "        1715       1.00      0.67      0.80        15\n",
      "        1716       1.00      0.41      0.58        22\n",
      "        1717       0.00      0.00      0.00         4\n",
      "        1718       0.86      0.43      0.57        14\n",
      "        1719       0.00      0.00      0.00         1\n",
      "        1720       1.00      0.40      0.57         5\n",
      "        1721       1.00      0.79      0.88        19\n",
      "        1722       1.00      0.42      0.59        19\n",
      "        1723       1.00      0.08      0.14        13\n",
      "        1724       1.00      0.55      0.71        11\n",
      "        1725       0.14      0.61      0.23        23\n",
      "        1726       1.00      0.32      0.48        22\n",
      "        1727       0.00      0.00      0.00         3\n",
      "        1728       1.00      0.12      0.22        16\n",
      "        1729       1.00      0.38      0.55         8\n",
      "        1730       0.00      0.00      0.00         7\n",
      "        1731       0.00      0.00      0.00        14\n",
      "        1732       0.57      0.53      0.55        15\n",
      "        1733       0.92      0.58      0.71        19\n",
      "        1734       1.00      0.35      0.52        23\n",
      "        1735       0.86      0.67      0.75         9\n",
      "        1736       1.00      0.12      0.22         8\n",
      "        1737       0.57      0.25      0.35        16\n",
      "        1738       0.00      0.00      0.00         1\n",
      "        1739       0.88      0.47      0.61        15\n",
      "        1740       0.00      0.00      0.00         1\n",
      "        1741       1.00      0.33      0.50        12\n",
      "        1742       0.00      0.00      0.00        20\n",
      "        1743       1.00      0.17      0.30        23\n",
      "        1744       1.00      0.67      0.80        21\n",
      "        1745       0.50      0.33      0.40         6\n",
      "        1746       0.25      0.14      0.18        14\n",
      "        1747       0.73      0.55      0.63        20\n",
      "        1748       0.13      0.18      0.15        11\n",
      "        1749       0.00      0.00      0.00         6\n",
      "        1750       0.00      0.00      0.00         2\n",
      "        1751       1.00      0.27      0.43        22\n",
      "        1752       1.00      0.40      0.57        10\n",
      "        1753       0.00      0.00      0.00         4\n",
      "        1754       1.00      0.60      0.75        20\n",
      "        1755       1.00      0.17      0.30        23\n",
      "        1756       1.00      0.05      0.09        21\n",
      "        1757       1.00      0.47      0.64        19\n",
      "        1758       0.00      0.00      0.00         8\n",
      "        1759       0.00      0.00      0.00        12\n",
      "        1760       1.00      0.35      0.52        17\n",
      "        1761       0.33      0.50      0.40         6\n",
      "        1762       0.78      0.44      0.56        16\n",
      "        1763       1.00      0.20      0.33        10\n",
      "        1764       0.36      0.60      0.45        20\n",
      "        1765       0.00      0.00      0.00         4\n",
      "        1766       1.00      0.44      0.62         9\n",
      "        1767       0.67      0.25      0.36        16\n",
      "        1768       1.00      0.14      0.25         7\n",
      "        1769       1.00      0.05      0.09        21\n",
      "        1770       0.00      0.00      0.00        17\n",
      "        1771       1.00      0.50      0.67        16\n",
      "        1772       1.00      0.60      0.75        20\n",
      "        1773       1.00      0.30      0.46        20\n",
      "        1774       0.88      0.54      0.67        13\n",
      "        1775       0.08      0.08      0.08        12\n",
      "        1776       0.86      0.57      0.69        21\n",
      "        1777       1.00      0.22      0.36         9\n",
      "        1778       1.00      0.14      0.25        21\n",
      "        1779       1.00      0.55      0.71        22\n",
      "        1780       0.76      0.73      0.74        22\n",
      "        1781       1.00      0.42      0.59        19\n",
      "        1782       1.00      0.17      0.30        23\n",
      "        1783       0.91      0.42      0.57        24\n",
      "        1784       0.17      0.33      0.22         6\n",
      "        1785       0.33      0.20      0.25         5\n",
      "        1786       1.00      0.60      0.75        20\n",
      "        1787       0.80      0.25      0.38        16\n",
      "        1788       1.00      0.22      0.36        23\n",
      "        1789       0.00      0.00      0.00         2\n",
      "        1790       0.11      0.25      0.15        12\n",
      "        1791       0.00      0.00      0.00        11\n",
      "        1792       0.83      0.50      0.62        20\n",
      "        1793       0.00      0.00      0.00         2\n",
      "        1794       0.00      0.00      0.00        23\n",
      "        1795       0.12      0.07      0.09        14\n",
      "        1796       0.90      0.53      0.67        17\n",
      "        1797       0.00      0.00      0.00         2\n",
      "        1798       1.00      0.41      0.58        22\n",
      "        1799       0.00      0.00      0.00         8\n",
      "        1800       1.00      0.14      0.24        22\n",
      "        1801       1.00      0.14      0.24        22\n",
      "        1802       0.00      0.00      0.00         1\n",
      "        1803       0.00      0.00      0.00        14\n",
      "        1804       1.00      0.11      0.20         9\n",
      "        1805       0.59      0.77      0.67        13\n",
      "        1806       1.00      0.70      0.82        20\n",
      "        1807       0.00      0.00      0.00        13\n",
      "        1808       1.00      0.17      0.29         6\n",
      "        1809       1.00      0.25      0.40        20\n",
      "        1810       0.00      0.00      0.00        17\n",
      "        1811       0.83      0.31      0.45        16\n",
      "        1812       1.00      0.44      0.62         9\n",
      "        1813       0.00      0.00      0.00        10\n",
      "        1814       1.00      0.55      0.71        20\n",
      "        1815       0.00      0.00      0.00         6\n",
      "        1816       1.00      0.25      0.40        20\n",
      "        1817       1.00      0.04      0.08        25\n",
      "        1818       0.75      0.69      0.72        13\n",
      "        1819       1.00      0.14      0.25         7\n",
      "        1820       0.11      0.43      0.17         7\n",
      "        1821       0.50      0.25      0.33         8\n",
      "        1822       1.00      0.54      0.70        13\n",
      "        1823       0.75      0.64      0.69        14\n",
      "        1824       0.00      0.00      0.00         9\n",
      "        1825       0.83      0.25      0.38        20\n",
      "        1826       1.00      0.22      0.36        18\n",
      "        1827       0.20      0.14      0.17         7\n",
      "        1828       0.00      0.00      0.00         7\n",
      "        1829       0.54      0.44      0.48        16\n",
      "        1830       0.00      0.00      0.00         6\n",
      "        1831       1.00      0.23      0.37        22\n",
      "        1832       0.00      0.00      0.00         6\n",
      "        1833       1.00      0.68      0.81        19\n",
      "        1834       0.00      0.00      0.00         6\n",
      "        1835       0.00      0.00      0.00         4\n",
      "        1836       0.47      0.50      0.48        14\n",
      "        1837       1.00      0.29      0.44        14\n",
      "        1838       1.00      0.57      0.72        23\n",
      "        1839       0.00      0.00      0.00         6\n",
      "        1840       0.00      0.00      0.00         3\n",
      "        1841       0.00      0.00      0.00         1\n",
      "        1842       1.00      0.14      0.25        21\n",
      "        1843       0.00      0.00      0.00         5\n",
      "        1844       0.80      0.73      0.76        11\n",
      "        1845       1.00      0.50      0.67        14\n",
      "        1846       1.00      0.29      0.44        21\n",
      "        1847       0.00      0.00      0.00         9\n",
      "        1848       1.00      0.32      0.48        19\n",
      "        1849       0.00      0.00      0.00        16\n",
      "        1850       0.00      0.00      0.00        18\n",
      "        1851       1.00      0.76      0.87        17\n",
      "        1852       0.00      0.00      0.00         3\n",
      "        1853       1.00      0.57      0.73        14\n",
      "        1854       0.00      0.00      0.00         3\n",
      "        1855       0.88      0.44      0.58        16\n",
      "        1856       0.62      0.29      0.40        17\n",
      "        1857       0.00      0.00      0.00         6\n",
      "        1858       0.73      0.62      0.67        13\n",
      "        1859       0.80      0.20      0.32        20\n",
      "        1860       0.00      0.00      0.00        21\n",
      "        1861       0.00      0.00      0.00         5\n",
      "        1862       1.00      0.38      0.55        16\n",
      "        1863       1.00      0.13      0.23        23\n",
      "        1864       0.00      0.00      0.00         5\n",
      "        1865       1.00      0.48      0.65        23\n",
      "        1866       0.71      0.24      0.36        21\n",
      "        1867       1.00      0.21      0.35        19\n",
      "        1868       1.00      0.57      0.73         7\n",
      "        1869       1.00      0.33      0.50        12\n",
      "        1870       1.00      0.17      0.29        18\n",
      "        1871       0.00      0.00      0.00         4\n",
      "        1872       0.00      0.00      0.00         6\n",
      "        1873       0.00      0.00      0.00         5\n",
      "        1874       1.00      0.11      0.20        18\n",
      "        1875       0.25      0.25      0.25         4\n",
      "        1876       0.00      0.00      0.00         5\n",
      "        1877       1.00      0.50      0.67        16\n",
      "        1878       0.67      0.32      0.43        19\n",
      "        1879       1.00      0.71      0.83        21\n",
      "        1880       0.00      0.00      0.00         4\n",
      "        1881       0.04      0.08      0.06        13\n",
      "        1882       1.00      0.18      0.31        11\n",
      "        1883       0.89      0.81      0.85        21\n",
      "        1884       1.00      0.12      0.22         8\n",
      "        1885       1.00      0.43      0.60        14\n",
      "        1886       1.00      0.25      0.40        16\n",
      "        1887       1.00      0.24      0.38        17\n",
      "        1888       0.80      0.36      0.50        11\n",
      "        1889       0.69      0.69      0.69        13\n",
      "        1890       0.07      0.08      0.07        13\n",
      "        1891       0.00      0.00      0.00         7\n",
      "        1892       1.00      0.33      0.50         9\n",
      "        1893       1.00      0.20      0.33        10\n",
      "        1894       0.94      0.83      0.88        18\n",
      "        1895       1.00      0.38      0.55        24\n",
      "        1896       1.00      0.25      0.40        12\n",
      "        1897       0.00      0.00      0.00        37\n",
      "        1898       1.00      0.69      0.82        13\n",
      "        1899       0.80      0.38      0.52        21\n",
      "        1900       0.00      0.00      0.00         7\n",
      "        1901       1.00      0.12      0.21        25\n",
      "        1902       1.00      0.27      0.43        11\n",
      "        1903       1.00      0.14      0.25        14\n",
      "        1904       0.00      0.00      0.00         6\n",
      "        1905       0.00      0.00      0.00         4\n",
      "        1906       0.00      0.00      0.00         6\n",
      "        1907       1.00      0.25      0.40         4\n",
      "        1908       1.00      0.65      0.79        20\n",
      "        1909       0.00      0.00      0.00         4\n",
      "        1910       0.00      0.00      0.00         9\n",
      "        1911       0.00      0.00      0.00        18\n",
      "        1912       0.67      0.75      0.71         8\n",
      "        1913       0.00      0.00      0.00        12\n",
      "        1914       1.00      0.57      0.73        14\n",
      "        1915       1.00      0.46      0.63        13\n",
      "        1916       0.80      0.17      0.29        23\n",
      "        1917       0.00      0.00      0.00        16\n",
      "        1918       0.00      0.00      0.00         2\n",
      "        1919       0.18      0.27      0.22        15\n",
      "        1920       1.00      0.18      0.30        17\n",
      "        1921       0.00      0.00      0.00         3\n",
      "        1922       1.00      0.33      0.50         3\n",
      "        1923       0.67      0.17      0.27        12\n",
      "        1924       1.00      0.09      0.17        11\n",
      "        1925       1.00      0.45      0.62        11\n",
      "        1926       0.00      0.00      0.00        16\n",
      "        1927       0.50      0.33      0.40         3\n",
      "        1928       1.00      0.25      0.40         4\n",
      "        1929       0.00      0.00      0.00         2\n",
      "        1930       0.43      0.30      0.35        10\n",
      "        1931       0.00      0.00      0.00         4\n",
      "        1932       1.00      0.11      0.20         9\n",
      "        1933       0.00      0.00      0.00         6\n",
      "        1934       0.00      0.00      0.00         6\n",
      "        1935       1.00      0.38      0.55        21\n",
      "        1936       1.00      0.83      0.91        18\n",
      "        1937       1.00      0.33      0.50        15\n",
      "        1938       0.00      0.00      0.00         1\n",
      "        1939       1.00      0.08      0.15        12\n",
      "        1940       1.00      0.06      0.12        16\n",
      "        1941       0.00      0.00      0.00        12\n",
      "        1942       1.00      0.38      0.55        21\n",
      "        1943       0.00      0.00      0.00         2\n",
      "        1944       1.00      0.17      0.29        18\n",
      "        1945       0.33      0.14      0.20         7\n",
      "        1946       1.00      0.10      0.18        10\n",
      "        1947       0.91      0.56      0.69        18\n",
      "        1948       0.00      0.00      0.00         3\n",
      "        1949       1.00      0.39      0.56        23\n",
      "        1950       0.00      0.00      0.00         1\n",
      "        1951       0.91      0.38      0.54        26\n",
      "        1952       1.00      0.32      0.48        19\n",
      "        1953       0.00      0.00      0.00        18\n",
      "        1954       1.00      0.56      0.71         9\n",
      "        1955       1.00      0.28      0.43        18\n",
      "        1956       0.00      0.00      0.00        18\n",
      "        1957       1.00      0.05      0.10        20\n",
      "        1958       1.00      0.65      0.79        17\n",
      "        1959       1.00      0.08      0.15        24\n",
      "        1960       0.43      0.38      0.40         8\n",
      "        1961       1.00      0.53      0.69        17\n",
      "        1962       1.00      0.33      0.50        12\n",
      "        1963       0.80      0.42      0.55        19\n",
      "        1964       1.00      0.50      0.67         8\n",
      "        1965       1.00      0.10      0.18        20\n",
      "        1966       0.00      0.00      0.00         4\n",
      "        1967       1.00      0.16      0.27        19\n",
      "        1968       0.00      0.00      0.00        12\n",
      "        1969       1.00      0.23      0.38        13\n",
      "        1970       0.00      0.00      0.00         3\n",
      "        1971       1.00      0.30      0.46        10\n",
      "        1972       1.00      0.10      0.18        10\n",
      "        1973       1.00      0.38      0.55        21\n",
      "        1974       0.00      0.00      0.00         1\n",
      "        1975       1.00      0.41      0.58        22\n",
      "        1976       0.00      0.00      0.00         5\n",
      "        1977       1.00      0.39      0.56        23\n",
      "        1978       0.62      0.62      0.62         8\n",
      "        1979       1.00      0.38      0.55        16\n",
      "        1980       0.00      0.00      0.00         5\n",
      "        1981       0.00      0.00      0.00         1\n",
      "        1982       1.00      0.24      0.38        21\n",
      "        1983       0.11      0.17      0.13         6\n",
      "        1984       0.00      0.00      0.00         5\n",
      "        1985       1.00      0.88      0.94        17\n",
      "        1986       0.00      0.00      0.00        10\n",
      "        1987       1.00      0.08      0.14        13\n",
      "        1988       0.00      0.00      0.00         4\n",
      "        1989       0.60      0.17      0.26        18\n",
      "        1990       0.00      0.00      0.00        11\n",
      "        1991       1.00      0.50      0.67         2\n",
      "        1992       0.00      0.00      0.00        12\n",
      "        1993       1.00      0.11      0.20         9\n",
      "        1994       0.00      0.00      0.00         6\n",
      "        1995       1.00      0.07      0.12        15\n",
      "        1996       1.00      0.37      0.54        19\n",
      "        1997       0.00      0.00      0.00         8\n",
      "        1998       1.00      0.67      0.80         6\n",
      "        1999       1.00      0.15      0.26        20\n",
      "        2000       1.00      0.36      0.53        11\n",
      "        2001       0.00      0.00      0.00         3\n",
      "        2002       1.00      0.27      0.42        26\n",
      "        2003       1.00      0.11      0.20        18\n",
      "        2004       0.00      0.00      0.00        14\n",
      "        2005       1.00      0.71      0.83        21\n",
      "        2006       1.00      0.05      0.10        19\n",
      "        2007       1.00      0.61      0.76        18\n",
      "        2008       1.00      0.16      0.27        19\n",
      "        2009       0.00      0.00      0.00        11\n",
      "        2010       1.00      0.22      0.36        18\n",
      "        2011       1.00      0.11      0.20         9\n",
      "        2012       1.00      0.59      0.74        17\n",
      "        2013       1.00      0.21      0.35        19\n",
      "        2014       0.00      0.00      0.00         3\n",
      "        2015       0.00      0.00      0.00        15\n",
      "        2016       0.00      0.00      0.00         6\n",
      "        2017       0.50      0.17      0.25         6\n",
      "        2018       1.00      0.20      0.33         5\n",
      "        2019       0.00      0.00      0.00        18\n",
      "        2020       0.40      0.50      0.44         4\n",
      "        2021       0.00      0.00      0.00         2\n",
      "        2022       0.00      0.00      0.00         4\n",
      "        2023       0.50      0.08      0.14        12\n",
      "        2024       0.53      0.60      0.56        15\n",
      "        2025       0.25      0.17      0.20        12\n",
      "        2026       1.00      0.33      0.50        15\n",
      "        2027       1.00      0.25      0.40        20\n",
      "        2028       0.82      0.88      0.85        16\n",
      "        2029       1.00      0.40      0.57        15\n",
      "        2030       0.50      0.09      0.15        11\n",
      "        2031       1.00      0.25      0.40        16\n",
      "        2032       1.00      0.14      0.25        21\n",
      "        2033       0.00      0.00      0.00         7\n",
      "        2034       0.00      0.00      0.00         3\n",
      "        2035       0.40      0.33      0.36         6\n",
      "        2036       1.00      0.44      0.62         9\n",
      "        2037       1.00      0.46      0.63        13\n",
      "        2038       1.00      0.12      0.22         8\n",
      "        2039       1.00      0.40      0.57        15\n",
      "        2040       0.00      0.00      0.00         9\n",
      "        2041       1.00      0.36      0.53        14\n",
      "        2042       0.00      0.00      0.00         8\n",
      "        2043       0.00      0.00      0.00         4\n",
      "        2044       0.00      0.00      0.00         3\n",
      "        2045       1.00      0.05      0.10        20\n",
      "        2046       1.00      0.56      0.71        18\n",
      "        2047       0.33      0.11      0.17         9\n",
      "        2048       0.00      0.00      0.00         4\n",
      "        2049       0.40      0.15      0.22        13\n",
      "        2050       0.90      0.53      0.67        17\n",
      "        2051       1.00      0.33      0.50         9\n",
      "        2052       1.00      0.20      0.33         5\n",
      "        2053       1.00      0.33      0.50        12\n",
      "        2054       0.00      0.00      0.00         3\n",
      "        2055       1.00      0.25      0.40        20\n",
      "        2056       0.00      0.00      0.00         2\n",
      "        2057       0.00      0.00      0.00         2\n",
      "        2058       0.75      0.67      0.71        18\n",
      "        2059       1.00      0.17      0.29         6\n",
      "        2060       0.00      0.00      0.00        23\n",
      "        2061       0.00      0.00      0.00        19\n",
      "        2062       1.00      0.69      0.81        16\n",
      "        2063       0.00      0.00      0.00         5\n",
      "        2064       1.00      0.16      0.27        19\n",
      "        2065       1.00      0.50      0.67         2\n",
      "        2066       1.00      0.05      0.09        21\n",
      "        2067       1.00      0.06      0.11        18\n",
      "        2068       1.00      0.08      0.14        13\n",
      "        2069       0.67      0.25      0.36         8\n",
      "        2070       0.00      0.00      0.00        13\n",
      "        2071       1.00      0.64      0.78        11\n",
      "        2072       1.00      0.75      0.86         8\n",
      "        2073       0.00      0.00      0.00         5\n",
      "        2074       0.00      0.00      0.00         2\n",
      "        2075       1.00      0.31      0.47        13\n",
      "        2076       0.00      0.00      0.00        21\n",
      "        2077       0.67      0.33      0.44         6\n",
      "        2078       0.00      0.00      0.00        11\n",
      "        2079       1.00      0.40      0.57        10\n",
      "        2080       0.00      0.00      0.00         3\n",
      "        2081       0.00      0.00      0.00         9\n",
      "        2082       0.00      0.00      0.00        11\n",
      "        2083       0.00      0.00      0.00         5\n",
      "        2084       1.00      0.25      0.40         4\n",
      "        2085       0.00      0.00      0.00        13\n",
      "        2086       1.00      0.17      0.29         6\n",
      "        2087       0.88      0.64      0.74        22\n",
      "        2088       0.25      0.12      0.17         8\n",
      "        2089       0.00      0.00      0.00         7\n",
      "        2090       0.67      0.22      0.33         9\n",
      "        2091       0.14      0.45      0.22        11\n",
      "        2092       1.00      0.15      0.26        20\n",
      "        2093       1.00      0.16      0.27        19\n",
      "        2094       1.00      0.46      0.63        24\n",
      "        2095       0.00      0.00      0.00        10\n",
      "        2096       1.00      0.10      0.18        20\n",
      "        2097       0.40      0.20      0.27        10\n",
      "        2098       1.00      0.75      0.86        16\n",
      "        2099       0.00      0.00      0.00         7\n",
      "        2100       1.00      0.50      0.67         2\n",
      "        2101       1.00      0.16      0.27        19\n",
      "        2102       0.00      0.00      0.00         8\n",
      "        2103       0.00      0.00      0.00         5\n",
      "        2104       1.00      0.27      0.42        15\n",
      "        2105       1.00      0.07      0.13        14\n",
      "        2106       0.00      0.00      0.00        12\n",
      "        2107       0.00      0.00      0.00         5\n",
      "        2108       0.00      0.00      0.00         8\n",
      "        2109       1.00      0.31      0.48        16\n",
      "        2110       0.00      0.00      0.00         9\n",
      "        2111       1.00      0.33      0.50         3\n",
      "        2112       1.00      0.19      0.32        21\n",
      "        2113       0.00      0.00      0.00         6\n",
      "        2114       1.00      0.32      0.48        19\n",
      "        2115       0.55      0.55      0.55        11\n",
      "        2116       0.00      0.00      0.00         5\n",
      "        2117       0.00      0.00      0.00        12\n",
      "        2118       0.88      0.88      0.88         8\n",
      "        2119       0.00      0.00      0.00         1\n",
      "        2120       0.00      0.00      0.00        18\n",
      "        2121       1.00      0.20      0.33         5\n",
      "        2122       0.00      0.00      0.00        14\n",
      "        2123       0.00      0.00      0.00         2\n",
      "        2124       1.00      0.27      0.42        15\n",
      "        2125       1.00      0.33      0.50        18\n",
      "        2126       0.67      0.36      0.47        11\n",
      "        2127       1.00      0.46      0.63        24\n",
      "        2128       1.00      0.21      0.34        24\n",
      "        2129       0.44      0.25      0.32        16\n",
      "        2130       1.00      0.58      0.73        19\n",
      "        2131       0.00      0.00      0.00         5\n",
      "        2132       1.00      0.28      0.43        18\n",
      "        2133       1.00      0.44      0.61        16\n",
      "        2134       0.83      0.31      0.45        16\n",
      "        2135       1.00      0.43      0.60        21\n",
      "        2136       1.00      0.29      0.44        21\n",
      "        2137       0.00      0.00      0.00        12\n",
      "        2138       0.00      0.00      0.00         3\n",
      "        2139       1.00      0.79      0.88        14\n",
      "        2140       0.00      0.00      0.00        13\n",
      "        2141       0.00      0.00      0.00         3\n",
      "        2142       1.00      0.36      0.53        14\n",
      "        2143       1.00      0.52      0.69        21\n",
      "        2144       0.25      0.05      0.09        19\n",
      "        2146       0.00      0.00      0.00         6\n",
      "        2147       1.00      0.06      0.12        16\n",
      "        2148       1.00      0.28      0.43        18\n",
      "        2149       1.00      0.38      0.56        13\n",
      "        2150       1.00      0.21      0.35        19\n",
      "        2151       1.00      0.62      0.77         8\n",
      "        2152       1.00      0.30      0.46        10\n",
      "        2153       0.83      0.62      0.71         8\n",
      "        2154       1.00      0.27      0.43        11\n",
      "        2155       0.00      0.00      0.00         2\n",
      "        2156       0.00      0.00      0.00         6\n",
      "        2157       0.00      0.00      0.00        15\n",
      "        2158       1.00      0.48      0.65        21\n",
      "        2159       1.00      0.50      0.67         2\n",
      "        2160       0.00      0.00      0.00         7\n",
      "        2161       1.00      0.38      0.55        16\n",
      "        2162       1.00      0.22      0.36         9\n",
      "        2163       0.00      0.00      0.00        20\n",
      "        2164       1.00      0.62      0.76        13\n",
      "        2165       0.00      0.00      0.00         6\n",
      "        2166       0.89      0.44      0.59        18\n",
      "        2167       0.50      0.20      0.29         5\n",
      "        2168       0.00      0.00      0.00         7\n",
      "        2169       1.00      0.33      0.50         6\n",
      "        2170       1.00      0.60      0.75        15\n",
      "        2171       1.00      0.08      0.14        13\n",
      "        2172       1.00      0.31      0.47        13\n",
      "        2173       0.00      0.00      0.00         5\n",
      "        2174       0.00      0.00      0.00         7\n",
      "        2175       1.00      0.19      0.32        16\n",
      "        2176       0.00      0.00      0.00         8\n",
      "        2177       0.05      0.06      0.05        18\n",
      "        2178       1.00      1.00      1.00         2\n",
      "        2179       0.94      0.76      0.84        21\n",
      "        2180       1.00      0.04      0.08        23\n",
      "        2181       0.47      0.47      0.47        19\n",
      "        2182       0.00      0.00      0.00         4\n",
      "        2183       1.00      0.14      0.25         7\n",
      "        2184       1.00      0.67      0.80        12\n",
      "        2185       0.00      0.00      0.00        19\n",
      "        2186       0.52      0.81      0.63        16\n",
      "        2187       1.00      0.22      0.36        18\n",
      "        2188       1.00      0.17      0.29         6\n",
      "        2189       0.00      0.00      0.00         5\n",
      "        2190       0.00      0.00      0.00         2\n",
      "        2191       1.00      0.56      0.71        18\n",
      "        2192       0.00      0.00      0.00         8\n",
      "        2193       0.00      0.00      0.00        24\n",
      "        2194       0.60      0.40      0.48        15\n",
      "        2195       0.48      0.77      0.59        13\n",
      "        2196       0.71      0.42      0.53        12\n",
      "        2197       1.00      0.25      0.40        12\n",
      "        2198       0.75      0.53      0.62        17\n",
      "        2199       0.56      0.22      0.31        23\n",
      "        2200       1.00      0.15      0.27        26\n",
      "        2201       1.00      0.25      0.40         4\n",
      "        2202       1.00      0.45      0.62        22\n",
      "        2203       0.00      0.00      0.00         3\n",
      "        2204       0.57      0.40      0.47        10\n",
      "        2205       0.19      0.43      0.27        14\n",
      "        2206       0.00      0.00      0.00        12\n",
      "        2207       0.00      0.00      0.00         8\n",
      "        2208       0.90      0.50      0.64        18\n",
      "        2209       1.00      0.50      0.67         8\n",
      "        2210       1.00      0.50      0.67         6\n",
      "        2211       1.00      0.29      0.44        21\n",
      "        2212       1.00      0.29      0.44        14\n",
      "        2213       1.00      0.18      0.31        22\n",
      "        2214       1.00      0.36      0.53        22\n",
      "        2215       1.00      0.25      0.40        12\n",
      "        2216       1.00      0.20      0.33        10\n",
      "        2217       1.00      0.25      0.40        12\n",
      "        2218       1.00      0.06      0.11        17\n",
      "        2219       1.00      0.58      0.73        19\n",
      "        2220       0.21      0.75      0.32        16\n",
      "        2221       1.00      0.70      0.82        20\n",
      "        2222       0.00      0.00      0.00         3\n",
      "        2223       1.00      0.17      0.29         6\n",
      "        2224       1.00      0.64      0.78        22\n",
      "        2225       0.00      0.00      0.00        11\n",
      "        2226       0.06      0.20      0.09        10\n",
      "        2227       1.00      0.17      0.29         6\n",
      "        2228       1.00      0.40      0.57         5\n",
      "        2229       1.00      0.55      0.71        20\n",
      "        2230       1.00      0.05      0.09        21\n",
      "        2231       1.00      0.24      0.38        17\n",
      "        2232       0.00      0.00      0.00         3\n",
      "        2233       1.00      0.29      0.44        14\n",
      "        2234       1.00      0.56      0.71         9\n",
      "        2235       1.00      0.60      0.75         5\n",
      "        2236       1.00      0.47      0.64        15\n",
      "        2237       0.00      0.00      0.00        16\n",
      "        2238       0.00      0.00      0.00         7\n",
      "        2239       0.42      0.38      0.40        13\n",
      "        2240       1.00      0.24      0.38        17\n",
      "        2241       0.00      0.00      0.00         3\n",
      "        2242       0.00      0.00      0.00         4\n",
      "        2243       1.00      0.44      0.62         9\n",
      "        2244       1.00      0.24      0.38        21\n",
      "        2245       0.89      0.73      0.80        22\n",
      "        2246       0.75      0.38      0.50         8\n",
      "        2247       1.00      0.10      0.18        10\n",
      "        2248       0.83      0.38      0.53        13\n",
      "        2249       1.00      0.27      0.43        11\n",
      "        2250       0.00      0.00      0.00        19\n",
      "        2251       0.80      0.22      0.35        18\n",
      "        2252       0.70      0.37      0.48        19\n",
      "        2253       0.00      0.00      0.00         3\n",
      "        2254       0.75      0.27      0.40        11\n",
      "        2255       0.00      0.00      0.00         1\n",
      "        2256       1.00      0.33      0.50         9\n",
      "        2257       0.00      0.00      0.00         3\n",
      "        2258       0.00      0.00      0.00        10\n",
      "        2259       1.00      0.65      0.79        20\n",
      "        2260       0.00      0.00      0.00         8\n",
      "        2261       1.00      0.47      0.64        19\n",
      "        2262       1.00      0.12      0.22         8\n",
      "        2263       0.00      0.00      0.00         3\n",
      "        2264       0.24      0.29      0.26        14\n",
      "        2265       1.00      0.24      0.38        21\n",
      "        2266       0.00      0.00      0.00        11\n",
      "        2267       0.36      0.82      0.50        11\n",
      "        2268       1.00      0.20      0.33         5\n",
      "        2269       0.00      0.00      0.00         9\n",
      "        2270       0.50      0.54      0.52        13\n",
      "        2271       1.00      0.43      0.60         7\n",
      "        2272       0.14      0.05      0.08        19\n",
      "        2273       0.00      0.00      0.00         4\n",
      "        2274       0.71      0.48      0.57        21\n",
      "        2275       0.00      0.00      0.00         2\n",
      "        2276       1.00      0.61      0.76        23\n",
      "        2277       1.00      0.38      0.55        16\n",
      "        2278       1.00      0.11      0.19        19\n",
      "        2279       0.86      0.63      0.73        19\n",
      "        2280       0.00      0.00      0.00         4\n",
      "        2281       0.00      0.00      0.00         4\n",
      "        2282       1.00      0.12      0.22         8\n",
      "        2283       0.00      0.00      0.00         7\n",
      "        2285       1.00      0.19      0.32        21\n",
      "        2286       0.46      0.40      0.43        15\n",
      "        2287       1.00      0.37      0.54        19\n",
      "        2288       0.67      0.33      0.44         6\n",
      "        2289       1.00      0.46      0.63        13\n",
      "        2290       1.00      0.20      0.33         5\n",
      "        2291       0.80      0.47      0.59        17\n",
      "        2292       1.00      0.17      0.29        12\n",
      "        2293       1.00      0.11      0.20         9\n",
      "        2294       1.00      0.43      0.60        14\n",
      "        2295       0.00      0.00      0.00         5\n",
      "        2296       0.00      0.00      0.00         7\n",
      "        2297       0.00      0.00      0.00        10\n",
      "        2298       0.81      0.81      0.81        16\n",
      "        2299       0.00      0.00      0.00         4\n",
      "        2300       1.00      0.33      0.50        12\n",
      "        2301       0.92      0.73      0.81        15\n",
      "        2302       0.91      0.59      0.71        17\n",
      "        2303       1.00      0.30      0.47        23\n",
      "        2304       1.00      0.12      0.22        16\n",
      "        2305       1.00      0.67      0.80        15\n",
      "        2306       1.00      0.25      0.40        12\n",
      "        2307       0.00      0.00      0.00         4\n",
      "        2308       1.00      0.21      0.35        14\n",
      "        2309       0.00      0.00      0.00         2\n",
      "        2310       1.00      0.14      0.25        21\n",
      "        2311       1.00      0.69      0.82        13\n",
      "        2312       1.00      0.70      0.82        20\n",
      "        2313       0.00      0.00      0.00         7\n",
      "        2314       1.00      0.11      0.20         9\n",
      "        2315       1.00      0.53      0.70        15\n",
      "        2316       1.00      0.67      0.80         9\n",
      "        2317       1.00      0.30      0.46        20\n",
      "        2318       1.00      0.44      0.62         9\n",
      "        2319       0.00      0.00      0.00         1\n",
      "        2320       0.71      0.42      0.53        12\n",
      "        2321       0.00      0.00      0.00        11\n",
      "        2322       0.00      0.00      0.00         3\n",
      "        2323       1.00      0.20      0.33         5\n",
      "        2324       0.00      0.00      0.00         2\n",
      "        2325       0.00      0.00      0.00        14\n",
      "        2326       1.00      0.14      0.25         7\n",
      "        2327       0.44      0.29      0.35        14\n",
      "        2328       1.00      0.23      0.37        22\n",
      "        2329       1.00      0.38      0.55        16\n",
      "        2330       0.50      0.29      0.36        14\n",
      "        2331       1.00      0.44      0.61        16\n",
      "        2332       1.00      0.41      0.58        17\n",
      "        2333       0.00      0.00      0.00         1\n",
      "        2334       1.00      0.29      0.44        21\n",
      "        2335       0.50      0.29      0.36         7\n",
      "        2336       0.00      0.00      0.00         4\n",
      "        2337       0.00      0.00      0.00         6\n",
      "        2338       0.80      0.50      0.62         8\n",
      "        2339       1.00      0.37      0.54        19\n",
      "        2340       1.00      0.33      0.50        15\n",
      "        2341       0.50      0.08      0.14        12\n",
      "        2342       0.00      0.00      0.00         6\n",
      "        2343       0.71      0.26      0.38        19\n",
      "        2344       0.38      0.36      0.37        22\n",
      "        2345       1.00      0.38      0.56        13\n",
      "        2346       1.00      0.53      0.69        17\n",
      "        2347       1.00      0.41      0.58        17\n",
      "        2348       1.00      0.14      0.25         7\n",
      "        2349       1.00      0.20      0.33        10\n",
      "        2350       1.00      0.18      0.30        17\n",
      "        2351       0.00      0.00      0.00         4\n",
      "        2352       1.00      0.31      0.48        16\n",
      "        2353       1.00      0.19      0.32        21\n",
      "        2354       0.00      0.00      0.00         9\n",
      "        2355       1.00      0.11      0.20         9\n",
      "        2356       1.00      0.67      0.80        15\n",
      "        2357       1.00      0.07      0.12        15\n",
      "        2358       1.00      0.36      0.53        11\n",
      "        2359       0.00      0.00      0.00         7\n",
      "        2360       1.00      0.62      0.76        21\n",
      "        2361       0.00      0.00      0.00        10\n",
      "        2362       0.57      0.31      0.40        13\n",
      "        2363       1.00      0.08      0.14        13\n",
      "        2364       1.00      0.44      0.62        18\n",
      "        2365       0.93      0.52      0.67        25\n",
      "        2366       1.00      0.50      0.67        16\n",
      "        2367       0.50      0.17      0.25         6\n",
      "        2368       1.00      0.06      0.11        18\n",
      "        2369       1.00      0.77      0.87        22\n",
      "        2370       1.00      0.39      0.56        18\n",
      "        2371       1.00      0.09      0.17        22\n",
      "        2372       0.00      0.00      0.00        11\n",
      "        2373       0.00      0.00      0.00         1\n",
      "        2374       0.00      0.00      0.00         6\n",
      "        2375       0.67      0.24      0.35        17\n",
      "        2376       0.00      0.00      0.00         8\n",
      "        2377       0.78      0.35      0.48        20\n",
      "        2378       1.00      0.40      0.57        10\n",
      "        2379       1.00      0.59      0.74        17\n",
      "        2380       1.00      0.47      0.64        15\n",
      "        2381       1.00      0.08      0.15        12\n",
      "        2382       0.94      0.71      0.81        24\n",
      "        2383       0.62      0.45      0.53        22\n",
      "        2384       1.00      0.38      0.55        16\n",
      "        2385       0.00      0.00      0.00         5\n",
      "        2386       0.00      0.00      0.00         2\n",
      "        2387       0.00      0.00      0.00        23\n",
      "        2388       1.00      0.45      0.62        20\n",
      "        2389       0.67      0.22      0.33         9\n",
      "        2390       0.08      0.40      0.13        20\n",
      "        2391       1.00      0.17      0.29        18\n",
      "        2392       1.00      0.60      0.75         5\n",
      "        2393       1.00      0.23      0.38        13\n",
      "        2394       0.60      0.90      0.72        10\n",
      "        2395       1.00      0.17      0.29         6\n",
      "        2396       0.93      0.54      0.68        24\n",
      "        2397       0.75      0.33      0.46        18\n",
      "        2398       0.00      0.00      0.00        15\n",
      "        2399       1.00      0.14      0.25         7\n",
      "        2400       1.00      0.23      0.37        22\n",
      "        2401       0.00      0.00      0.00         8\n",
      "        2402       0.00      0.00      0.00         2\n",
      "        2403       0.00      0.00      0.00         1\n",
      "        2404       0.83      0.62      0.71         8\n",
      "        2405       1.00      0.60      0.75        10\n",
      "        2406       1.00      0.62      0.76        13\n",
      "        2407       0.00      0.00      0.00         2\n",
      "        2408       1.00      0.33      0.50        15\n",
      "        2409       0.00      0.00      0.00        17\n",
      "        2410       0.75      0.67      0.71         9\n",
      "        2411       0.78      0.58      0.67        12\n",
      "        2412       1.00      0.83      0.91         6\n",
      "        2413       1.00      0.57      0.73        21\n",
      "        2414       1.00      0.25      0.40         8\n",
      "        2415       1.00      0.32      0.48        22\n",
      "        2416       1.00      0.07      0.13        14\n",
      "        2417       1.00      0.44      0.61        16\n",
      "        2418       1.00      0.50      0.67        22\n",
      "        2419       1.00      0.06      0.12        16\n",
      "        2420       0.94      0.71      0.81        24\n",
      "        2421       1.00      0.12      0.22         8\n",
      "        2422       1.00      0.05      0.10        19\n",
      "        2423       1.00      0.53      0.69        19\n",
      "        2424       1.00      0.22      0.36         9\n",
      "        2425       1.00      0.67      0.80        12\n",
      "        2426       1.00      0.52      0.69        21\n",
      "        2427       1.00      0.15      0.26        20\n",
      "        2428       0.00      0.00      0.00         1\n",
      "        2429       1.00      0.11      0.20        18\n",
      "        2430       1.00      0.43      0.60        21\n",
      "        2431       0.00      0.00      0.00         6\n",
      "        2432       0.00      0.00      0.00         8\n",
      "        2433       1.00      0.32      0.48        22\n",
      "        2434       1.00      0.10      0.17        21\n",
      "        2435       1.00      0.11      0.20         9\n",
      "        2436       1.00      0.71      0.83         7\n",
      "        2437       1.00      0.12      0.21        17\n",
      "        2438       0.69      0.55      0.61        20\n",
      "        2439       0.00      0.00      0.00         7\n",
      "        2440       0.25      0.20      0.22        10\n",
      "        2441       0.00      0.00      0.00         3\n",
      "        2442       1.00      0.67      0.80        18\n",
      "        2443       0.31      0.40      0.35        10\n",
      "        2444       0.00      0.00      0.00        15\n",
      "        2445       0.00      0.00      0.00         8\n",
      "        2446       1.00      0.22      0.36         9\n",
      "        2447       0.77      0.81      0.79        21\n",
      "        2448       0.00      0.00      0.00         5\n",
      "        2449       0.55      0.50      0.52        12\n",
      "        2450       0.45      0.48      0.47        21\n",
      "        2451       0.00      0.00      0.00         3\n",
      "        2452       0.00      0.00      0.00        11\n",
      "        2453       0.00      0.00      0.00         9\n",
      "        2454       0.86      0.60      0.71        20\n",
      "        2455       1.00      0.30      0.46        20\n",
      "        2456       0.00      0.00      0.00         5\n",
      "        2457       0.00      0.00      0.00        20\n",
      "        2458       0.00      0.00      0.00         7\n",
      "        2459       1.00      0.23      0.38        13\n",
      "        2460       1.00      0.13      0.24        15\n",
      "        2461       0.00      0.00      0.00         8\n",
      "        2462       1.00      0.44      0.62        18\n",
      "        2463       0.59      0.56      0.57        18\n",
      "        2464       1.00      0.05      0.09        21\n",
      "        2465       1.00      0.05      0.09        21\n",
      "        2466       0.00      0.00      0.00        15\n",
      "        2467       0.42      0.65      0.51        20\n",
      "        2468       1.00      0.17      0.29         6\n",
      "        2469       0.27      0.43      0.33         7\n",
      "        2470       1.00      0.67      0.80        18\n",
      "        2471       1.00      0.17      0.29        18\n",
      "        2472       1.00      0.28      0.43        18\n",
      "        2473       1.00      0.23      0.37        22\n",
      "        2474       0.00      0.00      0.00         5\n",
      "        2475       0.00      0.00      0.00        13\n",
      "        2476       0.00      0.00      0.00         5\n",
      "        2477       0.00      0.00      0.00         1\n",
      "        2478       0.00      0.00      0.00         6\n",
      "        2479       0.80      0.57      0.67         7\n",
      "        2480       0.00      0.00      0.00         4\n",
      "        2481       0.00      0.00      0.00        20\n",
      "        2482       0.35      0.41      0.38        17\n",
      "        2483       0.73      0.73      0.73        11\n",
      "        2484       1.00      0.42      0.59        19\n",
      "        2485       1.00      0.57      0.72        23\n",
      "        2486       0.00      0.00      0.00         5\n",
      "        2487       1.00      0.40      0.57        20\n",
      "        2488       0.00      0.00      0.00         6\n",
      "        2489       0.00      0.00      0.00        22\n",
      "        2490       1.00      0.46      0.63        13\n",
      "        2491       0.00      0.00      0.00        15\n",
      "        2492       1.00      0.08      0.15        12\n",
      "        2493       0.00      0.00      0.00         3\n",
      "        2494       0.00      0.00      0.00         3\n",
      "        2495       1.00      0.11      0.20         9\n",
      "        2496       0.00      0.00      0.00         7\n",
      "        2497       1.00      0.50      0.67        16\n",
      "        2498       1.00      0.05      0.09        22\n",
      "        2499       0.00      0.00      0.00         9\n",
      "        2500       0.33      0.08      0.12        13\n",
      "        2501       0.33      0.20      0.25         5\n",
      "        2502       1.00      0.25      0.40        20\n",
      "        2503       0.00      0.00      0.00        13\n",
      "        2504       0.67      0.44      0.53        18\n",
      "        2505       1.00      0.27      0.42        15\n",
      "        2506       0.17      0.08      0.11        13\n",
      "        2507       0.00      0.00      0.00         6\n",
      "        2508       0.00      0.00      0.00         1\n",
      "        2509       1.00      0.53      0.70        15\n",
      "        2510       1.00      0.25      0.40         8\n",
      "        2511       1.00      0.65      0.79        20\n",
      "        2512       1.00      0.14      0.25         7\n",
      "        2513       0.00      0.00      0.00         7\n",
      "        2514       1.00      0.43      0.60        21\n",
      "        2515       1.00      0.50      0.67        16\n",
      "        2516       1.00      0.50      0.67         6\n",
      "        2517       1.00      0.20      0.33         5\n",
      "        2518       1.00      0.38      0.55         8\n",
      "        2519       0.87      0.76      0.81        17\n",
      "        2520       0.00      0.00      0.00         3\n",
      "        2521       0.00      0.00      0.00        16\n",
      "        2522       0.00      0.00      0.00         9\n",
      "        2523       0.86      0.67      0.75         9\n",
      "        2524       1.00      0.06      0.12        16\n",
      "        2525       0.43      0.30      0.35        10\n",
      "        2526       0.33      0.11      0.17        18\n",
      "        2527       0.00      0.00      0.00         3\n",
      "        2528       1.00      0.35      0.52        23\n",
      "        2529       0.00      0.00      0.00         7\n",
      "        2530       1.00      0.10      0.18        20\n",
      "        2531       1.00      0.21      0.35        19\n",
      "        2532       0.00      0.00      0.00         3\n",
      "        2533       1.00      0.32      0.48        22\n",
      "        2534       1.00      0.87      0.93        23\n",
      "        2535       0.00      0.00      0.00         6\n",
      "        2536       1.00      0.33      0.50         9\n",
      "        2537       0.80      0.47      0.59        17\n",
      "        2538       0.75      0.38      0.50         8\n",
      "        2539       0.81      0.72      0.76        18\n",
      "        2540       1.00      0.12      0.22         8\n",
      "        2541       0.33      0.33      0.33        15\n",
      "        2542       0.00      0.00      0.00         7\n",
      "        2543       1.00      0.33      0.50        15\n",
      "        2544       0.00      0.00      0.00        13\n",
      "        2545       1.00      0.26      0.42        19\n",
      "        2546       1.00      0.40      0.57        20\n",
      "        2547       1.00      0.78      0.88        18\n",
      "        2548       0.78      0.47      0.58        15\n",
      "        2549       1.00      0.44      0.62        18\n",
      "        2550       1.00      0.43      0.60        14\n",
      "        2551       0.00      0.00      0.00         4\n",
      "        2552       1.00      0.50      0.67        10\n",
      "        2553       1.00      0.54      0.70        13\n",
      "        2554       1.00      0.05      0.10        19\n",
      "        2555       0.29      0.50      0.36         4\n",
      "        2556       1.00      0.62      0.76        13\n",
      "        2557       0.75      0.17      0.27        18\n",
      "        2558       0.88      0.54      0.67        13\n",
      "        2559       0.42      0.50      0.45        10\n",
      "        2560       1.00      0.11      0.20         9\n",
      "        2561       1.00      0.20      0.33        20\n",
      "        2562       0.00      0.00      0.00        16\n",
      "        2563       1.00      0.35      0.52        23\n",
      "        2564       0.00      0.00      0.00        16\n",
      "        2565       1.00      0.27      0.43        11\n",
      "        2566       0.44      0.42      0.43        19\n",
      "        2567       0.00      0.00      0.00         3\n",
      "        2568       1.00      0.25      0.40         4\n",
      "        2569       0.71      0.67      0.69        18\n",
      "        2570       0.00      0.00      0.00         1\n",
      "        2571       0.00      0.00      0.00         4\n",
      "        2572       1.00      0.14      0.25         7\n",
      "        2573       1.00      0.25      0.40        16\n",
      "        2574       1.00      0.14      0.25        21\n",
      "        2575       1.00      0.20      0.33        20\n",
      "        2576       1.00      0.41      0.58        17\n",
      "        2577       1.00      0.14      0.25         7\n",
      "        2578       1.00      0.29      0.44         7\n",
      "        2579       1.00      0.41      0.58        17\n",
      "        2580       0.25      0.12      0.17         8\n",
      "        2581       0.00      0.00      0.00        19\n",
      "        2582       0.67      0.11      0.18        19\n",
      "        2583       0.00      0.00      0.00         3\n",
      "        2584       1.00      1.00      1.00         4\n",
      "        2585       0.00      0.00      0.00         4\n",
      "        2586       1.00      0.23      0.37        22\n",
      "        2587       0.00      0.00      0.00         6\n",
      "        2588       0.00      0.00      0.00         1\n",
      "        2589       0.00      0.00      0.00        18\n",
      "        2590       0.67      0.29      0.40         7\n",
      "        2591       1.00      0.40      0.57        20\n",
      "        2592       1.00      0.40      0.57        20\n",
      "        2593       1.00      0.12      0.21        17\n",
      "        2594       0.00      0.00      0.00        12\n",
      "        2595       1.00      0.80      0.89        20\n",
      "        2596       0.00      0.00      0.00         3\n",
      "        2597       1.00      0.21      0.35        14\n",
      "        2598       0.00      0.00      0.00         6\n",
      "        2599       1.00      0.32      0.48        19\n",
      "        2600       0.00      0.00      0.00         7\n",
      "        2601       0.00      0.00      0.00         3\n",
      "        2602       0.92      0.61      0.73        18\n",
      "        2603       1.00      0.05      0.10        20\n",
      "        2604       1.00      0.04      0.08        24\n",
      "        2605       0.80      0.24      0.36        17\n",
      "        2606       1.00      0.22      0.36         9\n",
      "        2607       1.00      0.25      0.40        20\n",
      "        2608       0.00      0.00      0.00        17\n",
      "        2609       0.00      0.00      0.00         3\n",
      "        2610       1.00      0.55      0.71        11\n",
      "        2611       0.00      0.00      0.00         3\n",
      "        2612       1.00      0.54      0.70        13\n",
      "        2613       0.00      0.00      0.00         3\n",
      "        2614       0.00      0.00      0.00         5\n",
      "        2615       0.67      0.20      0.31        10\n",
      "        2616       0.00      0.00      0.00        20\n",
      "        2617       0.14      0.18      0.16        11\n",
      "        2618       1.00      0.60      0.75        20\n",
      "        2619       1.00      0.22      0.36        23\n",
      "        2620       1.00      0.20      0.33        15\n",
      "        2621       1.00      0.10      0.18        10\n",
      "        2622       1.00      0.60      0.75        20\n",
      "        2623       0.00      0.00      0.00         1\n",
      "        2624       1.00      0.05      0.10        20\n",
      "        2625       1.00      0.42      0.59        19\n",
      "        2626       0.75      0.14      0.24        21\n",
      "        2627       0.07      0.06      0.06        17\n",
      "        2628       1.00      0.25      0.40         8\n",
      "        2629       1.00      0.25      0.40        20\n",
      "        2630       1.00      0.14      0.25        21\n",
      "        2631       0.00      0.00      0.00        16\n",
      "        2632       1.00      0.50      0.67         2\n",
      "        2633       0.00      0.00      0.00         1\n",
      "        2634       0.67      0.40      0.50         5\n",
      "        2635       0.00      0.00      0.00         3\n",
      "        2636       0.00      0.00      0.00         4\n",
      "        2637       1.00      0.50      0.67        20\n",
      "        2638       1.00      0.04      0.08        24\n",
      "        2639       0.00      0.00      0.00         9\n",
      "        2640       0.00      0.00      0.00        22\n",
      "        2641       1.00      0.40      0.57        15\n",
      "        2642       1.00      0.25      0.40        24\n",
      "        2643       1.00      0.23      0.38        13\n",
      "        2644       1.00      0.15      0.27        13\n",
      "        2645       0.67      0.25      0.36         8\n",
      "        2646       1.00      0.54      0.70        13\n",
      "        2647       0.00      0.00      0.00         5\n",
      "        2648       0.88      0.37      0.52        19\n",
      "        2649       0.00      0.00      0.00         2\n",
      "        2650       0.00      0.00      0.00         6\n",
      "        2651       1.00      0.29      0.44         7\n",
      "        2652       0.80      0.33      0.47        12\n",
      "        2653       1.00      0.11      0.20         9\n",
      "        2654       0.00      0.00      0.00        10\n",
      "        2655       1.00      0.46      0.63        13\n",
      "        2656       1.00      0.75      0.86        12\n",
      "        2657       0.00      0.00      0.00         6\n",
      "        2658       0.00      0.00      0.00         9\n",
      "        2659       1.00      0.17      0.29         6\n",
      "        2660       0.00      0.00      0.00        10\n",
      "        2661       1.00      0.20      0.33        20\n",
      "        2662       1.00      0.43      0.61        23\n",
      "        2663       0.14      0.07      0.10        14\n",
      "        2664       0.00      0.00      0.00         6\n",
      "        2665       1.00      0.29      0.44        14\n",
      "        2666       1.00      0.23      0.38        13\n",
      "        2667       0.67      0.53      0.59        15\n",
      "        2668       0.80      0.40      0.53        20\n",
      "        2669       1.00      0.50      0.67         8\n",
      "        2670       1.00      0.29      0.44        14\n",
      "        2671       0.00      0.00      0.00         7\n",
      "        2672       1.00      0.33      0.50         3\n",
      "        2673       1.00      0.12      0.22        16\n",
      "        2674       0.00      0.00      0.00         2\n",
      "        2675       1.00      0.50      0.67        16\n",
      "        2676       0.00      0.00      0.00        18\n",
      "        2677       0.00      0.00      0.00         6\n",
      "        2678       1.00      0.24      0.38        21\n",
      "        2679       0.00      0.00      0.00         6\n",
      "        2680       1.00      0.12      0.22         8\n",
      "        2681       0.00      0.00      0.00         3\n",
      "        2682       1.00      0.57      0.73        14\n",
      "        2683       0.50      0.27      0.35        11\n",
      "        2684       1.00      0.12      0.22        16\n",
      "        2685       0.00      0.00      0.00        17\n",
      "        2686       1.00      0.24      0.38        21\n",
      "        2687       1.00      0.50      0.67        10\n",
      "        2688       0.00      0.00      0.00         1\n",
      "        2689       0.86      0.30      0.44        20\n",
      "        2690       0.75      0.21      0.33        14\n",
      "        2691       0.00      0.00      0.00        15\n",
      "        2692       0.92      0.58      0.71        19\n",
      "        2693       0.33      0.85      0.48        13\n",
      "        2694       0.17      0.10      0.12        10\n",
      "        2695       0.00      0.00      0.00         5\n",
      "        2696       0.00      0.00      0.00         6\n",
      "        2697       1.00      0.20      0.33        10\n",
      "        2698       0.00      0.00      0.00        17\n",
      "        2699       0.86      0.50      0.63        12\n",
      "        2700       1.00      0.29      0.44        14\n",
      "        2701       1.00      0.21      0.35        14\n",
      "        2702       0.00      0.00      0.00         2\n",
      "        2703       0.60      0.43      0.50        14\n",
      "        2704       1.00      0.25      0.40        20\n",
      "        2705       0.00      0.00      0.00         2\n",
      "        2706       0.56      0.50      0.53        20\n",
      "        2707       0.00      0.00      0.00         8\n",
      "        2708       0.00      0.00      0.00         2\n",
      "        2709       0.00      0.00      0.00         5\n",
      "        2710       0.67      0.11      0.19        18\n",
      "        2711       1.00      0.10      0.18        20\n",
      "        2712       0.07      0.40      0.12        20\n",
      "        2713       1.00      0.69      0.82        13\n",
      "        2714       0.00      0.00      0.00        20\n",
      "        2715       1.00      0.10      0.17        21\n",
      "        2716       0.50      0.36      0.42        11\n",
      "        2717       1.00      0.16      0.28        25\n",
      "        2718       0.75      0.38      0.50        16\n",
      "        2719       0.38      0.67      0.48         9\n",
      "        2720       0.00      0.00      0.00         6\n",
      "        2721       0.05      0.46      0.09        13\n",
      "        2722       0.00      0.00      0.00         1\n",
      "        2723       0.00      0.00      0.00         7\n",
      "        2724       0.00      0.00      0.00         3\n",
      "        2725       0.32      0.43      0.36        14\n",
      "        2726       1.00      0.31      0.48        16\n",
      "        2727       0.68      0.72      0.70        18\n",
      "        2728       1.00      0.50      0.67        12\n",
      "        2729       1.00      0.25      0.40        16\n",
      "        2730       1.00      0.29      0.44         7\n",
      "        2731       1.00      0.33      0.50         6\n",
      "        2732       1.00      0.28      0.43        18\n",
      "        2733       1.00      0.29      0.44         7\n",
      "        2734       1.00      0.33      0.50         6\n",
      "        2735       1.00      0.45      0.62        20\n",
      "        2736       0.00      0.00      0.00         2\n",
      "        2737       0.33      0.22      0.27         9\n",
      "        2738       1.00      0.48      0.65        25\n",
      "        2739       1.00      0.14      0.25        14\n",
      "        2740       1.00      0.25      0.40         8\n",
      "        2741       0.00      0.00      0.00        14\n",
      "        2742       1.00      0.50      0.67        20\n",
      "        2743       1.00      0.44      0.61        25\n",
      "        2744       0.78      0.64      0.70        22\n",
      "        2745       0.00      0.00      0.00         6\n",
      "        2746       0.00      0.00      0.00         2\n",
      "        2747       1.00      0.17      0.29        12\n",
      "        2748       1.00      0.25      0.40         8\n",
      "        2749       0.00      0.00      0.00         3\n",
      "        2750       0.00      0.00      0.00        17\n",
      "        2751       1.00      0.26      0.42        19\n",
      "        2752       0.50      0.25      0.33         8\n",
      "        2753       1.00      0.62      0.76        13\n",
      "        2754       1.00      0.36      0.53        14\n",
      "        2755       1.00      0.12      0.22         8\n",
      "        2756       1.00      0.62      0.76        13\n",
      "        2757       1.00      0.62      0.77        16\n",
      "        2758       1.00      0.33      0.50         3\n",
      "        2759       0.25      0.20      0.22         5\n",
      "        2760       0.00      0.00      0.00         3\n",
      "        2761       1.00      0.78      0.88         9\n",
      "        2762       0.00      0.00      0.00         1\n",
      "        2763       0.00      0.00      0.00        11\n",
      "        2764       1.00      0.40      0.57        10\n",
      "        2765       1.00      0.43      0.60         7\n",
      "        2766       0.00      0.00      0.00         3\n",
      "        2767       1.00      0.08      0.14        13\n",
      "        2768       0.50      0.06      0.11        17\n",
      "        2769       0.62      0.48      0.54        21\n",
      "        2770       0.67      0.29      0.40        14\n",
      "        2771       1.00      0.35      0.52        17\n",
      "        2772       1.00      0.14      0.25         7\n",
      "        2773       1.00      0.61      0.76        23\n",
      "        2774       0.00      0.00      0.00         1\n",
      "        2775       0.00      0.00      0.00         4\n",
      "        2776       1.00      0.58      0.73        19\n",
      "        2777       1.00      0.14      0.25         7\n",
      "        2778       0.50      0.50      0.50         8\n",
      "        2779       0.00      0.00      0.00        20\n",
      "        2780       1.00      0.53      0.69        19\n",
      "        2781       0.82      0.43      0.56        21\n",
      "        2782       1.00      0.38      0.55         8\n",
      "        2783       0.00      0.00      0.00        14\n",
      "        2784       0.42      0.56      0.48        18\n",
      "        2785       1.00      0.67      0.80        18\n",
      "        2786       0.00      0.00      0.00         2\n",
      "        2787       1.00      0.59      0.74        22\n",
      "        2788       1.00      0.25      0.40         8\n",
      "        2789       1.00      0.15      0.26        20\n",
      "        2790       0.00      0.00      0.00        21\n",
      "        2791       0.90      0.47      0.62        19\n",
      "        2792       0.00      0.00      0.00         4\n",
      "        2793       0.50      0.20      0.29         5\n",
      "        2794       1.00      0.44      0.62         9\n",
      "        2795       0.91      0.53      0.67        19\n",
      "        2796       0.88      0.50      0.64        14\n",
      "        2797       0.00      0.00      0.00         3\n",
      "        2798       0.00      0.00      0.00        19\n",
      "        2799       0.00      0.00      0.00         7\n",
      "        2800       0.89      0.71      0.79        24\n",
      "        2801       1.00      0.08      0.14        13\n",
      "        2802       1.00      0.45      0.62        11\n",
      "        2803       1.00      0.40      0.57         5\n",
      "        2804       0.00      0.00      0.00         4\n",
      "        2805       0.00      0.00      0.00         2\n",
      "        2806       0.43      0.18      0.25        17\n",
      "        2807       1.00      0.25      0.40         8\n",
      "        2808       1.00      0.07      0.13        14\n",
      "        2809       1.00      0.25      0.40        20\n",
      "        2810       0.00      0.00      0.00         3\n",
      "        2811       1.00      0.06      0.11        17\n",
      "        2812       0.00      0.00      0.00         4\n",
      "        2813       0.00      0.00      0.00         5\n",
      "        2814       0.00      0.00      0.00         7\n",
      "        2815       0.67      0.22      0.33         9\n",
      "        2816       0.00      0.00      0.00         5\n",
      "        2817       0.00      0.00      0.00         7\n",
      "        2818       0.00      0.00      0.00        24\n",
      "        2819       1.00      0.12      0.22         8\n",
      "        2820       1.00      0.12      0.22        24\n",
      "        2821       0.94      0.71      0.81        24\n",
      "        2822       0.00      0.00      0.00         2\n",
      "        2823       0.80      0.19      0.31        21\n",
      "        2824       0.62      0.29      0.40        17\n",
      "        2825       0.00      0.00      0.00         6\n",
      "        2826       1.00      0.14      0.25         7\n",
      "        2827       0.50      0.20      0.29         5\n",
      "        2828       0.00      0.00      0.00         1\n",
      "        2829       0.78      0.47      0.58        15\n",
      "        2830       1.00      0.32      0.48        22\n",
      "        2831       0.00      0.00      0.00        18\n",
      "        2832       1.00      0.15      0.27        13\n",
      "        2833       0.25      0.40      0.31        10\n",
      "        2834       0.92      0.50      0.65        22\n",
      "        2835       0.75      0.43      0.55        14\n",
      "        2836       1.00      0.38      0.56        13\n",
      "        2837       1.00      0.14      0.25        14\n",
      "        2838       1.00      0.20      0.33         5\n",
      "        2839       1.00      0.44      0.61        25\n",
      "        2840       1.00      0.20      0.33         5\n",
      "        2841       1.00      0.35      0.52        17\n",
      "        2842       0.00      0.00      0.00        16\n",
      "        2843       1.00      0.17      0.29         6\n",
      "        2844       0.89      0.62      0.73        13\n",
      "        2845       1.00      0.45      0.62        11\n",
      "        2846       0.00      0.00      0.00         3\n",
      "        2847       0.00      0.00      0.00         3\n",
      "        2848       0.00      0.00      0.00         9\n",
      "        2849       0.00      0.00      0.00         2\n",
      "        2850       1.00      0.47      0.64        19\n",
      "        2851       1.00      0.76      0.86        21\n",
      "        2852       0.00      0.00      0.00         4\n",
      "        2853       1.00      0.39      0.56        18\n",
      "        2854       1.00      0.20      0.33        10\n",
      "        2855       0.00      0.00      0.00         4\n",
      "        2856       0.00      0.00      0.00         2\n",
      "        2857       1.00      0.64      0.78        14\n",
      "        2858       1.00      0.33      0.50        21\n",
      "        2859       0.00      0.00      0.00         7\n",
      "        2860       0.53      0.50      0.51        18\n",
      "        2861       1.00      0.33      0.50         3\n",
      "        2862       1.00      0.38      0.55         8\n",
      "        2863       0.00      0.00      0.00        11\n",
      "        2864       0.00      0.00      0.00        15\n",
      "        2865       0.57      0.21      0.31        19\n",
      "        2866       1.00      0.18      0.31        22\n",
      "        2867       0.00      0.00      0.00        19\n",
      "        2868       0.00      0.00      0.00         2\n",
      "        2869       0.00      0.00      0.00         3\n",
      "        2870       0.67      0.22      0.33        18\n",
      "        2871       1.00      0.32      0.48        25\n",
      "        2872       1.00      0.64      0.78        25\n",
      "        2873       0.00      0.00      0.00         1\n",
      "        2874       1.00      0.79      0.88        19\n",
      "        2875       1.00      0.14      0.25        14\n",
      "        2876       1.00      0.20      0.33        15\n",
      "        2877       1.00      0.70      0.82        23\n",
      "        2878       0.73      0.76      0.74        21\n",
      "        2879       0.33      0.35      0.34        17\n",
      "        2880       1.00      0.35      0.52        20\n",
      "        2881       1.00      0.77      0.87        13\n",
      "        2882       0.00      0.00      0.00         8\n",
      "        2883       0.00      0.00      0.00         1\n",
      "        2884       0.00      0.00      0.00         5\n",
      "        2885       0.00      0.00      0.00        23\n",
      "        2886       1.00      0.52      0.69        21\n",
      "        2887       0.00      0.00      0.00         3\n",
      "        2888       1.00      0.10      0.18        20\n",
      "        2889       1.00      0.08      0.15        24\n",
      "        2890       0.00      0.00      0.00         2\n",
      "        2891       1.00      0.79      0.88        24\n",
      "        2892       1.00      0.38      0.55         8\n",
      "        2893       0.30      0.38      0.33         8\n",
      "        2894       0.78      0.39      0.52        18\n",
      "        2895       1.00      0.29      0.44         7\n",
      "        2896       1.00      0.17      0.29         6\n",
      "        2897       0.00      0.00      0.00         7\n",
      "        2898       0.40      0.27      0.32        15\n",
      "        2899       0.50      0.57      0.53        14\n",
      "        2900       0.02      0.60      0.04        15\n",
      "        2901       1.00      0.11      0.20        18\n",
      "        2902       1.00      0.14      0.24        22\n",
      "        2903       0.00      0.00      0.00         7\n",
      "        2904       1.00      0.22      0.36        18\n",
      "        2905       1.00      0.77      0.87        13\n",
      "        2906       1.00      0.38      0.55        16\n",
      "        2907       0.00      0.00      0.00         2\n",
      "        2908       0.00      0.00      0.00        19\n",
      "        2909       0.00      0.00      0.00         2\n",
      "        2910       0.00      0.00      0.00         2\n",
      "        2911       1.00      0.91      0.95        23\n",
      "        2912       0.50      0.25      0.33         8\n",
      "        2913       1.00      0.60      0.75         5\n",
      "        2914       0.00      0.00      0.00        12\n",
      "        2915       1.00      0.56      0.71         9\n",
      "        2916       0.00      0.00      0.00        14\n",
      "        2917       1.00      0.47      0.64        19\n",
      "        2918       1.00      0.43      0.60         7\n",
      "        2919       0.00      0.00      0.00        22\n",
      "        2920       0.00      0.00      0.00         3\n",
      "        2921       0.00      0.00      0.00         7\n",
      "        2922       0.50      0.25      0.33         4\n",
      "        2923       0.11      0.25      0.15         4\n",
      "        2924       0.00      0.00      0.00        19\n",
      "        2925       1.00      0.80      0.89        10\n",
      "        2926       1.00      0.50      0.67        14\n",
      "        2927       1.00      0.16      0.27        19\n",
      "        2928       0.88      0.67      0.76        21\n",
      "        2929       0.71      0.55      0.62        22\n",
      "        2930       1.00      0.63      0.77        19\n",
      "        2931       0.67      0.40      0.50        15\n",
      "        2932       0.00      0.00      0.00        13\n",
      "        2933       0.50      0.05      0.08        22\n",
      "        2934       1.00      0.22      0.36        18\n",
      "        2935       1.00      0.70      0.82        20\n",
      "        2936       0.00      0.00      0.00         2\n",
      "        2937       1.00      0.30      0.46        10\n",
      "        2938       1.00      0.40      0.57         5\n",
      "        2939       0.50      0.11      0.17        19\n",
      "        2940       0.83      0.50      0.62        10\n",
      "        2941       0.00      0.00      0.00        15\n",
      "        2942       1.00      0.50      0.67        20\n",
      "        2943       1.00      0.20      0.33         5\n",
      "        2944       0.00      0.00      0.00         2\n",
      "        2945       1.00      0.08      0.15        12\n",
      "        2946       1.00      0.20      0.33         5\n",
      "        2947       1.00      0.12      0.22         8\n",
      "        2948       0.00      0.00      0.00         9\n",
      "        2949       1.00      0.75      0.86         4\n",
      "        2950       0.00      0.00      0.00         5\n",
      "        2951       1.00      0.43      0.60         7\n",
      "        2952       0.75      0.38      0.50         8\n",
      "        2953       1.00      0.82      0.90        11\n",
      "        2954       1.00      0.11      0.20        18\n",
      "        2955       0.57      0.44      0.50         9\n",
      "        2956       0.00      0.00      0.00         4\n",
      "        2957       0.60      0.15      0.24        20\n",
      "        2958       0.20      0.22      0.21         9\n",
      "        2959       0.00      0.00      0.00         7\n",
      "        2960       0.67      0.25      0.36         8\n",
      "        2961       1.00      0.15      0.27        13\n",
      "        2962       0.00      0.00      0.00        21\n",
      "        2963       0.00      0.00      0.00         6\n",
      "        2964       1.00      0.09      0.17        22\n",
      "        2965       0.00      0.00      0.00        15\n",
      "        2966       1.00      0.22      0.36         9\n",
      "        2967       1.00      0.46      0.63        13\n",
      "        2968       0.00      0.00      0.00         5\n",
      "        2969       1.00      0.38      0.55        16\n",
      "        2970       1.00      0.21      0.35        14\n",
      "        2971       0.78      0.72      0.75        25\n",
      "        2972       1.00      0.27      0.43        22\n",
      "        2973       1.00      0.18      0.30        17\n",
      "        2974       0.85      0.69      0.76        16\n",
      "        2975       0.26      0.50      0.34        22\n",
      "        2976       0.64      0.50      0.56        18\n",
      "        2977       0.00      0.00      0.00         8\n",
      "        2978       0.67      0.50      0.57        12\n",
      "        2979       0.00      0.00      0.00         9\n",
      "        2980       1.00      0.60      0.75        20\n",
      "        2981       0.00      0.00      0.00        11\n",
      "        2982       0.00      0.00      0.00         4\n",
      "        2983       1.00      0.07      0.13        14\n",
      "        2984       0.00      0.00      0.00        22\n",
      "        2985       0.64      0.50      0.56        18\n",
      "        2986       1.00      0.70      0.82        10\n",
      "        2987       0.00      0.00      0.00        18\n",
      "        2988       1.00      0.42      0.59        12\n",
      "        2989       0.00      0.00      0.00         6\n",
      "        2990       0.00      0.00      0.00         2\n",
      "        2991       1.00      0.45      0.62        20\n",
      "        2992       0.00      0.00      0.00         8\n",
      "        2993       1.00      0.33      0.50         3\n",
      "        2994       1.00      0.55      0.71        22\n",
      "        2995       1.00      0.25      0.40         4\n",
      "        2996       0.88      0.44      0.58        16\n",
      "        2997       0.83      0.59      0.69        17\n",
      "        2998       0.25      0.25      0.25         4\n",
      "        2999       0.00      0.00      0.00        14\n",
      "        3000       0.82      0.74      0.78        19\n",
      "        3001       0.25      0.21      0.23        14\n",
      "        3002       0.56      0.58      0.57        24\n",
      "        3003       0.33      0.07      0.12        14\n",
      "        3004       0.00      0.00      0.00         5\n",
      "        3005       1.00      0.25      0.40         4\n",
      "        3006       1.00      0.09      0.17        11\n",
      "        3007       1.00      0.07      0.13        14\n",
      "        3008       1.00      0.19      0.32        16\n",
      "        3009       0.00      0.00      0.00        20\n",
      "        3010       0.80      0.73      0.76        11\n",
      "        3011       0.60      0.43      0.50        14\n",
      "        3012       0.92      1.00      0.96        11\n",
      "        3013       0.00      0.00      0.00        16\n",
      "        3014       0.83      0.29      0.43        17\n",
      "        3015       1.00      0.29      0.44        14\n",
      "        3016       0.80      0.42      0.55        19\n",
      "        3017       1.00      0.44      0.62        18\n",
      "        3018       1.00      0.33      0.50         6\n",
      "        3019       1.00      0.50      0.67         8\n",
      "        3020       0.00      0.00      0.00         6\n",
      "        3021       0.00      0.00      0.00         7\n",
      "        3022       1.00      0.46      0.63        13\n",
      "        3023       0.44      0.58      0.50        12\n",
      "        3024       0.00      0.00      0.00         7\n",
      "        3025       0.00      0.00      0.00         3\n",
      "        3026       0.00      0.00      0.00         1\n",
      "        3027       0.00      0.00      0.00         5\n",
      "        3028       0.00      0.00      0.00        15\n",
      "        3029       1.00      0.43      0.60         7\n",
      "        3030       0.29      0.19      0.23        21\n",
      "        3031       0.00      0.00      0.00        16\n",
      "        3032       1.00      0.46      0.63        13\n",
      "        3033       1.00      0.06      0.12        16\n",
      "        3034       0.00      0.00      0.00         6\n",
      "        3035       1.00      0.05      0.10        19\n",
      "        3036       0.50      0.20      0.29        15\n",
      "        3037       0.00      0.00      0.00         1\n",
      "        3038       1.00      0.77      0.87        13\n",
      "        3039       0.00      0.00      0.00         7\n",
      "        3040       0.60      0.32      0.41        19\n",
      "        3041       0.07      0.25      0.11         4\n",
      "        3042       0.50      0.20      0.29         5\n",
      "        3043       0.00      0.00      0.00         6\n",
      "        3044       0.00      0.00      0.00        13\n",
      "        3045       0.38      0.30      0.33        10\n",
      "        3046       1.00      0.25      0.40        20\n",
      "        3047       0.44      0.40      0.42        10\n",
      "        3048       0.00      0.00      0.00         1\n",
      "        3049       0.00      0.00      0.00         2\n",
      "        3050       0.00      0.00      0.00        12\n",
      "        3051       0.50      0.25      0.33        12\n",
      "\n",
      "    accuracy                           0.31     38527\n",
      "   macro avg       0.58      0.25      0.32     38527\n",
      "weighted avg       0.70      0.31      0.39     38527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_test_path = '/home/ielab/dataset/ms1m_dataset/test'\n",
    "\n",
    "og_testset = CustomDataset(og_test_path, trans)\n",
    "og_testloader = DataLoader(og_testset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2117904",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_test_results = []\n",
    "og_test_labels = []\n",
    "\n",
    "embedder.eval()\n",
    "with torch.no_grad():\n",
    "  for img, label in og_testloader:\n",
    "    img = img.cuda()\n",
    "    og_test_results.append(embedder(img).cpu().detach().numpy())\n",
    "    og_test_labels.append(label)\n",
    "        \n",
    "og_test_results = np.concatenate(og_test_results)\n",
    "og_test_labels = np.concatenate(og_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46341b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_test_pred = model.predict(og_test_results)\n",
    "(og_test_pred == og_test_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f3ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedder, './mosaic_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa01549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcface",
   "language": "python",
   "name": "arcface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
