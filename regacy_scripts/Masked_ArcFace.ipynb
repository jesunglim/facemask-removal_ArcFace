{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b210ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b1a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bef26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ea4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a08571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/ielab/dataset/ms1m_dataset/train_masked/'\n",
    "test_path = '/home/ielab/dataset/ms1m_dataset/test_masked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4148c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = os.listdir(train_path)\n",
    "\n",
    "lb = {string : i for i,string in enumerate(col_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7903cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1977',\n",
       " '2782',\n",
       " '1236',\n",
       " '2546',\n",
       " '1867',\n",
       " '1647',\n",
       " '1458',\n",
       " '1686',\n",
       " '2647',\n",
       " '1504',\n",
       " '1148',\n",
       " '2373',\n",
       " '2246',\n",
       " '3262',\n",
       " '407',\n",
       " '1569',\n",
       " '226',\n",
       " '1663',\n",
       " '3590',\n",
       " '172',\n",
       " '1991',\n",
       " '192',\n",
       " '3183',\n",
       " '183',\n",
       " '1170',\n",
       " '1191',\n",
       " '256',\n",
       " '1618',\n",
       " '1731',\n",
       " '2076',\n",
       " '203',\n",
       " '3264',\n",
       " '1791',\n",
       " '2709',\n",
       " '2362',\n",
       " '2053',\n",
       " '2657',\n",
       " '3002',\n",
       " '1525',\n",
       " '2954',\n",
       " '2820',\n",
       " '2903',\n",
       " '3372',\n",
       " '2719',\n",
       " '1141',\n",
       " '2222',\n",
       " '2730',\n",
       " '265',\n",
       " '348',\n",
       " '1944',\n",
       " '3036',\n",
       " '1443',\n",
       " '1039',\n",
       " '3522',\n",
       " '1414',\n",
       " '1425',\n",
       " '1473',\n",
       " '1697',\n",
       " '2724',\n",
       " '1902',\n",
       " '1355',\n",
       " '2385',\n",
       " '466',\n",
       " '2276',\n",
       " '1015',\n",
       " '3338',\n",
       " '1481',\n",
       " '1433',\n",
       " '2144',\n",
       " '3578',\n",
       " '2529',\n",
       " '1486',\n",
       " '2157',\n",
       " '3501',\n",
       " '1259',\n",
       " '3095',\n",
       " '2240',\n",
       " '2883',\n",
       " '1915',\n",
       " '1004',\n",
       " '1107',\n",
       " '472',\n",
       " '2467',\n",
       " '1490',\n",
       " '2391',\n",
       " '148',\n",
       " '3345',\n",
       " '2387',\n",
       " '413',\n",
       " '2111',\n",
       " '2326',\n",
       " '2694',\n",
       " '2840',\n",
       " '1285',\n",
       " '2279',\n",
       " '1658',\n",
       " '3350',\n",
       " '3524',\n",
       " '1442',\n",
       " '1570',\n",
       " '2428',\n",
       " '2079',\n",
       " '2088',\n",
       " '1153',\n",
       " '2571',\n",
       " '3541',\n",
       " '1718',\n",
       " '2159',\n",
       " '2539',\n",
       " '2744',\n",
       " '2244',\n",
       " '1319',\n",
       " '3364',\n",
       " '3550',\n",
       " '2907',\n",
       " '1773',\n",
       " '1333',\n",
       " '112',\n",
       " '3407',\n",
       " '121',\n",
       " '272',\n",
       " '2045',\n",
       " '188',\n",
       " '2139',\n",
       " '171',\n",
       " '1403',\n",
       " '1101',\n",
       " '1277',\n",
       " '2656',\n",
       " '3567',\n",
       " '2106',\n",
       " '3294',\n",
       " '251',\n",
       " '1696',\n",
       " '2801',\n",
       " '1082',\n",
       " '2348',\n",
       " '1667',\n",
       " '229',\n",
       " '3486',\n",
       " '2143',\n",
       " '17',\n",
       " '1353',\n",
       " '1224',\n",
       " '2566',\n",
       " '454',\n",
       " '2655',\n",
       " '352',\n",
       " '1740',\n",
       " '2772',\n",
       " '3007',\n",
       " '1322',\n",
       " '3552',\n",
       " '1517',\n",
       " '1691',\n",
       " '279',\n",
       " '1633',\n",
       " '1776',\n",
       " '2218',\n",
       " '222',\n",
       " '1387',\n",
       " '1485',\n",
       " '3181',\n",
       " '3429',\n",
       " '2108',\n",
       " '1172',\n",
       " '1331',\n",
       " '1061',\n",
       " '206',\n",
       " '1713',\n",
       " '3343',\n",
       " '2004',\n",
       " '1519',\n",
       " '2522',\n",
       " '2948',\n",
       " '3109',\n",
       " '1662',\n",
       " '2006',\n",
       " '2417',\n",
       " '3101',\n",
       " '1565',\n",
       " '2177',\n",
       " '1019',\n",
       " '3049',\n",
       " '1755',\n",
       " '2845',\n",
       " '2650',\n",
       " '2731',\n",
       " '2747',\n",
       " '1802',\n",
       " '2551',\n",
       " '2749',\n",
       " '146',\n",
       " '1023',\n",
       " '264',\n",
       " '38',\n",
       " '2226',\n",
       " '1450',\n",
       " '1900',\n",
       " '2015',\n",
       " '27',\n",
       " '343',\n",
       " '1245',\n",
       " '1297',\n",
       " '1936',\n",
       " '299',\n",
       " '1942',\n",
       " '3158',\n",
       " '2171',\n",
       " '2860',\n",
       " '2190',\n",
       " '1782',\n",
       " '1901',\n",
       " '2921',\n",
       " '2750',\n",
       " '1027',\n",
       " '1806',\n",
       " '3091',\n",
       " '102',\n",
       " '3594',\n",
       " '3057',\n",
       " '2942',\n",
       " '3342',\n",
       " '1842',\n",
       " '3066',\n",
       " '3276',\n",
       " '2054',\n",
       " '335',\n",
       " '2602',\n",
       " '2034',\n",
       " '1121',\n",
       " '1117',\n",
       " '1218',\n",
       " '1578',\n",
       " '351',\n",
       " '2158',\n",
       " '2297',\n",
       " '2720',\n",
       " '2314',\n",
       " '2613',\n",
       " '3451',\n",
       " '2212',\n",
       " '1631',\n",
       " '2920',\n",
       " '1871',\n",
       " '2927',\n",
       " '1666',\n",
       " '3481',\n",
       " '3128',\n",
       " '3375',\n",
       " '1890',\n",
       " '3517',\n",
       " '140',\n",
       " '3558',\n",
       " '2738',\n",
       " '1612',\n",
       " '3527',\n",
       " '3255',\n",
       " '1952',\n",
       " '1737',\n",
       " '2619',\n",
       " '1855',\n",
       " '2824',\n",
       " '1924',\n",
       " '3497',\n",
       " '2209',\n",
       " '1254',\n",
       " '1492',\n",
       " '2187',\n",
       " '2147',\n",
       " '1126',\n",
       " '30',\n",
       " '1732',\n",
       " '2836',\n",
       " '3282',\n",
       " '2061',\n",
       " '1051',\n",
       " '1309',\n",
       " '3256',\n",
       " '1135',\n",
       " '2926',\n",
       " '3061',\n",
       " '1550',\n",
       " '1394',\n",
       " '2432',\n",
       " '1335',\n",
       " '2114',\n",
       " '2188',\n",
       " '2736',\n",
       " '2600',\n",
       " '1876',\n",
       " '1427',\n",
       " '2507',\n",
       " '3409',\n",
       " '2623',\n",
       " '3260',\n",
       " '2197',\n",
       " '48',\n",
       " '1847',\n",
       " '3365',\n",
       " '3521',\n",
       " '1833',\n",
       " '1212',\n",
       " '2880',\n",
       " '2375',\n",
       " '1883',\n",
       " '288',\n",
       " '2189',\n",
       " '1190',\n",
       " '1342',\n",
       " '304',\n",
       " '1242',\n",
       " '2024',\n",
       " '3004',\n",
       " '2032',\n",
       " '2050',\n",
       " '1874',\n",
       " '1553',\n",
       " '161',\n",
       " '3452',\n",
       " '1469',\n",
       " '1194',\n",
       " '2257',\n",
       " '1854',\n",
       " '3357',\n",
       " '1653',\n",
       " '1798',\n",
       " '2634',\n",
       " '2543',\n",
       " '2669',\n",
       " '2561',\n",
       " '3116',\n",
       " '1704',\n",
       " '2173',\n",
       " '2858',\n",
       " '1088',\n",
       " '2991',\n",
       " '1683',\n",
       " '3026',\n",
       " '3192',\n",
       " '3013',\n",
       " '2664',\n",
       " '33',\n",
       " '159',\n",
       " '3141',\n",
       " '3277',\n",
       " '2356',\n",
       " '1513',\n",
       " '3392',\n",
       " '2347',\n",
       " '3490',\n",
       " '3402',\n",
       " '1888',\n",
       " '1783',\n",
       " '1202',\n",
       " '2353',\n",
       " '1270',\n",
       " '241',\n",
       " '2848',\n",
       " '42',\n",
       " '270',\n",
       " '3431',\n",
       " '1307',\n",
       " '457',\n",
       " '310',\n",
       " '2446',\n",
       " '2063',\n",
       " '2291',\n",
       " '2967',\n",
       " '3198',\n",
       " '1086',\n",
       " '249',\n",
       " '487',\n",
       " '3153',\n",
       " '1047',\n",
       " '1334',\n",
       " '2122',\n",
       " '14',\n",
       " '2141',\n",
       " '2396',\n",
       " '3414',\n",
       " '1008',\n",
       " '1652',\n",
       " '3416',\n",
       " '1145',\n",
       " '1675',\n",
       " '291',\n",
       " '1775',\n",
       " '1324',\n",
       " '2398',\n",
       " '132',\n",
       " '2864',\n",
       " '1934',\n",
       " '3306',\n",
       " '1318',\n",
       " '2294',\n",
       " '2340',\n",
       " '3332',\n",
       " '2030',\n",
       " '152',\n",
       " '2628',\n",
       " '3540',\n",
       " '1698',\n",
       " '2900',\n",
       " '2586',\n",
       " '114',\n",
       " '1471',\n",
       " '1484',\n",
       " '373',\n",
       " '1456',\n",
       " '2180',\n",
       " '463',\n",
       " '3437',\n",
       " '2945',\n",
       " '1286',\n",
       " '1420',\n",
       " '3075',\n",
       " '2409',\n",
       " '1932',\n",
       " '1444',\n",
       " '1939',\n",
       " '1745',\n",
       " '1743',\n",
       " '2378',\n",
       " '2828',\n",
       " '2110',\n",
       " '2545',\n",
       " '2721',\n",
       " '139',\n",
       " '1695',\n",
       " '1476',\n",
       " '1724',\n",
       " '399',\n",
       " '374',\n",
       " '2581',\n",
       " '3010',\n",
       " '1830',\n",
       " '3275',\n",
       " '3156',\n",
       " '2072',\n",
       " '1156',\n",
       " '1817',\n",
       " '1734',\n",
       " '1099',\n",
       " '3460',\n",
       " '1009',\n",
       " '3510',\n",
       " '1114',\n",
       " '1719',\n",
       " '1796',\n",
       " '3063',\n",
       " '3143',\n",
       " '293',\n",
       " '484',\n",
       " '338',\n",
       " '2041',\n",
       " '1188',\n",
       " '2210',\n",
       " '1729',\n",
       " '3371',\n",
       " '2357',\n",
       " '2642',\n",
       " '2127',\n",
       " '3538',\n",
       " '2960',\n",
       " '2254',\n",
       " '2327',\n",
       " '3381',\n",
       " '427',\n",
       " '3467',\n",
       " '3241',\n",
       " '1628',\n",
       " '2207',\n",
       " '401',\n",
       " '3528',\n",
       " '2463',\n",
       " '3014',\n",
       " '1749',\n",
       " '1822',\n",
       " '2697',\n",
       " '2018',\n",
       " '2913',\n",
       " '2973',\n",
       " '2413',\n",
       " '1753',\n",
       " '2264',\n",
       " '1044',\n",
       " '1584',\n",
       " '2293',\n",
       " '1777',\n",
       " '3395',\n",
       " '1247',\n",
       " '2531',\n",
       " '3423',\n",
       " '1657',\n",
       " '247',\n",
       " '3575',\n",
       " '200',\n",
       " '3568',\n",
       " '1410',\n",
       " '3113',\n",
       " '3505',\n",
       " '2205',\n",
       " '2646',\n",
       " '1778',\n",
       " '2275',\n",
       " '3503',\n",
       " '1020',\n",
       " '3577',\n",
       " '2443',\n",
       " '465',\n",
       " '2372',\n",
       " '136',\n",
       " '167',\n",
       " '2448',\n",
       " '2194',\n",
       " '1163',\n",
       " '1349',\n",
       " '362',\n",
       " '1603',\n",
       " '1841',\n",
       " '2771',\n",
       " '3464',\n",
       " '1621',\n",
       " '3178',\n",
       " '3461',\n",
       " '1408',\n",
       " '3100',\n",
       " '1232',\n",
       " '2511',\n",
       " '1308',\n",
       " '2155',\n",
       " '3440',\n",
       " '3386',\n",
       " '1025',\n",
       " '3515',\n",
       " '2390',\n",
       " '1221',\n",
       " '2027',\n",
       " '2242',\n",
       " '3339',\n",
       " '398',\n",
       " '2306',\n",
       " '461',\n",
       " '1261',\n",
       " '375',\n",
       " '2596',\n",
       " '2492',\n",
       " '2007',\n",
       " '2313',\n",
       " '3270',\n",
       " '2249',\n",
       " '3278',\n",
       " '1041',\n",
       " '1500',\n",
       " '2295',\n",
       " '3518',\n",
       " '3133',\n",
       " '2454',\n",
       " '3265',\n",
       " '1445',\n",
       " '2224',\n",
       " '1970',\n",
       " '426',\n",
       " '467',\n",
       " '1368',\n",
       " '1937',\n",
       " '419',\n",
       " '2524',\n",
       " '2399',\n",
       " '1870',\n",
       " '3436',\n",
       " '1146',\n",
       " '388',\n",
       " '1042',\n",
       " '2608',\n",
       " '1098',\n",
       " '3219',\n",
       " '1611',\n",
       " '1311',\n",
       " '2852',\n",
       " '1877',\n",
       " '2648',\n",
       " '412',\n",
       " '1850',\n",
       " '1155',\n",
       " '1797',\n",
       " '3190',\n",
       " '2838',\n",
       " '1371',\n",
       " '2521',\n",
       " '1812',\n",
       " '2485',\n",
       " '2331',\n",
       " '2253',\n",
       " '3126',\n",
       " '2068',\n",
       " '1105',\n",
       " '322',\n",
       " '282',\n",
       " '2424',\n",
       " '3573',\n",
       " '2874',\n",
       " '3068',\n",
       " '3508',\n",
       " '1979',\n",
       " '1945',\n",
       " '424',\n",
       " '3096',\n",
       " '3235',\n",
       " '2423',\n",
       " '1196',\n",
       " '3076',\n",
       " '1639',\n",
       " '2762',\n",
       " '475',\n",
       " '1195',\n",
       " '2956',\n",
       " '3382',\n",
       " '186',\n",
       " '1586',\n",
       " '1175',\n",
       " '447',\n",
       " '1511',\n",
       " '324',\n",
       " '2859',\n",
       " '3152',\n",
       " '1917',\n",
       " '1493',\n",
       " '3245',\n",
       " '2270',\n",
       " '3543',\n",
       " '488',\n",
       " '3356',\n",
       " '2073',\n",
       " '3018',\n",
       " '1220',\n",
       " '2784',\n",
       " '1894',\n",
       " '1118',\n",
       " '1809',\n",
       " '3466',\n",
       " '3195',\n",
       " '3085',\n",
       " '3445',\n",
       " '1710',\n",
       " '2690',\n",
       " '1237',\n",
       " '157',\n",
       " '234',\n",
       " '1068',\n",
       " '201',\n",
       " '3355',\n",
       " '2542',\n",
       " '2156',\n",
       " '1637',\n",
       " '280',\n",
       " '1167',\n",
       " '3433',\n",
       " '1885',\n",
       " '2220',\n",
       " '3379',\n",
       " '2096',\n",
       " '2761',\n",
       " '1316',\n",
       " '2316',\n",
       " '1625',\n",
       " '3556',\n",
       " '1198',\n",
       " '471',\n",
       " '1060',\n",
       " '1954',\n",
       " '147',\n",
       " '29',\n",
       " '3058',\n",
       " '2058',\n",
       " '1112',\n",
       " '1363',\n",
       " '106',\n",
       " '2788',\n",
       " '1507',\n",
       " '2764',\n",
       " '2365',\n",
       " '2368',\n",
       " '422',\n",
       " '5',\n",
       " '2981',\n",
       " '1540',\n",
       " '1590',\n",
       " '300',\n",
       " '2002',\n",
       " '2822',\n",
       " '15',\n",
       " '276',\n",
       " '1472',\n",
       " '2483',\n",
       " '3123',\n",
       " '277',\n",
       " '2518',\n",
       " '1680',\n",
       " '2559',\n",
       " '1385',\n",
       " '2280',\n",
       " '2816',\n",
       " '19',\n",
       " '3363',\n",
       " '1572',\n",
       " '1248',\n",
       " '2704',\n",
       " '2812',\n",
       " '3498',\n",
       " '2089',\n",
       " '2493',\n",
       " '2292',\n",
       " '126',\n",
       " '2059',\n",
       " '1610',\n",
       " '3191',\n",
       " '41',\n",
       " '3246',\n",
       " '2425',\n",
       " '1110',\n",
       " '2376',\n",
       " '1238',\n",
       " '483',\n",
       " '1305',\n",
       " '298',\n",
       " '3584',\n",
       " '3525',\n",
       " '208',\n",
       " '2773',\n",
       " '1892',\n",
       " '3348',\n",
       " '2896',\n",
       " '3427',\n",
       " '2951',\n",
       " '11',\n",
       " '1341',\n",
       " '2843',\n",
       " '2049',\n",
       " '1654',\n",
       " '1836',\n",
       " '2161',\n",
       " '2632',\n",
       " '1256',\n",
       " '371',\n",
       " '2367',\n",
       " '2556',\n",
       " '2885',\n",
       " '2043',\n",
       " '1528',\n",
       " '1482',\n",
       " '329',\n",
       " '2841',\n",
       " '1116',\n",
       " '2671',\n",
       " '1974',\n",
       " '2733',\n",
       " '2401',\n",
       " '2434',\n",
       " '1765',\n",
       " '2093',\n",
       " '325',\n",
       " '2178',\n",
       " '237',\n",
       " '3471',\n",
       " '1839',\n",
       " '3012',\n",
       " '1687',\n",
       " '3316',\n",
       " '2169',\n",
       " '2935',\n",
       " '1693',\n",
       " '3531',\n",
       " '456',\n",
       " '1206',\n",
       " '1409',\n",
       " '1976',\n",
       " '1055',\n",
       " '1744',\n",
       " '2290',\n",
       " '2184',\n",
       " '2918',\n",
       " '23',\n",
       " '1067',\n",
       " '2804',\n",
       " '1391',\n",
       " '1903',\n",
       " '1661',\n",
       " '410',\n",
       " '3047',\n",
       " '1838',\n",
       " '2476',\n",
       " '2998',\n",
       " '3507',\n",
       " '3030',\n",
       " '1250',\n",
       " '2972',\n",
       " '2217',\n",
       " '3286',\n",
       " '2703',\n",
       " '2388',\n",
       " '1398',\n",
       " '3579',\n",
       " '2429',\n",
       " '1071',\n",
       " '2262',\n",
       " '168',\n",
       " '233',\n",
       " '1028',\n",
       " '2658',\n",
       " '2592',\n",
       " '170',\n",
       " '2394',\n",
       " '1134',\n",
       " '2455',\n",
       " '3103',\n",
       " '1968',\n",
       " '1130',\n",
       " '1480',\n",
       " '135',\n",
       " '1592',\n",
       " '3475',\n",
       " '376',\n",
       " '2131',\n",
       " '430',\n",
       " '1860',\n",
       " '340',\n",
       " '1676',\n",
       " '2377',\n",
       " '1136',\n",
       " '2405',\n",
       " '2082',\n",
       " '1235',\n",
       " '2986',\n",
       " '1563',\n",
       " '305',\n",
       " '3425',\n",
       " '1964',\n",
       " '1295',\n",
       " '1328',\n",
       " '2039',\n",
       " '1560',\n",
       " '355',\n",
       " '196',\n",
       " '2823',\n",
       " '2489',\n",
       " '357',\n",
       " '1024',\n",
       " '1378',\n",
       " '1787',\n",
       " '2639',\n",
       " '1477',\n",
       " '2995',\n",
       " '1717',\n",
       " '1655',\n",
       " '2201',\n",
       " '1595',\n",
       " '1125',\n",
       " '1432',\n",
       " '1026',\n",
       " '3117',\n",
       " '2839',\n",
       " '2115',\n",
       " '3244',\n",
       " '2803',\n",
       " '3297',\n",
       " '1545',\n",
       " '177',\n",
       " '2320',\n",
       " '1814',\n",
       " '3591',\n",
       " '2821',\n",
       " '2379',\n",
       " '1861',\n",
       " '2136',\n",
       " '2167',\n",
       " '1012',\n",
       " '1288',\n",
       " '3560',\n",
       " '2046',\n",
       " '327',\n",
       " '1547',\n",
       " '3146',\n",
       " '3127',\n",
       " '2478',\n",
       " '214',\n",
       " '3564',\n",
       " '1638',\n",
       " '320',\n",
       " '2247',\n",
       " '1029',\n",
       " '3378',\n",
       " '2758',\n",
       " '3582',\n",
       " '2358',\n",
       " '3051',\n",
       " '3323',\n",
       " '2255',\n",
       " '1402',\n",
       " '230',\n",
       " '502',\n",
       " '3019',\n",
       " '3037',\n",
       " '3060',\n",
       " '1258',\n",
       " '1113',\n",
       " '1384',\n",
       " '1035',\n",
       " '1640',\n",
       " '2876',\n",
       " '1824',\n",
       " '3086',\n",
       " '1826',\n",
       " '266',\n",
       " '3104',\n",
       " '3448',\n",
       " '2540',\n",
       " '2575',\n",
       " '2404',\n",
       " '1161',\n",
       " '2591',\n",
       " '2035',\n",
       " '1750',\n",
       " '2175',\n",
       " '3229',\n",
       " '3344',\n",
       " '1620',\n",
       " '2503',\n",
       " '2593',\n",
       " '3201',\n",
       " '1383',\n",
       " '2056',\n",
       " '1527',\n",
       " '1186',\n",
       " '2234',\n",
       " '2810',\n",
       " '1083',\n",
       " '120',\n",
       " '2325',\n",
       " '1793',\n",
       " '2746',\n",
       " '1865',\n",
       " '1827',\n",
       " '3196',\n",
       " '32',\n",
       " '2513',\n",
       " '2498',\n",
       " '3208',\n",
       " '2119',\n",
       " '3449',\n",
       " '1509',\n",
       " '1227',\n",
       " '2003',\n",
       " '1580',\n",
       " '2759',\n",
       " '3596',\n",
       " '3167',\n",
       " '2767',\n",
       " '365',\n",
       " '1255',\n",
       " '1891',\n",
       " '2113',\n",
       " '319',\n",
       " '1671',\n",
       " '209',\n",
       " '283',\n",
       " '2977',\n",
       " '145',\n",
       " '275',\n",
       " '2727',\n",
       " '1557',\n",
       " '2020',\n",
       " '1147',\n",
       " '3384',\n",
       " '1439',\n",
       " '3098',\n",
       " '1692',\n",
       " '2070',\n",
       " '2802',\n",
       " '3454',\n",
       " '3028',\n",
       " '2060',\n",
       " '415',\n",
       " '1338',\n",
       " '3478',\n",
       " '2902',\n",
       " '2055',\n",
       " '1304',\n",
       " '3418',\n",
       " '1152',\n",
       " '1034',\n",
       " '1303',\n",
       " '1097',\n",
       " '387',\n",
       " '1452',\n",
       " '2150',\n",
       " '3559',\n",
       " '478',\n",
       " '3069',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3aeb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1977': 0,\n",
       " '2782': 1,\n",
       " '1236': 2,\n",
       " '2546': 3,\n",
       " '1867': 4,\n",
       " '1647': 5,\n",
       " '1458': 6,\n",
       " '1686': 7,\n",
       " '2647': 8,\n",
       " '1504': 9,\n",
       " '1148': 10,\n",
       " '2373': 11,\n",
       " '2246': 12,\n",
       " '3262': 13,\n",
       " '407': 14,\n",
       " '1569': 15,\n",
       " '226': 16,\n",
       " '1663': 17,\n",
       " '3590': 18,\n",
       " '172': 19,\n",
       " '1991': 20,\n",
       " '192': 21,\n",
       " '3183': 22,\n",
       " '183': 23,\n",
       " '1170': 24,\n",
       " '1191': 25,\n",
       " '256': 26,\n",
       " '1618': 27,\n",
       " '1731': 28,\n",
       " '2076': 29,\n",
       " '203': 30,\n",
       " '3264': 31,\n",
       " '1791': 32,\n",
       " '2709': 33,\n",
       " '2362': 34,\n",
       " '2053': 35,\n",
       " '2657': 36,\n",
       " '3002': 37,\n",
       " '1525': 38,\n",
       " '2954': 39,\n",
       " '2820': 40,\n",
       " '2903': 41,\n",
       " '3372': 42,\n",
       " '2719': 43,\n",
       " '1141': 44,\n",
       " '2222': 45,\n",
       " '2730': 46,\n",
       " '265': 47,\n",
       " '348': 48,\n",
       " '1944': 49,\n",
       " '3036': 50,\n",
       " '1443': 51,\n",
       " '1039': 52,\n",
       " '3522': 53,\n",
       " '1414': 54,\n",
       " '1425': 55,\n",
       " '1473': 56,\n",
       " '1697': 57,\n",
       " '2724': 58,\n",
       " '1902': 59,\n",
       " '1355': 60,\n",
       " '2385': 61,\n",
       " '466': 62,\n",
       " '2276': 63,\n",
       " '1015': 64,\n",
       " '3338': 65,\n",
       " '1481': 66,\n",
       " '1433': 67,\n",
       " '2144': 68,\n",
       " '3578': 69,\n",
       " '2529': 70,\n",
       " '1486': 71,\n",
       " '2157': 72,\n",
       " '3501': 73,\n",
       " '1259': 74,\n",
       " '3095': 75,\n",
       " '2240': 76,\n",
       " '2883': 77,\n",
       " '1915': 78,\n",
       " '1004': 79,\n",
       " '1107': 80,\n",
       " '472': 81,\n",
       " '2467': 82,\n",
       " '1490': 83,\n",
       " '2391': 84,\n",
       " '148': 85,\n",
       " '3345': 86,\n",
       " '2387': 87,\n",
       " '413': 88,\n",
       " '2111': 89,\n",
       " '2326': 90,\n",
       " '2694': 91,\n",
       " '2840': 92,\n",
       " '1285': 93,\n",
       " '2279': 94,\n",
       " '1658': 95,\n",
       " '3350': 96,\n",
       " '3524': 97,\n",
       " '1442': 98,\n",
       " '1570': 99,\n",
       " '2428': 100,\n",
       " '2079': 101,\n",
       " '2088': 102,\n",
       " '1153': 103,\n",
       " '2571': 104,\n",
       " '3541': 105,\n",
       " '1718': 106,\n",
       " '2159': 107,\n",
       " '2539': 108,\n",
       " '2744': 109,\n",
       " '2244': 110,\n",
       " '1319': 111,\n",
       " '3364': 112,\n",
       " '3550': 113,\n",
       " '2907': 114,\n",
       " '1773': 115,\n",
       " '1333': 116,\n",
       " '112': 117,\n",
       " '3407': 118,\n",
       " '121': 119,\n",
       " '272': 120,\n",
       " '2045': 121,\n",
       " '188': 122,\n",
       " '2139': 123,\n",
       " '171': 124,\n",
       " '1403': 125,\n",
       " '1101': 126,\n",
       " '1277': 127,\n",
       " '2656': 128,\n",
       " '3567': 129,\n",
       " '2106': 130,\n",
       " '3294': 131,\n",
       " '251': 132,\n",
       " '1696': 133,\n",
       " '2801': 134,\n",
       " '1082': 135,\n",
       " '2348': 136,\n",
       " '1667': 137,\n",
       " '229': 138,\n",
       " '3486': 139,\n",
       " '2143': 140,\n",
       " '17': 141,\n",
       " '1353': 142,\n",
       " '1224': 143,\n",
       " '2566': 144,\n",
       " '454': 145,\n",
       " '2655': 146,\n",
       " '352': 147,\n",
       " '1740': 148,\n",
       " '2772': 149,\n",
       " '3007': 150,\n",
       " '1322': 151,\n",
       " '3552': 152,\n",
       " '1517': 153,\n",
       " '1691': 154,\n",
       " '279': 155,\n",
       " '1633': 156,\n",
       " '1776': 157,\n",
       " '2218': 158,\n",
       " '222': 159,\n",
       " '1387': 160,\n",
       " '1485': 161,\n",
       " '3181': 162,\n",
       " '3429': 163,\n",
       " '2108': 164,\n",
       " '1172': 165,\n",
       " '1331': 166,\n",
       " '1061': 167,\n",
       " '206': 168,\n",
       " '1713': 169,\n",
       " '3343': 170,\n",
       " '2004': 171,\n",
       " '1519': 172,\n",
       " '2522': 173,\n",
       " '2948': 174,\n",
       " '3109': 175,\n",
       " '1662': 176,\n",
       " '2006': 177,\n",
       " '2417': 178,\n",
       " '3101': 179,\n",
       " '1565': 180,\n",
       " '2177': 181,\n",
       " '1019': 182,\n",
       " '3049': 183,\n",
       " '1755': 184,\n",
       " '2845': 185,\n",
       " '2650': 186,\n",
       " '2731': 187,\n",
       " '2747': 188,\n",
       " '1802': 189,\n",
       " '2551': 190,\n",
       " '2749': 191,\n",
       " '146': 192,\n",
       " '1023': 193,\n",
       " '264': 194,\n",
       " '38': 195,\n",
       " '2226': 196,\n",
       " '1450': 197,\n",
       " '1900': 198,\n",
       " '2015': 199,\n",
       " '27': 200,\n",
       " '343': 201,\n",
       " '1245': 202,\n",
       " '1297': 203,\n",
       " '1936': 204,\n",
       " '299': 205,\n",
       " '1942': 206,\n",
       " '3158': 207,\n",
       " '2171': 208,\n",
       " '2860': 209,\n",
       " '2190': 210,\n",
       " '1782': 211,\n",
       " '1901': 212,\n",
       " '2921': 213,\n",
       " '2750': 214,\n",
       " '1027': 215,\n",
       " '1806': 216,\n",
       " '3091': 217,\n",
       " '102': 218,\n",
       " '3594': 219,\n",
       " '3057': 220,\n",
       " '2942': 221,\n",
       " '3342': 222,\n",
       " '1842': 223,\n",
       " '3066': 224,\n",
       " '3276': 225,\n",
       " '2054': 226,\n",
       " '335': 227,\n",
       " '2602': 228,\n",
       " '2034': 229,\n",
       " '1121': 230,\n",
       " '1117': 231,\n",
       " '1218': 232,\n",
       " '1578': 233,\n",
       " '351': 234,\n",
       " '2158': 235,\n",
       " '2297': 236,\n",
       " '2720': 237,\n",
       " '2314': 238,\n",
       " '2613': 239,\n",
       " '3451': 240,\n",
       " '2212': 241,\n",
       " '1631': 242,\n",
       " '2920': 243,\n",
       " '1871': 244,\n",
       " '2927': 245,\n",
       " '1666': 246,\n",
       " '3481': 247,\n",
       " '3128': 248,\n",
       " '3375': 249,\n",
       " '1890': 250,\n",
       " '3517': 251,\n",
       " '140': 252,\n",
       " '3558': 253,\n",
       " '2738': 254,\n",
       " '1612': 255,\n",
       " '3527': 256,\n",
       " '3255': 257,\n",
       " '1952': 258,\n",
       " '1737': 259,\n",
       " '2619': 260,\n",
       " '1855': 261,\n",
       " '2824': 262,\n",
       " '1924': 263,\n",
       " '3497': 264,\n",
       " '2209': 265,\n",
       " '1254': 266,\n",
       " '1492': 267,\n",
       " '2187': 268,\n",
       " '2147': 269,\n",
       " '1126': 270,\n",
       " '30': 271,\n",
       " '1732': 272,\n",
       " '2836': 273,\n",
       " '3282': 274,\n",
       " '2061': 275,\n",
       " '1051': 276,\n",
       " '1309': 277,\n",
       " '3256': 278,\n",
       " '1135': 279,\n",
       " '2926': 280,\n",
       " '3061': 281,\n",
       " '1550': 282,\n",
       " '1394': 283,\n",
       " '2432': 284,\n",
       " '1335': 285,\n",
       " '2114': 286,\n",
       " '2188': 287,\n",
       " '2736': 288,\n",
       " '2600': 289,\n",
       " '1876': 290,\n",
       " '1427': 291,\n",
       " '2507': 292,\n",
       " '3409': 293,\n",
       " '2623': 294,\n",
       " '3260': 295,\n",
       " '2197': 296,\n",
       " '48': 297,\n",
       " '1847': 298,\n",
       " '3365': 299,\n",
       " '3521': 300,\n",
       " '1833': 301,\n",
       " '1212': 302,\n",
       " '2880': 303,\n",
       " '2375': 304,\n",
       " '1883': 305,\n",
       " '288': 306,\n",
       " '2189': 307,\n",
       " '1190': 308,\n",
       " '1342': 309,\n",
       " '304': 310,\n",
       " '1242': 311,\n",
       " '2024': 312,\n",
       " '3004': 313,\n",
       " '2032': 314,\n",
       " '2050': 315,\n",
       " '1874': 316,\n",
       " '1553': 317,\n",
       " '161': 318,\n",
       " '3452': 319,\n",
       " '1469': 320,\n",
       " '1194': 321,\n",
       " '2257': 322,\n",
       " '1854': 323,\n",
       " '3357': 324,\n",
       " '1653': 325,\n",
       " '1798': 326,\n",
       " '2634': 327,\n",
       " '2543': 328,\n",
       " '2669': 329,\n",
       " '2561': 330,\n",
       " '3116': 331,\n",
       " '1704': 332,\n",
       " '2173': 333,\n",
       " '2858': 334,\n",
       " '1088': 335,\n",
       " '2991': 336,\n",
       " '1683': 337,\n",
       " '3026': 338,\n",
       " '3192': 339,\n",
       " '3013': 340,\n",
       " '2664': 341,\n",
       " '33': 342,\n",
       " '159': 343,\n",
       " '3141': 344,\n",
       " '3277': 345,\n",
       " '2356': 346,\n",
       " '1513': 347,\n",
       " '3392': 348,\n",
       " '2347': 349,\n",
       " '3490': 350,\n",
       " '3402': 351,\n",
       " '1888': 352,\n",
       " '1783': 353,\n",
       " '1202': 354,\n",
       " '2353': 355,\n",
       " '1270': 356,\n",
       " '241': 357,\n",
       " '2848': 358,\n",
       " '42': 359,\n",
       " '270': 360,\n",
       " '3431': 361,\n",
       " '1307': 362,\n",
       " '457': 363,\n",
       " '310': 364,\n",
       " '2446': 365,\n",
       " '2063': 366,\n",
       " '2291': 367,\n",
       " '2967': 368,\n",
       " '3198': 369,\n",
       " '1086': 370,\n",
       " '249': 371,\n",
       " '487': 372,\n",
       " '3153': 373,\n",
       " '1047': 374,\n",
       " '1334': 375,\n",
       " '2122': 376,\n",
       " '14': 377,\n",
       " '2141': 378,\n",
       " '2396': 379,\n",
       " '3414': 380,\n",
       " '1008': 381,\n",
       " '1652': 382,\n",
       " '3416': 383,\n",
       " '1145': 384,\n",
       " '1675': 385,\n",
       " '291': 386,\n",
       " '1775': 387,\n",
       " '1324': 388,\n",
       " '2398': 389,\n",
       " '132': 390,\n",
       " '2864': 391,\n",
       " '1934': 392,\n",
       " '3306': 393,\n",
       " '1318': 394,\n",
       " '2294': 395,\n",
       " '2340': 396,\n",
       " '3332': 397,\n",
       " '2030': 398,\n",
       " '152': 399,\n",
       " '2628': 400,\n",
       " '3540': 401,\n",
       " '1698': 402,\n",
       " '2900': 403,\n",
       " '2586': 404,\n",
       " '114': 405,\n",
       " '1471': 406,\n",
       " '1484': 407,\n",
       " '373': 408,\n",
       " '1456': 409,\n",
       " '2180': 410,\n",
       " '463': 411,\n",
       " '3437': 412,\n",
       " '2945': 413,\n",
       " '1286': 414,\n",
       " '1420': 415,\n",
       " '3075': 416,\n",
       " '2409': 417,\n",
       " '1932': 418,\n",
       " '1444': 419,\n",
       " '1939': 420,\n",
       " '1745': 421,\n",
       " '1743': 422,\n",
       " '2378': 423,\n",
       " '2828': 424,\n",
       " '2110': 425,\n",
       " '2545': 426,\n",
       " '2721': 427,\n",
       " '139': 428,\n",
       " '1695': 429,\n",
       " '1476': 430,\n",
       " '1724': 431,\n",
       " '399': 432,\n",
       " '374': 433,\n",
       " '2581': 434,\n",
       " '3010': 435,\n",
       " '1830': 436,\n",
       " '3275': 437,\n",
       " '3156': 438,\n",
       " '2072': 439,\n",
       " '1156': 440,\n",
       " '1817': 441,\n",
       " '1734': 442,\n",
       " '1099': 443,\n",
       " '3460': 444,\n",
       " '1009': 445,\n",
       " '3510': 446,\n",
       " '1114': 447,\n",
       " '1719': 448,\n",
       " '1796': 449,\n",
       " '3063': 450,\n",
       " '3143': 451,\n",
       " '293': 452,\n",
       " '484': 453,\n",
       " '338': 454,\n",
       " '2041': 455,\n",
       " '1188': 456,\n",
       " '2210': 457,\n",
       " '1729': 458,\n",
       " '3371': 459,\n",
       " '2357': 460,\n",
       " '2642': 461,\n",
       " '2127': 462,\n",
       " '3538': 463,\n",
       " '2960': 464,\n",
       " '2254': 465,\n",
       " '2327': 466,\n",
       " '3381': 467,\n",
       " '427': 468,\n",
       " '3467': 469,\n",
       " '3241': 470,\n",
       " '1628': 471,\n",
       " '2207': 472,\n",
       " '401': 473,\n",
       " '3528': 474,\n",
       " '2463': 475,\n",
       " '3014': 476,\n",
       " '1749': 477,\n",
       " '1822': 478,\n",
       " '2697': 479,\n",
       " '2018': 480,\n",
       " '2913': 481,\n",
       " '2973': 482,\n",
       " '2413': 483,\n",
       " '1753': 484,\n",
       " '2264': 485,\n",
       " '1044': 486,\n",
       " '1584': 487,\n",
       " '2293': 488,\n",
       " '1777': 489,\n",
       " '3395': 490,\n",
       " '1247': 491,\n",
       " '2531': 492,\n",
       " '3423': 493,\n",
       " '1657': 494,\n",
       " '247': 495,\n",
       " '3575': 496,\n",
       " '200': 497,\n",
       " '3568': 498,\n",
       " '1410': 499,\n",
       " '3113': 500,\n",
       " '3505': 501,\n",
       " '2205': 502,\n",
       " '2646': 503,\n",
       " '1778': 504,\n",
       " '2275': 505,\n",
       " '3503': 506,\n",
       " '1020': 507,\n",
       " '3577': 508,\n",
       " '2443': 509,\n",
       " '465': 510,\n",
       " '2372': 511,\n",
       " '136': 512,\n",
       " '167': 513,\n",
       " '2448': 514,\n",
       " '2194': 515,\n",
       " '1163': 516,\n",
       " '1349': 517,\n",
       " '362': 518,\n",
       " '1603': 519,\n",
       " '1841': 520,\n",
       " '2771': 521,\n",
       " '3464': 522,\n",
       " '1621': 523,\n",
       " '3178': 524,\n",
       " '3461': 525,\n",
       " '1408': 526,\n",
       " '3100': 527,\n",
       " '1232': 528,\n",
       " '2511': 529,\n",
       " '1308': 530,\n",
       " '2155': 531,\n",
       " '3440': 532,\n",
       " '3386': 533,\n",
       " '1025': 534,\n",
       " '3515': 535,\n",
       " '2390': 536,\n",
       " '1221': 537,\n",
       " '2027': 538,\n",
       " '2242': 539,\n",
       " '3339': 540,\n",
       " '398': 541,\n",
       " '2306': 542,\n",
       " '461': 543,\n",
       " '1261': 544,\n",
       " '375': 545,\n",
       " '2596': 546,\n",
       " '2492': 547,\n",
       " '2007': 548,\n",
       " '2313': 549,\n",
       " '3270': 550,\n",
       " '2249': 551,\n",
       " '3278': 552,\n",
       " '1041': 553,\n",
       " '1500': 554,\n",
       " '2295': 555,\n",
       " '3518': 556,\n",
       " '3133': 557,\n",
       " '2454': 558,\n",
       " '3265': 559,\n",
       " '1445': 560,\n",
       " '2224': 561,\n",
       " '1970': 562,\n",
       " '426': 563,\n",
       " '467': 564,\n",
       " '1368': 565,\n",
       " '1937': 566,\n",
       " '419': 567,\n",
       " '2524': 568,\n",
       " '2399': 569,\n",
       " '1870': 570,\n",
       " '3436': 571,\n",
       " '1146': 572,\n",
       " '388': 573,\n",
       " '1042': 574,\n",
       " '2608': 575,\n",
       " '1098': 576,\n",
       " '3219': 577,\n",
       " '1611': 578,\n",
       " '1311': 579,\n",
       " '2852': 580,\n",
       " '1877': 581,\n",
       " '2648': 582,\n",
       " '412': 583,\n",
       " '1850': 584,\n",
       " '1155': 585,\n",
       " '1797': 586,\n",
       " '3190': 587,\n",
       " '2838': 588,\n",
       " '1371': 589,\n",
       " '2521': 590,\n",
       " '1812': 591,\n",
       " '2485': 592,\n",
       " '2331': 593,\n",
       " '2253': 594,\n",
       " '3126': 595,\n",
       " '2068': 596,\n",
       " '1105': 597,\n",
       " '322': 598,\n",
       " '282': 599,\n",
       " '2424': 600,\n",
       " '3573': 601,\n",
       " '2874': 602,\n",
       " '3068': 603,\n",
       " '3508': 604,\n",
       " '1979': 605,\n",
       " '1945': 606,\n",
       " '424': 607,\n",
       " '3096': 608,\n",
       " '3235': 609,\n",
       " '2423': 610,\n",
       " '1196': 611,\n",
       " '3076': 612,\n",
       " '1639': 613,\n",
       " '2762': 614,\n",
       " '475': 615,\n",
       " '1195': 616,\n",
       " '2956': 617,\n",
       " '3382': 618,\n",
       " '186': 619,\n",
       " '1586': 620,\n",
       " '1175': 621,\n",
       " '447': 622,\n",
       " '1511': 623,\n",
       " '324': 624,\n",
       " '2859': 625,\n",
       " '3152': 626,\n",
       " '1917': 627,\n",
       " '1493': 628,\n",
       " '3245': 629,\n",
       " '2270': 630,\n",
       " '3543': 631,\n",
       " '488': 632,\n",
       " '3356': 633,\n",
       " '2073': 634,\n",
       " '3018': 635,\n",
       " '1220': 636,\n",
       " '2784': 637,\n",
       " '1894': 638,\n",
       " '1118': 639,\n",
       " '1809': 640,\n",
       " '3466': 641,\n",
       " '3195': 642,\n",
       " '3085': 643,\n",
       " '3445': 644,\n",
       " '1710': 645,\n",
       " '2690': 646,\n",
       " '1237': 647,\n",
       " '157': 648,\n",
       " '234': 649,\n",
       " '1068': 650,\n",
       " '201': 651,\n",
       " '3355': 652,\n",
       " '2542': 653,\n",
       " '2156': 654,\n",
       " '1637': 655,\n",
       " '280': 656,\n",
       " '1167': 657,\n",
       " '3433': 658,\n",
       " '1885': 659,\n",
       " '2220': 660,\n",
       " '3379': 661,\n",
       " '2096': 662,\n",
       " '2761': 663,\n",
       " '1316': 664,\n",
       " '2316': 665,\n",
       " '1625': 666,\n",
       " '3556': 667,\n",
       " '1198': 668,\n",
       " '471': 669,\n",
       " '1060': 670,\n",
       " '1954': 671,\n",
       " '147': 672,\n",
       " '29': 673,\n",
       " '3058': 674,\n",
       " '2058': 675,\n",
       " '1112': 676,\n",
       " '1363': 677,\n",
       " '106': 678,\n",
       " '2788': 679,\n",
       " '1507': 680,\n",
       " '2764': 681,\n",
       " '2365': 682,\n",
       " '2368': 683,\n",
       " '422': 684,\n",
       " '5': 685,\n",
       " '2981': 686,\n",
       " '1540': 687,\n",
       " '1590': 688,\n",
       " '300': 689,\n",
       " '2002': 690,\n",
       " '2822': 691,\n",
       " '15': 692,\n",
       " '276': 693,\n",
       " '1472': 694,\n",
       " '2483': 695,\n",
       " '3123': 696,\n",
       " '277': 697,\n",
       " '2518': 698,\n",
       " '1680': 699,\n",
       " '2559': 700,\n",
       " '1385': 701,\n",
       " '2280': 702,\n",
       " '2816': 703,\n",
       " '19': 704,\n",
       " '3363': 705,\n",
       " '1572': 706,\n",
       " '1248': 707,\n",
       " '2704': 708,\n",
       " '2812': 709,\n",
       " '3498': 710,\n",
       " '2089': 711,\n",
       " '2493': 712,\n",
       " '2292': 713,\n",
       " '126': 714,\n",
       " '2059': 715,\n",
       " '1610': 716,\n",
       " '3191': 717,\n",
       " '41': 718,\n",
       " '3246': 719,\n",
       " '2425': 720,\n",
       " '1110': 721,\n",
       " '2376': 722,\n",
       " '1238': 723,\n",
       " '483': 724,\n",
       " '1305': 725,\n",
       " '298': 726,\n",
       " '3584': 727,\n",
       " '3525': 728,\n",
       " '208': 729,\n",
       " '2773': 730,\n",
       " '1892': 731,\n",
       " '3348': 732,\n",
       " '2896': 733,\n",
       " '3427': 734,\n",
       " '2951': 735,\n",
       " '11': 736,\n",
       " '1341': 737,\n",
       " '2843': 738,\n",
       " '2049': 739,\n",
       " '1654': 740,\n",
       " '1836': 741,\n",
       " '2161': 742,\n",
       " '2632': 743,\n",
       " '1256': 744,\n",
       " '371': 745,\n",
       " '2367': 746,\n",
       " '2556': 747,\n",
       " '2885': 748,\n",
       " '2043': 749,\n",
       " '1528': 750,\n",
       " '1482': 751,\n",
       " '329': 752,\n",
       " '2841': 753,\n",
       " '1116': 754,\n",
       " '2671': 755,\n",
       " '1974': 756,\n",
       " '2733': 757,\n",
       " '2401': 758,\n",
       " '2434': 759,\n",
       " '1765': 760,\n",
       " '2093': 761,\n",
       " '325': 762,\n",
       " '2178': 763,\n",
       " '237': 764,\n",
       " '3471': 765,\n",
       " '1839': 766,\n",
       " '3012': 767,\n",
       " '1687': 768,\n",
       " '3316': 769,\n",
       " '2169': 770,\n",
       " '2935': 771,\n",
       " '1693': 772,\n",
       " '3531': 773,\n",
       " '456': 774,\n",
       " '1206': 775,\n",
       " '1409': 776,\n",
       " '1976': 777,\n",
       " '1055': 778,\n",
       " '1744': 779,\n",
       " '2290': 780,\n",
       " '2184': 781,\n",
       " '2918': 782,\n",
       " '23': 783,\n",
       " '1067': 784,\n",
       " '2804': 785,\n",
       " '1391': 786,\n",
       " '1903': 787,\n",
       " '1661': 788,\n",
       " '410': 789,\n",
       " '3047': 790,\n",
       " '1838': 791,\n",
       " '2476': 792,\n",
       " '2998': 793,\n",
       " '3507': 794,\n",
       " '3030': 795,\n",
       " '1250': 796,\n",
       " '2972': 797,\n",
       " '2217': 798,\n",
       " '3286': 799,\n",
       " '2703': 800,\n",
       " '2388': 801,\n",
       " '1398': 802,\n",
       " '3579': 803,\n",
       " '2429': 804,\n",
       " '1071': 805,\n",
       " '2262': 806,\n",
       " '168': 807,\n",
       " '233': 808,\n",
       " '1028': 809,\n",
       " '2658': 810,\n",
       " '2592': 811,\n",
       " '170': 812,\n",
       " '2394': 813,\n",
       " '1134': 814,\n",
       " '2455': 815,\n",
       " '3103': 816,\n",
       " '1968': 817,\n",
       " '1130': 818,\n",
       " '1480': 819,\n",
       " '135': 820,\n",
       " '1592': 821,\n",
       " '3475': 822,\n",
       " '376': 823,\n",
       " '2131': 824,\n",
       " '430': 825,\n",
       " '1860': 826,\n",
       " '340': 827,\n",
       " '1676': 828,\n",
       " '2377': 829,\n",
       " '1136': 830,\n",
       " '2405': 831,\n",
       " '2082': 832,\n",
       " '1235': 833,\n",
       " '2986': 834,\n",
       " '1563': 835,\n",
       " '305': 836,\n",
       " '3425': 837,\n",
       " '1964': 838,\n",
       " '1295': 839,\n",
       " '1328': 840,\n",
       " '2039': 841,\n",
       " '1560': 842,\n",
       " '355': 843,\n",
       " '196': 844,\n",
       " '2823': 845,\n",
       " '2489': 846,\n",
       " '357': 847,\n",
       " '1024': 848,\n",
       " '1378': 849,\n",
       " '1787': 850,\n",
       " '2639': 851,\n",
       " '1477': 852,\n",
       " '2995': 853,\n",
       " '1717': 854,\n",
       " '1655': 855,\n",
       " '2201': 856,\n",
       " '1595': 857,\n",
       " '1125': 858,\n",
       " '1432': 859,\n",
       " '1026': 860,\n",
       " '3117': 861,\n",
       " '2839': 862,\n",
       " '2115': 863,\n",
       " '3244': 864,\n",
       " '2803': 865,\n",
       " '3297': 866,\n",
       " '1545': 867,\n",
       " '177': 868,\n",
       " '2320': 869,\n",
       " '1814': 870,\n",
       " '3591': 871,\n",
       " '2821': 872,\n",
       " '2379': 873,\n",
       " '1861': 874,\n",
       " '2136': 875,\n",
       " '2167': 876,\n",
       " '1012': 877,\n",
       " '1288': 878,\n",
       " '3560': 879,\n",
       " '2046': 880,\n",
       " '327': 881,\n",
       " '1547': 882,\n",
       " '3146': 883,\n",
       " '3127': 884,\n",
       " '2478': 885,\n",
       " '214': 886,\n",
       " '3564': 887,\n",
       " '1638': 888,\n",
       " '320': 889,\n",
       " '2247': 890,\n",
       " '1029': 891,\n",
       " '3378': 892,\n",
       " '2758': 893,\n",
       " '3582': 894,\n",
       " '2358': 895,\n",
       " '3051': 896,\n",
       " '3323': 897,\n",
       " '2255': 898,\n",
       " '1402': 899,\n",
       " '230': 900,\n",
       " '502': 901,\n",
       " '3019': 902,\n",
       " '3037': 903,\n",
       " '3060': 904,\n",
       " '1258': 905,\n",
       " '1113': 906,\n",
       " '1384': 907,\n",
       " '1035': 908,\n",
       " '1640': 909,\n",
       " '2876': 910,\n",
       " '1824': 911,\n",
       " '3086': 912,\n",
       " '1826': 913,\n",
       " '266': 914,\n",
       " '3104': 915,\n",
       " '3448': 916,\n",
       " '2540': 917,\n",
       " '2575': 918,\n",
       " '2404': 919,\n",
       " '1161': 920,\n",
       " '2591': 921,\n",
       " '2035': 922,\n",
       " '1750': 923,\n",
       " '2175': 924,\n",
       " '3229': 925,\n",
       " '3344': 926,\n",
       " '1620': 927,\n",
       " '2503': 928,\n",
       " '2593': 929,\n",
       " '3201': 930,\n",
       " '1383': 931,\n",
       " '2056': 932,\n",
       " '1527': 933,\n",
       " '1186': 934,\n",
       " '2234': 935,\n",
       " '2810': 936,\n",
       " '1083': 937,\n",
       " '120': 938,\n",
       " '2325': 939,\n",
       " '1793': 940,\n",
       " '2746': 941,\n",
       " '1865': 942,\n",
       " '1827': 943,\n",
       " '3196': 944,\n",
       " '32': 945,\n",
       " '2513': 946,\n",
       " '2498': 947,\n",
       " '3208': 948,\n",
       " '2119': 949,\n",
       " '3449': 950,\n",
       " '1509': 951,\n",
       " '1227': 952,\n",
       " '2003': 953,\n",
       " '1580': 954,\n",
       " '2759': 955,\n",
       " '3596': 956,\n",
       " '3167': 957,\n",
       " '2767': 958,\n",
       " '365': 959,\n",
       " '1255': 960,\n",
       " '1891': 961,\n",
       " '2113': 962,\n",
       " '319': 963,\n",
       " '1671': 964,\n",
       " '209': 965,\n",
       " '283': 966,\n",
       " '2977': 967,\n",
       " '145': 968,\n",
       " '275': 969,\n",
       " '2727': 970,\n",
       " '1557': 971,\n",
       " '2020': 972,\n",
       " '1147': 973,\n",
       " '3384': 974,\n",
       " '1439': 975,\n",
       " '3098': 976,\n",
       " '1692': 977,\n",
       " '2070': 978,\n",
       " '2802': 979,\n",
       " '3454': 980,\n",
       " '3028': 981,\n",
       " '2060': 982,\n",
       " '415': 983,\n",
       " '1338': 984,\n",
       " '3478': 985,\n",
       " '2902': 986,\n",
       " '2055': 987,\n",
       " '1304': 988,\n",
       " '3418': 989,\n",
       " '1152': 990,\n",
       " '1034': 991,\n",
       " '1303': 992,\n",
       " '1097': 993,\n",
       " '387': 994,\n",
       " '1452': 995,\n",
       " '2150': 996,\n",
       " '3559': 997,\n",
       " '478': 998,\n",
       " '3069': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d132ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e34e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classe = len(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9b07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba37ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.dir_list = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.files = []\n",
    "        for folder in self.dir_list:\n",
    "          folder_list = os.listdir(os.path.join(img_dir, folder))\n",
    "          for file in folder_list:\n",
    "            self.files.append([\n",
    "                os.path.join(self.img_dir, folder+'/'+file),\n",
    "                folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.files[idx][0])\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        label = self.files[idx][1]\n",
    "        label = lb[label]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb1b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainset = CustomDataset(train_path, trans)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = CustomDataset(test_path, trans)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ad169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ef55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc6eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size, margin, scale):\n",
    "        \"\"\"\n",
    "        ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "        (https://arxiv.org/pdf/1801.07698.pdf)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "            margin: m in the paper, the angular margin penalty in radians\n",
    "            scale: s in the paper, feature scale\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        cosine = self.get_cosine(embeddings) # (None, n_classes)\n",
    "        mask = self.get_target_mask(labels) # (None, n_classes)\n",
    "        cosine_of_target_classes = cosine[mask == 1] # (None, )\n",
    "        modified_cosine_of_target_classes = self.modify_cosine_of_target_classes(\n",
    "            cosine_of_target_classes\n",
    "        ) # (None, )\n",
    "        diff = (modified_cosine_of_target_classes - cosine_of_target_classes).unsqueeze(1) # (None,1)\n",
    "        logits = cosine + (mask * diff) # (None, n_classes)\n",
    "        logits = self.scale_logits(logits) # (None, n_classes)\n",
    "        return logits\n",
    "        \n",
    "    def get_cosine(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "        Returns:\n",
    "            cosine: (None, n_classes)\n",
    "        \"\"\"\n",
    "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.W))\n",
    "        return cosine\n",
    "    \n",
    "    def get_target_mask(self, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            mask: (None, n_classes)\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        onehot = torch.zeros(batch_size, self.num_classes, device=labels.device)\n",
    "        onehot.scatter_(1, labels.unsqueeze(-1), 1)\n",
    "        return onehot\n",
    "        \n",
    "    def modify_cosine_of_target_classes(self, cosine_of_target_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cosine_of_target_classes: (None,)\n",
    "        Returns:\n",
    "            modified_cosine_of_target_classes: (None,)\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        # theta in the paper\n",
    "        angles = torch.acos(torch.clamp(cosine_of_target_classes, -1 + eps, 1 - eps))\n",
    "        return torch.cos(angles + self.margin)\n",
    "    \n",
    "    def scale_logits(self, logits):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (None, n_classes)\n",
    "        Returns:\n",
    "            scaled_logits: (None, n_classes)\n",
    "        \"\"\"\n",
    "        return logits * self.scale\n",
    "    \n",
    "class SoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size):\n",
    "        \"\"\"\n",
    "        Regular softmax loss (1 fc layer without bias + CrossEntropyLoss)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        logits = F.linear(embeddings, self.W)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f3c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet50(pretrained = False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Linear(1000, embedding_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.model(images)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.classifier(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3128edcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Batch: 4032 - Loss: 17.361507 - Time:12.214573621749878\n",
      "Epoch: 0 - Batch: 8128 - Loss: 17.051888 - Time:21.63335609436035\n",
      "Epoch: 0 - Batch: 12224 - Loss: 17.179497 - Time:31.055153846740723\n",
      "Epoch: 0 - Batch: 16320 - Loss: 17.309452 - Time:40.4832181930542\n",
      "Epoch: 0 - Batch: 20416 - Loss: 17.119143 - Time:49.915668964385986\n",
      "Epoch: 0 - Batch: 24512 - Loss: 17.253811 - Time:59.37677884101868\n",
      "Epoch: 0 - Batch: 28608 - Loss: 17.187641 - Time:68.81080675125122\n",
      "Epoch: 0 - Batch: 32704 - Loss: 16.971745 - Time:78.2370913028717\n",
      "Epoch: 0 - Batch: 36800 - Loss: 16.974154 - Time:87.66201162338257\n",
      "Epoch: 0 - Batch: 40896 - Loss: 16.931511 - Time:97.10685849189758\n",
      "Epoch: 0 - Batch: 44992 - Loss: 17.151213 - Time:106.5352783203125\n",
      "Epoch: 0 - Batch: 49088 - Loss: 17.150532 - Time:115.96126341819763\n",
      "Epoch: 0 - Batch: 53184 - Loss: 16.965466 - Time:125.39626431465149\n",
      "Epoch: 0 - Batch: 57280 - Loss: 17.024460 - Time:134.81561160087585\n",
      "Epoch: 0 - Batch: 61376 - Loss: 17.164036 - Time:144.23131108283997\n",
      "Epoch: 0 - Batch: 65472 - Loss: 17.263826 - Time:153.65168476104736\n",
      "Epoch: 0 - Batch: 69568 - Loss: 17.115307 - Time:163.06812024116516\n",
      "Epoch: 0 - Batch: 73664 - Loss: 17.232107 - Time:172.48443055152893\n",
      "Epoch: 0 - Batch: 77760 - Loss: 17.049942 - Time:181.89983558654785\n",
      "Epoch: 0 - Batch: 81856 - Loss: 17.096001 - Time:191.3150327205658\n",
      "Epoch: 0 - Batch: 85952 - Loss: 16.915852 - Time:200.74818968772888\n",
      "Epoch: 0 - Batch: 90048 - Loss: 17.189680 - Time:210.15981030464172\n",
      "Epoch: 0 - Batch: 94144 - Loss: 17.032007 - Time:219.57484698295593\n",
      "Epoch: 0 - Batch: 98240 - Loss: 17.216423 - Time:228.98968958854675\n",
      "Epoch: 0 - Batch: 102336 - Loss: 17.230677 - Time:238.42029190063477\n",
      "Epoch: 0 - Batch: 106432 - Loss: 16.891243 - Time:247.83339834213257\n",
      "Epoch: 0 - Batch: 110528 - Loss: 16.971413 - Time:257.2451491355896\n",
      "Epoch: 0 - Batch: 114624 - Loss: 17.056309 - Time:266.6750557422638\n",
      "Epoch: 0 - Batch: 118720 - Loss: 17.033518 - Time:276.0873317718506\n",
      "Epoch: 0 - Batch: 122816 - Loss: 17.250505 - Time:285.4992182254791\n",
      "Epoch: 0 - Batch: 126912 - Loss: 17.001270 - Time:294.9109683036804\n",
      "Epoch: 0 - Batch: 131008 - Loss: 17.101564 - Time:304.32313418388367\n",
      "Epoch: 0 - Batch: 135104 - Loss: 17.109009 - Time:313.73526549339294\n",
      "Epoch: 0 - Batch: 139200 - Loss: 17.294636 - Time:323.1480357646942\n",
      "Epoch: 0 - Batch: 143296 - Loss: 17.116104 - Time:332.5601279735565\n",
      "Epoch: 0 - Batch: 147392 - Loss: 17.079330 - Time:341.99339175224304\n",
      "Epoch: 0 - Batch: 151488 - Loss: 16.827478 - Time:351.40737199783325\n",
      "Epoch: 1 - Batch: 4032 - Loss: 16.986668 - Time:9.575132131576538\n",
      "Epoch: 1 - Batch: 8128 - Loss: 16.893116 - Time:18.9870765209198\n",
      "Epoch: 1 - Batch: 12224 - Loss: 17.158710 - Time:28.39985728263855\n",
      "Epoch: 1 - Batch: 16320 - Loss: 17.042175 - Time:37.81441330909729\n",
      "Epoch: 1 - Batch: 20416 - Loss: 16.807238 - Time:47.25364089012146\n",
      "Epoch: 1 - Batch: 24512 - Loss: 17.036476 - Time:56.66528105735779\n",
      "Epoch: 1 - Batch: 28608 - Loss: 16.908241 - Time:66.0765495300293\n",
      "Epoch: 1 - Batch: 32704 - Loss: 17.006468 - Time:75.48670387268066\n",
      "Epoch: 1 - Batch: 36800 - Loss: 16.943703 - Time:84.91582298278809\n",
      "Epoch: 1 - Batch: 40896 - Loss: 16.816128 - Time:94.32771444320679\n",
      "Epoch: 1 - Batch: 44992 - Loss: 16.868036 - Time:103.73943567276001\n",
      "Epoch: 1 - Batch: 49088 - Loss: 16.865374 - Time:113.16979813575745\n",
      "Epoch: 1 - Batch: 53184 - Loss: 16.873978 - Time:122.58076024055481\n",
      "Epoch: 1 - Batch: 57280 - Loss: 16.798288 - Time:131.9928333759308\n",
      "Epoch: 1 - Batch: 61376 - Loss: 16.797075 - Time:141.40430235862732\n",
      "Epoch: 1 - Batch: 65472 - Loss: 16.842348 - Time:150.81533575057983\n",
      "Epoch: 1 - Batch: 69568 - Loss: 16.943663 - Time:160.22685194015503\n",
      "Epoch: 1 - Batch: 73664 - Loss: 16.894920 - Time:169.63794803619385\n",
      "Epoch: 1 - Batch: 77760 - Loss: 16.869614 - Time:179.047945022583\n",
      "Epoch: 1 - Batch: 81856 - Loss: 16.801712 - Time:188.47668552398682\n",
      "Epoch: 1 - Batch: 85952 - Loss: 16.786190 - Time:197.88776111602783\n",
      "Epoch: 1 - Batch: 90048 - Loss: 16.755342 - Time:207.29973530769348\n",
      "Epoch: 1 - Batch: 94144 - Loss: 16.857862 - Time:216.7122962474823\n",
      "Epoch: 1 - Batch: 98240 - Loss: 16.857327 - Time:226.14122486114502\n",
      "Epoch: 1 - Batch: 102336 - Loss: 16.898396 - Time:235.55071306228638\n",
      "Epoch: 1 - Batch: 106432 - Loss: 17.011158 - Time:244.96299767494202\n",
      "Epoch: 1 - Batch: 110528 - Loss: 16.869408 - Time:254.3924481868744\n",
      "Epoch: 1 - Batch: 114624 - Loss: 16.592613 - Time:263.80314803123474\n",
      "Epoch: 1 - Batch: 118720 - Loss: 16.838800 - Time:273.2154483795166\n",
      "Epoch: 1 - Batch: 122816 - Loss: 16.828594 - Time:282.62747621536255\n",
      "Epoch: 1 - Batch: 126912 - Loss: 16.596605 - Time:292.04059076309204\n",
      "Epoch: 1 - Batch: 131008 - Loss: 16.775553 - Time:301.45238518714905\n",
      "Epoch: 1 - Batch: 135104 - Loss: 17.057362 - Time:310.86742997169495\n",
      "Epoch: 1 - Batch: 139200 - Loss: 16.841421 - Time:320.2824728488922\n",
      "Epoch: 1 - Batch: 143296 - Loss: 16.870363 - Time:329.71342873573303\n",
      "Epoch: 1 - Batch: 147392 - Loss: 16.955084 - Time:339.12701869010925\n",
      "Epoch: 1 - Batch: 151488 - Loss: 17.009920 - Time:348.53892946243286\n",
      "Epoch: 2 - Batch: 4032 - Loss: 16.673800 - Time:9.579635381698608\n",
      "Epoch: 2 - Batch: 8128 - Loss: 16.491037 - Time:18.99499011039734\n",
      "Epoch: 2 - Batch: 12224 - Loss: 16.610809 - Time:28.41222095489502\n",
      "Epoch: 2 - Batch: 16320 - Loss: 16.791840 - Time:37.85456943511963\n",
      "Epoch: 2 - Batch: 20416 - Loss: 17.077034 - Time:47.267345666885376\n",
      "Epoch: 2 - Batch: 24512 - Loss: 16.900856 - Time:56.6830792427063\n",
      "Epoch: 2 - Batch: 28608 - Loss: 16.753675 - Time:66.09660005569458\n",
      "Epoch: 2 - Batch: 32704 - Loss: 16.676188 - Time:75.53046250343323\n",
      "Epoch: 2 - Batch: 36800 - Loss: 16.802877 - Time:84.94356274604797\n",
      "Epoch: 2 - Batch: 40896 - Loss: 16.958233 - Time:94.35663890838623\n",
      "Epoch: 2 - Batch: 44992 - Loss: 16.797121 - Time:103.78804016113281\n",
      "Epoch: 2 - Batch: 49088 - Loss: 16.678526 - Time:113.20235681533813\n",
      "Epoch: 2 - Batch: 53184 - Loss: 16.897673 - Time:122.617924451828\n",
      "Epoch: 2 - Batch: 57280 - Loss: 16.672028 - Time:132.0315978527069\n",
      "Epoch: 2 - Batch: 61376 - Loss: 16.591425 - Time:141.44605112075806\n",
      "Epoch: 2 - Batch: 65472 - Loss: 16.655426 - Time:150.85939192771912\n",
      "Epoch: 2 - Batch: 69568 - Loss: 16.907225 - Time:160.273535490036\n",
      "Epoch: 2 - Batch: 73664 - Loss: 16.707977 - Time:169.68801307678223\n",
      "Epoch: 2 - Batch: 77760 - Loss: 16.890749 - Time:179.12262439727783\n",
      "Epoch: 2 - Batch: 81856 - Loss: 16.974569 - Time:188.53395819664001\n",
      "Epoch: 2 - Batch: 85952 - Loss: 16.628286 - Time:197.94644117355347\n",
      "Epoch: 2 - Batch: 90048 - Loss: 16.752832 - Time:207.3583629131317\n",
      "Epoch: 2 - Batch: 94144 - Loss: 16.797949 - Time:216.78928470611572\n",
      "Epoch: 2 - Batch: 98240 - Loss: 16.909891 - Time:226.20183539390564\n",
      "Epoch: 2 - Batch: 102336 - Loss: 16.757437 - Time:235.61603116989136\n",
      "Epoch: 2 - Batch: 106432 - Loss: 16.793795 - Time:245.04727149009705\n",
      "Epoch: 2 - Batch: 110528 - Loss: 16.667221 - Time:254.45958971977234\n",
      "Epoch: 2 - Batch: 114624 - Loss: 16.544418 - Time:263.87279438972473\n",
      "Epoch: 2 - Batch: 118720 - Loss: 16.744701 - Time:273.28838324546814\n",
      "Epoch: 2 - Batch: 122816 - Loss: 16.686642 - Time:282.70132780075073\n",
      "Epoch: 2 - Batch: 126912 - Loss: 16.681206 - Time:292.1148564815521\n",
      "Epoch: 2 - Batch: 131008 - Loss: 16.624573 - Time:301.5298833847046\n",
      "Epoch: 2 - Batch: 135104 - Loss: 16.510323 - Time:310.9415490627289\n",
      "Epoch: 2 - Batch: 139200 - Loss: 16.703602 - Time:320.37026381492615\n",
      "Epoch: 2 - Batch: 143296 - Loss: 16.799835 - Time:329.783570766449\n",
      "Epoch: 2 - Batch: 147392 - Loss: 16.530178 - Time:339.19833040237427\n",
      "Epoch: 2 - Batch: 151488 - Loss: 16.560513 - Time:348.6125566959381\n",
      "Epoch: 3 - Batch: 4032 - Loss: 16.893885 - Time:9.595550060272217\n",
      "Epoch: 3 - Batch: 8128 - Loss: 16.506340 - Time:19.01038908958435\n",
      "Epoch: 3 - Batch: 12224 - Loss: 16.577814 - Time:28.42611789703369\n",
      "Epoch: 3 - Batch: 16320 - Loss: 16.503326 - Time:37.839903831481934\n",
      "Epoch: 3 - Batch: 20416 - Loss: 16.653534 - Time:47.27339577674866\n",
      "Epoch: 3 - Batch: 24512 - Loss: 16.447990 - Time:56.68627166748047\n",
      "Epoch: 3 - Batch: 28608 - Loss: 16.605490 - Time:66.0999174118042\n",
      "Epoch: 3 - Batch: 32704 - Loss: 16.564747 - Time:75.53013253211975\n",
      "Epoch: 3 - Batch: 36800 - Loss: 16.610466 - Time:84.94510459899902\n",
      "Epoch: 3 - Batch: 40896 - Loss: 16.720182 - Time:94.35893726348877\n",
      "Epoch: 3 - Batch: 44992 - Loss: 16.476263 - Time:103.77061152458191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 - Batch: 49088 - Loss: 16.591591 - Time:113.18501925468445\n",
      "Epoch: 3 - Batch: 53184 - Loss: 16.338840 - Time:122.59879493713379\n",
      "Epoch: 3 - Batch: 57280 - Loss: 16.337900 - Time:132.01120615005493\n",
      "Epoch: 3 - Batch: 61376 - Loss: 16.325525 - Time:141.423259973526\n",
      "Epoch: 3 - Batch: 65472 - Loss: 16.344061 - Time:150.853586435318\n",
      "Epoch: 3 - Batch: 69568 - Loss: 16.366693 - Time:160.26542234420776\n",
      "Epoch: 3 - Batch: 73664 - Loss: 16.547134 - Time:169.67703485488892\n",
      "Epoch: 3 - Batch: 77760 - Loss: 16.369228 - Time:179.0916759967804\n",
      "Epoch: 3 - Batch: 81856 - Loss: 16.378080 - Time:188.52169489860535\n",
      "Epoch: 3 - Batch: 85952 - Loss: 16.414351 - Time:197.9334876537323\n",
      "Epoch: 3 - Batch: 90048 - Loss: 16.586966 - Time:207.34606552124023\n",
      "Epoch: 3 - Batch: 94144 - Loss: 16.494598 - Time:216.77755498886108\n",
      "Epoch: 3 - Batch: 98240 - Loss: 16.475344 - Time:226.18973350524902\n",
      "Epoch: 3 - Batch: 102336 - Loss: 16.330709 - Time:235.60386109352112\n",
      "Epoch: 3 - Batch: 106432 - Loss: 16.413763 - Time:245.01758527755737\n",
      "Epoch: 3 - Batch: 110528 - Loss: 16.416092 - Time:254.42935037612915\n",
      "Epoch: 3 - Batch: 114624 - Loss: 16.640188 - Time:263.84329533576965\n",
      "Epoch: 3 - Batch: 118720 - Loss: 16.648979 - Time:273.25841546058655\n",
      "Epoch: 3 - Batch: 122816 - Loss: 16.418377 - Time:282.6736845970154\n",
      "Epoch: 3 - Batch: 126912 - Loss: 16.450602 - Time:292.10571813583374\n",
      "Epoch: 3 - Batch: 131008 - Loss: 16.556164 - Time:301.5209710597992\n",
      "Epoch: 3 - Batch: 135104 - Loss: 16.728392 - Time:310.93442487716675\n",
      "Epoch: 3 - Batch: 139200 - Loss: 16.301710 - Time:320.34995436668396\n",
      "Epoch: 3 - Batch: 143296 - Loss: 16.559235 - Time:329.76568698883057\n",
      "Epoch: 3 - Batch: 147392 - Loss: 16.273869 - Time:339.2001292705536\n",
      "Epoch: 3 - Batch: 151488 - Loss: 16.349056 - Time:348.615168094635\n",
      "Epoch: 4 - Batch: 4032 - Loss: 16.655182 - Time:9.601316690444946\n",
      "Epoch: 4 - Batch: 8128 - Loss: 16.211735 - Time:19.01870083808899\n",
      "Epoch: 4 - Batch: 12224 - Loss: 16.281029 - Time:28.434371948242188\n",
      "Epoch: 4 - Batch: 16320 - Loss: 16.477638 - Time:37.84754133224487\n",
      "Epoch: 4 - Batch: 20416 - Loss: 16.343693 - Time:47.26221966743469\n",
      "Epoch: 4 - Batch: 24512 - Loss: 16.033743 - Time:56.67414402961731\n",
      "Epoch: 4 - Batch: 28608 - Loss: 16.167933 - Time:66.08774638175964\n",
      "Epoch: 4 - Batch: 32704 - Loss: 16.558479 - Time:75.50107645988464\n",
      "Epoch: 4 - Batch: 36800 - Loss: 16.183498 - Time:84.93120098114014\n",
      "Epoch: 4 - Batch: 40896 - Loss: 16.323563 - Time:94.34265899658203\n",
      "Epoch: 4 - Batch: 44992 - Loss: 16.200649 - Time:103.75509881973267\n",
      "Epoch: 4 - Batch: 49088 - Loss: 16.265266 - Time:113.16699409484863\n",
      "Epoch: 4 - Batch: 53184 - Loss: 16.403503 - Time:122.59800434112549\n",
      "Epoch: 4 - Batch: 57280 - Loss: 16.249559 - Time:132.0108618736267\n",
      "Epoch: 4 - Batch: 61376 - Loss: 16.312140 - Time:141.42340350151062\n",
      "Epoch: 4 - Batch: 65472 - Loss: 16.181183 - Time:150.8531141281128\n",
      "Epoch: 4 - Batch: 69568 - Loss: 16.099848 - Time:160.2660858631134\n",
      "Epoch: 4 - Batch: 73664 - Loss: 15.978646 - Time:169.6787190437317\n",
      "Epoch: 4 - Batch: 77760 - Loss: 16.126423 - Time:179.09011888504028\n",
      "Epoch: 4 - Batch: 81856 - Loss: 16.170607 - Time:188.50270438194275\n",
      "Epoch: 4 - Batch: 85952 - Loss: 16.279449 - Time:197.9169590473175\n",
      "Epoch: 4 - Batch: 90048 - Loss: 15.782185 - Time:207.3324100971222\n",
      "Epoch: 4 - Batch: 94144 - Loss: 16.319237 - Time:216.7468318939209\n",
      "Epoch: 4 - Batch: 98240 - Loss: 16.154114 - Time:226.1749496459961\n",
      "Epoch: 4 - Batch: 102336 - Loss: 16.166588 - Time:235.58847451210022\n",
      "Epoch: 4 - Batch: 106432 - Loss: 16.485979 - Time:245.00420546531677\n",
      "Epoch: 4 - Batch: 110528 - Loss: 16.031656 - Time:254.41805601119995\n",
      "Epoch: 4 - Batch: 114624 - Loss: 15.821193 - Time:263.8488173484802\n",
      "Epoch: 4 - Batch: 118720 - Loss: 16.201921 - Time:273.26087737083435\n",
      "Epoch: 4 - Batch: 122816 - Loss: 16.131163 - Time:282.67578053474426\n",
      "Epoch: 4 - Batch: 126912 - Loss: 16.000996 - Time:292.1063640117645\n",
      "Epoch: 4 - Batch: 131008 - Loss: 15.892493 - Time:301.518239736557\n",
      "Epoch: 4 - Batch: 135104 - Loss: 15.965229 - Time:310.9342255592346\n",
      "Epoch: 4 - Batch: 139200 - Loss: 15.851874 - Time:320.3477506637573\n",
      "Epoch: 4 - Batch: 143296 - Loss: 16.077467 - Time:329.76045060157776\n",
      "Epoch: 4 - Batch: 147392 - Loss: 16.238905 - Time:339.172030210495\n",
      "Epoch: 4 - Batch: 151488 - Loss: 16.013046 - Time:348.5855996608734\n",
      "Epoch: 5 - Batch: 4032 - Loss: 16.019217 - Time:9.602423906326294\n",
      "Epoch: 5 - Batch: 8128 - Loss: 16.095533 - Time:19.01829242706299\n",
      "Epoch: 5 - Batch: 12224 - Loss: 15.823296 - Time:28.433375597000122\n",
      "Epoch: 5 - Batch: 16320 - Loss: 16.355015 - Time:37.84768605232239\n",
      "Epoch: 5 - Batch: 20416 - Loss: 16.095346 - Time:47.26061224937439\n",
      "Epoch: 5 - Batch: 24512 - Loss: 16.245306 - Time:56.67290282249451\n",
      "Epoch: 5 - Batch: 28608 - Loss: 15.774046 - Time:66.08456182479858\n",
      "Epoch: 5 - Batch: 32704 - Loss: 16.136154 - Time:75.4951765537262\n",
      "Epoch: 5 - Batch: 36800 - Loss: 16.183790 - Time:84.92461323738098\n",
      "Epoch: 5 - Batch: 40896 - Loss: 15.959588 - Time:94.33807015419006\n",
      "Epoch: 5 - Batch: 44992 - Loss: 15.839060 - Time:103.75029468536377\n",
      "Epoch: 5 - Batch: 49088 - Loss: 16.146879 - Time:113.1611099243164\n",
      "Epoch: 5 - Batch: 53184 - Loss: 15.961020 - Time:122.58960032463074\n",
      "Epoch: 5 - Batch: 57280 - Loss: 15.961684 - Time:132.0023593902588\n",
      "Epoch: 5 - Batch: 61376 - Loss: 15.982675 - Time:141.4173502922058\n",
      "Epoch: 5 - Batch: 65472 - Loss: 16.087048 - Time:150.84718108177185\n",
      "Epoch: 5 - Batch: 69568 - Loss: 16.180008 - Time:160.25765323638916\n",
      "Epoch: 5 - Batch: 73664 - Loss: 15.777690 - Time:169.67185163497925\n",
      "Epoch: 5 - Batch: 77760 - Loss: 15.839652 - Time:179.08512783050537\n",
      "Epoch: 5 - Batch: 81856 - Loss: 16.111952 - Time:188.49678897857666\n",
      "Epoch: 5 - Batch: 85952 - Loss: 16.020424 - Time:197.91174483299255\n",
      "Epoch: 5 - Batch: 90048 - Loss: 16.021328 - Time:207.32451224327087\n",
      "Epoch: 5 - Batch: 94144 - Loss: 16.025976 - Time:216.73657512664795\n",
      "Epoch: 5 - Batch: 98240 - Loss: 15.854095 - Time:226.168860912323\n",
      "Epoch: 5 - Batch: 102336 - Loss: 15.694084 - Time:235.58019018173218\n",
      "Epoch: 5 - Batch: 106432 - Loss: 15.594145 - Time:244.99372005462646\n",
      "Epoch: 5 - Batch: 110528 - Loss: 16.012918 - Time:254.40605425834656\n",
      "Epoch: 5 - Batch: 114624 - Loss: 15.497198 - Time:263.8180527687073\n",
      "Epoch: 5 - Batch: 118720 - Loss: 16.103994 - Time:273.2474830150604\n",
      "Epoch: 5 - Batch: 122816 - Loss: 15.556538 - Time:282.66001987457275\n",
      "Epoch: 5 - Batch: 126912 - Loss: 15.611676 - Time:292.08999133110046\n",
      "Epoch: 5 - Batch: 131008 - Loss: 15.753861 - Time:301.5018095970154\n",
      "Epoch: 5 - Batch: 135104 - Loss: 15.705238 - Time:310.91560316085815\n",
      "Epoch: 5 - Batch: 139200 - Loss: 16.136383 - Time:320.3273448944092\n",
      "Epoch: 5 - Batch: 143296 - Loss: 15.714834 - Time:329.73764276504517\n",
      "Epoch: 5 - Batch: 147392 - Loss: 16.083761 - Time:339.1496431827545\n",
      "Epoch: 5 - Batch: 151488 - Loss: 15.687488 - Time:348.5630588531494\n",
      "Epoch: 6 - Batch: 4032 - Loss: 15.908765 - Time:9.600097417831421\n",
      "Epoch: 6 - Batch: 8128 - Loss: 15.111525 - Time:19.01178479194641\n",
      "Epoch: 6 - Batch: 12224 - Loss: 16.040720 - Time:28.424222230911255\n",
      "Epoch: 6 - Batch: 16320 - Loss: 15.836370 - Time:37.836358308792114\n",
      "Epoch: 6 - Batch: 20416 - Loss: 15.511606 - Time:47.24726104736328\n",
      "Epoch: 6 - Batch: 24512 - Loss: 15.608425 - Time:56.659674406051636\n",
      "Epoch: 6 - Batch: 28608 - Loss: 15.834041 - Time:66.07049655914307\n",
      "Epoch: 6 - Batch: 32704 - Loss: 15.521369 - Time:75.48209857940674\n",
      "Epoch: 6 - Batch: 36800 - Loss: 15.748933 - Time:84.90824770927429\n",
      "Epoch: 6 - Batch: 40896 - Loss: 15.302573 - Time:94.31995296478271\n",
      "Epoch: 6 - Batch: 44992 - Loss: 15.646605 - Time:103.73093366622925\n",
      "Epoch: 6 - Batch: 49088 - Loss: 15.510463 - Time:113.14300322532654\n",
      "Epoch: 6 - Batch: 53184 - Loss: 15.491045 - Time:122.57100701332092\n",
      "Epoch: 6 - Batch: 57280 - Loss: 15.678683 - Time:131.98190832138062\n",
      "Epoch: 6 - Batch: 61376 - Loss: 15.394773 - Time:141.3929123878479\n",
      "Epoch: 6 - Batch: 65472 - Loss: 15.815116 - Time:150.8205690383911\n",
      "Epoch: 6 - Batch: 69568 - Loss: 15.678031 - Time:160.23042178153992\n",
      "Epoch: 6 - Batch: 73664 - Loss: 14.714073 - Time:169.64145588874817\n",
      "Epoch: 6 - Batch: 77760 - Loss: 15.312687 - Time:179.0549304485321\n",
      "Epoch: 6 - Batch: 81856 - Loss: 15.607857 - Time:188.4678328037262\n",
      "Epoch: 6 - Batch: 85952 - Loss: 15.476951 - Time:197.87971115112305\n",
      "Epoch: 6 - Batch: 90048 - Loss: 15.084308 - Time:207.29049062728882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 - Batch: 94144 - Loss: 15.033865 - Time:216.70112824440002\n",
      "Epoch: 6 - Batch: 98240 - Loss: 15.388899 - Time:226.13171672821045\n",
      "Epoch: 6 - Batch: 102336 - Loss: 15.572194 - Time:235.54218554496765\n",
      "Epoch: 6 - Batch: 106432 - Loss: 15.699821 - Time:244.9537239074707\n",
      "Epoch: 6 - Batch: 110528 - Loss: 15.215359 - Time:254.36501479148865\n",
      "Epoch: 6 - Batch: 114624 - Loss: 15.226972 - Time:263.77609491348267\n",
      "Epoch: 6 - Batch: 118720 - Loss: 15.408610 - Time:273.2069489955902\n",
      "Epoch: 6 - Batch: 122816 - Loss: 15.197136 - Time:282.6181755065918\n",
      "Epoch: 6 - Batch: 126912 - Loss: 15.423554 - Time:292.03005480766296\n",
      "Epoch: 6 - Batch: 131008 - Loss: 15.627878 - Time:301.4608783721924\n",
      "Epoch: 6 - Batch: 135104 - Loss: 15.104167 - Time:310.87580704689026\n",
      "Epoch: 6 - Batch: 139200 - Loss: 14.933500 - Time:320.29018092155457\n",
      "Epoch: 6 - Batch: 143296 - Loss: 15.294626 - Time:329.7031362056732\n",
      "Epoch: 6 - Batch: 147392 - Loss: 15.418038 - Time:339.1132085323334\n",
      "Epoch: 6 - Batch: 151488 - Loss: 15.009151 - Time:348.52308320999146\n",
      "Epoch: 7 - Batch: 4032 - Loss: 14.708924 - Time:9.605693578720093\n",
      "Epoch: 7 - Batch: 8128 - Loss: 14.468114 - Time:19.021263360977173\n",
      "Epoch: 7 - Batch: 12224 - Loss: 14.803814 - Time:28.43680691719055\n",
      "Epoch: 7 - Batch: 16320 - Loss: 15.050216 - Time:37.85029864311218\n",
      "Epoch: 7 - Batch: 20416 - Loss: 15.054349 - Time:47.26485276222229\n",
      "Epoch: 7 - Batch: 24512 - Loss: 14.762589 - Time:56.67657423019409\n",
      "Epoch: 7 - Batch: 28608 - Loss: 14.579974 - Time:66.08815479278564\n",
      "Epoch: 7 - Batch: 32704 - Loss: 14.910801 - Time:75.49902105331421\n",
      "Epoch: 7 - Batch: 36800 - Loss: 15.143829 - Time:84.93193554878235\n",
      "Epoch: 7 - Batch: 40896 - Loss: 14.525688 - Time:94.34532642364502\n",
      "Epoch: 7 - Batch: 44992 - Loss: 14.605895 - Time:103.75731539726257\n",
      "Epoch: 7 - Batch: 49088 - Loss: 14.499027 - Time:113.17161774635315\n",
      "Epoch: 7 - Batch: 53184 - Loss: 14.696129 - Time:122.60195016860962\n",
      "Epoch: 7 - Batch: 57280 - Loss: 14.829246 - Time:132.01388382911682\n",
      "Epoch: 7 - Batch: 61376 - Loss: 14.979559 - Time:141.4247682094574\n",
      "Epoch: 7 - Batch: 65472 - Loss: 14.720167 - Time:150.853511095047\n",
      "Epoch: 7 - Batch: 69568 - Loss: 14.524834 - Time:160.26481175422668\n",
      "Epoch: 7 - Batch: 73664 - Loss: 14.609135 - Time:169.6768913269043\n",
      "Epoch: 7 - Batch: 77760 - Loss: 14.486951 - Time:179.0898404121399\n",
      "Epoch: 7 - Batch: 81856 - Loss: 14.501899 - Time:188.50221872329712\n",
      "Epoch: 7 - Batch: 85952 - Loss: 14.734607 - Time:197.91403698921204\n",
      "Epoch: 7 - Batch: 90048 - Loss: 14.157236 - Time:207.32684445381165\n",
      "Epoch: 7 - Batch: 94144 - Loss: 14.744317 - Time:216.73588371276855\n",
      "Epoch: 7 - Batch: 98240 - Loss: 14.051277 - Time:226.1648211479187\n",
      "Epoch: 7 - Batch: 102336 - Loss: 14.615095 - Time:235.57722520828247\n",
      "Epoch: 7 - Batch: 106432 - Loss: 14.930933 - Time:244.98933935165405\n",
      "Epoch: 7 - Batch: 110528 - Loss: 14.540238 - Time:254.3997676372528\n",
      "Epoch: 7 - Batch: 114624 - Loss: 14.503839 - Time:263.8138749599457\n",
      "Epoch: 7 - Batch: 118720 - Loss: 13.710008 - Time:273.2422320842743\n",
      "Epoch: 7 - Batch: 122816 - Loss: 15.051618 - Time:282.65338587760925\n",
      "Epoch: 7 - Batch: 126912 - Loss: 14.052627 - Time:292.0646197795868\n",
      "Epoch: 7 - Batch: 131008 - Loss: 14.377352 - Time:301.49350237846375\n",
      "Epoch: 7 - Batch: 135104 - Loss: 14.315771 - Time:310.90480065345764\n",
      "Epoch: 7 - Batch: 139200 - Loss: 13.857861 - Time:320.3167972564697\n",
      "Epoch: 7 - Batch: 143296 - Loss: 14.357564 - Time:329.73097038269043\n",
      "Epoch: 7 - Batch: 147392 - Loss: 14.361120 - Time:339.1437907218933\n",
      "Epoch: 7 - Batch: 151488 - Loss: 14.303639 - Time:348.5571186542511\n",
      "Epoch: 8 - Batch: 4032 - Loss: 14.045487 - Time:9.576499938964844\n",
      "Epoch: 8 - Batch: 8128 - Loss: 13.607158 - Time:19.02156138420105\n",
      "Epoch: 8 - Batch: 12224 - Loss: 13.859147 - Time:28.435924530029297\n",
      "Epoch: 8 - Batch: 16320 - Loss: 13.650189 - Time:37.848247051239014\n",
      "Epoch: 8 - Batch: 20416 - Loss: 13.655369 - Time:47.26055669784546\n",
      "Epoch: 8 - Batch: 24512 - Loss: 14.399424 - Time:56.69146180152893\n",
      "Epoch: 8 - Batch: 28608 - Loss: 13.724462 - Time:66.10508322715759\n",
      "Epoch: 8 - Batch: 32704 - Loss: 14.141375 - Time:75.51837849617004\n",
      "Epoch: 8 - Batch: 36800 - Loss: 13.108179 - Time:84.94802594184875\n",
      "Epoch: 8 - Batch: 40896 - Loss: 13.329910 - Time:94.36253833770752\n",
      "Epoch: 8 - Batch: 44992 - Loss: 13.824709 - Time:103.77597951889038\n",
      "Epoch: 8 - Batch: 49088 - Loss: 14.055007 - Time:113.1908631324768\n",
      "Epoch: 8 - Batch: 53184 - Loss: 14.013548 - Time:122.60436820983887\n",
      "Epoch: 8 - Batch: 57280 - Loss: 13.303185 - Time:132.0193314552307\n",
      "Epoch: 8 - Batch: 61376 - Loss: 13.558905 - Time:141.4320251941681\n",
      "Epoch: 8 - Batch: 65472 - Loss: 13.878411 - Time:150.84412622451782\n",
      "Epoch: 8 - Batch: 69568 - Loss: 13.137789 - Time:160.27352142333984\n",
      "Epoch: 8 - Batch: 73664 - Loss: 13.571965 - Time:169.6837944984436\n",
      "Epoch: 8 - Batch: 77760 - Loss: 12.948975 - Time:179.09758853912354\n",
      "Epoch: 8 - Batch: 81856 - Loss: 13.549397 - Time:188.51284742355347\n",
      "Epoch: 8 - Batch: 85952 - Loss: 13.291046 - Time:197.94637298583984\n",
      "Epoch: 8 - Batch: 90048 - Loss: 12.575224 - Time:207.3591079711914\n",
      "Epoch: 8 - Batch: 94144 - Loss: 13.588292 - Time:216.77436351776123\n",
      "Epoch: 8 - Batch: 98240 - Loss: 13.236286 - Time:226.20740485191345\n",
      "Epoch: 8 - Batch: 102336 - Loss: 12.817616 - Time:235.6234073638916\n",
      "Epoch: 8 - Batch: 106432 - Loss: 13.146432 - Time:245.03849601745605\n",
      "Epoch: 8 - Batch: 110528 - Loss: 13.128589 - Time:254.45224785804749\n",
      "Epoch: 8 - Batch: 114624 - Loss: 13.274243 - Time:263.86351251602173\n",
      "Epoch: 8 - Batch: 118720 - Loss: 13.460208 - Time:273.2740390300751\n",
      "Epoch: 8 - Batch: 122816 - Loss: 12.728388 - Time:282.68284010887146\n",
      "Epoch: 8 - Batch: 126912 - Loss: 12.966759 - Time:292.0940065383911\n",
      "Epoch: 8 - Batch: 131008 - Loss: 12.761214 - Time:301.507239818573\n",
      "Epoch: 8 - Batch: 135104 - Loss: 12.475233 - Time:310.9380609989166\n",
      "Epoch: 8 - Batch: 139200 - Loss: 13.327636 - Time:320.3511176109314\n",
      "Epoch: 8 - Batch: 143296 - Loss: 12.046773 - Time:329.7649555206299\n",
      "Epoch: 8 - Batch: 147392 - Loss: 12.729925 - Time:339.17891359329224\n",
      "Epoch: 8 - Batch: 151488 - Loss: 13.221434 - Time:348.60916805267334\n",
      "Epoch: 9 - Batch: 4032 - Loss: 12.962268 - Time:9.573713541030884\n",
      "Epoch: 9 - Batch: 8128 - Loss: 12.716245 - Time:18.991783380508423\n",
      "Epoch: 9 - Batch: 12224 - Loss: 11.536261 - Time:28.436686038970947\n",
      "Epoch: 9 - Batch: 16320 - Loss: 11.381412 - Time:37.85174202919006\n",
      "Epoch: 9 - Batch: 20416 - Loss: 13.572297 - Time:47.267059564590454\n",
      "Epoch: 9 - Batch: 24512 - Loss: 11.572117 - Time:56.697617292404175\n",
      "Epoch: 9 - Batch: 28608 - Loss: 12.163589 - Time:66.11114192008972\n",
      "Epoch: 9 - Batch: 32704 - Loss: 11.834466 - Time:75.52257657051086\n",
      "Epoch: 9 - Batch: 36800 - Loss: 11.880530 - Time:84.93521642684937\n",
      "Epoch: 9 - Batch: 40896 - Loss: 12.599758 - Time:94.3482825756073\n",
      "Epoch: 9 - Batch: 44992 - Loss: 13.066184 - Time:103.75871729850769\n",
      "Epoch: 9 - Batch: 49088 - Loss: 12.323874 - Time:113.17185258865356\n",
      "Epoch: 9 - Batch: 53184 - Loss: 12.002569 - Time:122.58250904083252\n",
      "Epoch: 9 - Batch: 57280 - Loss: 12.191381 - Time:132.01340436935425\n",
      "Epoch: 9 - Batch: 61376 - Loss: 12.523052 - Time:141.42478156089783\n",
      "Epoch: 9 - Batch: 65472 - Loss: 12.094427 - Time:150.83800792694092\n",
      "Epoch: 9 - Batch: 69568 - Loss: 12.175080 - Time:160.24886107444763\n",
      "Epoch: 9 - Batch: 73664 - Loss: 11.796065 - Time:169.67797207832336\n",
      "Epoch: 9 - Batch: 77760 - Loss: 11.945656 - Time:179.09121966362\n",
      "Epoch: 9 - Batch: 81856 - Loss: 11.030486 - Time:188.50244975090027\n",
      "Epoch: 9 - Batch: 85952 - Loss: 11.419092 - Time:197.93269777297974\n",
      "Epoch: 9 - Batch: 90048 - Loss: 12.081687 - Time:207.3427996635437\n",
      "Epoch: 9 - Batch: 94144 - Loss: 10.527879 - Time:216.75553584098816\n",
      "Epoch: 9 - Batch: 98240 - Loss: 11.598644 - Time:226.16641974449158\n",
      "Epoch: 9 - Batch: 102336 - Loss: 11.006282 - Time:235.57843041419983\n",
      "Epoch: 9 - Batch: 106432 - Loss: 12.417038 - Time:244.99259042739868\n",
      "Epoch: 9 - Batch: 110528 - Loss: 11.743020 - Time:254.40974879264832\n",
      "Epoch: 9 - Batch: 114624 - Loss: 11.010459 - Time:263.8240783214569\n",
      "Epoch: 9 - Batch: 118720 - Loss: 11.717723 - Time:273.25153136253357\n",
      "Epoch: 9 - Batch: 122816 - Loss: 11.068807 - Time:282.6632308959961\n",
      "Epoch: 9 - Batch: 126912 - Loss: 11.652485 - Time:292.0750744342804\n",
      "Epoch: 9 - Batch: 131008 - Loss: 11.668013 - Time:301.48815870285034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 - Batch: 135104 - Loss: 11.174631 - Time:310.91861963272095\n",
      "Epoch: 9 - Batch: 139200 - Loss: 10.601727 - Time:320.3342695236206\n",
      "Epoch: 9 - Batch: 143296 - Loss: 10.448202 - Time:329.74677181243896\n",
      "Epoch: 9 - Batch: 147392 - Loss: 10.458442 - Time:339.1777720451355\n",
      "Epoch: 9 - Batch: 151488 - Loss: 10.865146 - Time:348.59156346321106\n",
      "Epoch: 10 - Batch: 4032 - Loss: 10.511045 - Time:9.577880144119263\n",
      "Epoch: 10 - Batch: 8128 - Loss: 11.571114 - Time:18.99225616455078\n",
      "Epoch: 10 - Batch: 12224 - Loss: 10.537813 - Time:28.43175768852234\n",
      "Epoch: 10 - Batch: 16320 - Loss: 11.127252 - Time:37.844266176223755\n",
      "Epoch: 10 - Batch: 20416 - Loss: 10.165695 - Time:47.25762915611267\n",
      "Epoch: 10 - Batch: 24512 - Loss: 11.201471 - Time:56.66965174674988\n",
      "Epoch: 10 - Batch: 28608 - Loss: 10.563341 - Time:66.08358430862427\n",
      "Epoch: 10 - Batch: 32704 - Loss: 10.469930 - Time:75.49524188041687\n",
      "Epoch: 10 - Batch: 36800 - Loss: 11.213416 - Time:84.90872001647949\n",
      "Epoch: 10 - Batch: 40896 - Loss: 9.760038 - Time:94.3243579864502\n",
      "Epoch: 10 - Batch: 44992 - Loss: 9.771868 - Time:103.7560966014862\n",
      "Epoch: 10 - Batch: 49088 - Loss: 8.796687 - Time:113.16872334480286\n",
      "Epoch: 10 - Batch: 53184 - Loss: 9.641866 - Time:122.58071422576904\n",
      "Epoch: 10 - Batch: 57280 - Loss: 10.652750 - Time:131.99537134170532\n",
      "Epoch: 10 - Batch: 61376 - Loss: 11.497024 - Time:141.42801022529602\n",
      "Epoch: 10 - Batch: 65472 - Loss: 9.150352 - Time:150.8423261642456\n",
      "Epoch: 10 - Batch: 69568 - Loss: 10.639482 - Time:160.25670075416565\n",
      "Epoch: 10 - Batch: 73664 - Loss: 10.460118 - Time:169.68807768821716\n",
      "Epoch: 10 - Batch: 77760 - Loss: 10.376880 - Time:179.10074019432068\n",
      "Epoch: 10 - Batch: 81856 - Loss: 9.913551 - Time:188.5149848461151\n",
      "Epoch: 10 - Batch: 85952 - Loss: 10.356938 - Time:197.93001532554626\n",
      "Epoch: 10 - Batch: 90048 - Loss: 10.374345 - Time:207.34268832206726\n",
      "Epoch: 10 - Batch: 94144 - Loss: 9.576590 - Time:216.75564694404602\n",
      "Epoch: 10 - Batch: 98240 - Loss: 9.677743 - Time:226.168527841568\n",
      "Epoch: 10 - Batch: 102336 - Loss: 9.791713 - Time:235.5816261768341\n",
      "Epoch: 10 - Batch: 106432 - Loss: 10.033216 - Time:245.0094699859619\n",
      "Epoch: 10 - Batch: 110528 - Loss: 10.846367 - Time:254.42227983474731\n",
      "Epoch: 10 - Batch: 114624 - Loss: 9.636648 - Time:263.83392095565796\n",
      "Epoch: 10 - Batch: 118720 - Loss: 10.367695 - Time:273.2460150718689\n",
      "Epoch: 10 - Batch: 122816 - Loss: 9.655766 - Time:282.6760296821594\n",
      "Epoch: 10 - Batch: 126912 - Loss: 10.677187 - Time:292.0909233093262\n",
      "Epoch: 10 - Batch: 131008 - Loss: 9.817989 - Time:301.50352334976196\n",
      "Epoch: 10 - Batch: 135104 - Loss: 9.559256 - Time:310.93310737609863\n",
      "Epoch: 10 - Batch: 139200 - Loss: 9.362197 - Time:320.3442552089691\n",
      "Epoch: 10 - Batch: 143296 - Loss: 9.024415 - Time:329.75642228126526\n",
      "Epoch: 10 - Batch: 147392 - Loss: 9.424438 - Time:339.1687994003296\n",
      "Epoch: 10 - Batch: 151488 - Loss: 9.194689 - Time:348.5819847583771\n",
      "Epoch: 11 - Batch: 4032 - Loss: 8.440996 - Time:9.581421136856079\n",
      "Epoch: 11 - Batch: 8128 - Loss: 8.336775 - Time:19.023693323135376\n",
      "Epoch: 11 - Batch: 12224 - Loss: 7.788002 - Time:28.439460515975952\n",
      "Epoch: 11 - Batch: 16320 - Loss: 8.546276 - Time:37.85225868225098\n",
      "Epoch: 11 - Batch: 20416 - Loss: 8.549316 - Time:47.265259981155396\n",
      "Epoch: 11 - Batch: 24512 - Loss: 7.970702 - Time:56.676594734191895\n",
      "Epoch: 11 - Batch: 28608 - Loss: 8.907393 - Time:66.090975522995\n",
      "Epoch: 11 - Batch: 32704 - Loss: 8.323947 - Time:75.50587034225464\n",
      "Epoch: 11 - Batch: 36800 - Loss: 8.121977 - Time:84.9182345867157\n",
      "Epoch: 11 - Batch: 40896 - Loss: 7.945563 - Time:94.34957146644592\n",
      "Epoch: 11 - Batch: 44992 - Loss: 9.459097 - Time:103.76424407958984\n",
      "Epoch: 11 - Batch: 49088 - Loss: 8.057016 - Time:113.1776852607727\n",
      "Epoch: 11 - Batch: 53184 - Loss: 8.310946 - Time:122.58889579772949\n",
      "Epoch: 11 - Batch: 57280 - Loss: 7.867589 - Time:132.01742148399353\n",
      "Epoch: 11 - Batch: 61376 - Loss: 8.276864 - Time:141.42878818511963\n",
      "Epoch: 11 - Batch: 65472 - Loss: 8.573903 - Time:150.84113764762878\n",
      "Epoch: 11 - Batch: 69568 - Loss: 8.430843 - Time:160.2727746963501\n",
      "Epoch: 11 - Batch: 73664 - Loss: 9.175470 - Time:169.683109998703\n",
      "Epoch: 11 - Batch: 77760 - Loss: 8.077767 - Time:179.0946023464203\n",
      "Epoch: 11 - Batch: 81856 - Loss: 9.214485 - Time:188.50963187217712\n",
      "Epoch: 11 - Batch: 85952 - Loss: 8.927613 - Time:197.92161893844604\n",
      "Epoch: 11 - Batch: 90048 - Loss: 9.030100 - Time:207.33467411994934\n",
      "Epoch: 11 - Batch: 94144 - Loss: 9.067842 - Time:216.74741172790527\n",
      "Epoch: 11 - Batch: 98240 - Loss: 9.703393 - Time:226.15869998931885\n",
      "Epoch: 11 - Batch: 102336 - Loss: 9.455197 - Time:235.58714056015015\n",
      "Epoch: 11 - Batch: 106432 - Loss: 8.000518 - Time:244.9982888698578\n",
      "Epoch: 11 - Batch: 110528 - Loss: 8.863598 - Time:254.4108555316925\n",
      "Epoch: 11 - Batch: 114624 - Loss: 8.832543 - Time:263.8229522705078\n",
      "Epoch: 11 - Batch: 118720 - Loss: 7.814254 - Time:273.25150418281555\n",
      "Epoch: 11 - Batch: 122816 - Loss: 7.840360 - Time:282.6650242805481\n",
      "Epoch: 11 - Batch: 126912 - Loss: 8.347172 - Time:292.0787892341614\n",
      "Epoch: 11 - Batch: 131008 - Loss: 8.356191 - Time:301.50957226753235\n",
      "Epoch: 11 - Batch: 135104 - Loss: 8.469975 - Time:310.92164731025696\n",
      "Epoch: 11 - Batch: 139200 - Loss: 8.213284 - Time:320.33415031433105\n",
      "Epoch: 11 - Batch: 143296 - Loss: 8.449395 - Time:329.7490825653076\n",
      "Epoch: 11 - Batch: 147392 - Loss: 7.119843 - Time:339.162052154541\n",
      "Epoch: 11 - Batch: 151488 - Loss: 8.970238 - Time:348.5749843120575\n",
      "Epoch: 12 - Batch: 4032 - Loss: 6.783526 - Time:9.579728603363037\n",
      "Epoch: 12 - Batch: 8128 - Loss: 6.725996 - Time:18.995174884796143\n",
      "Epoch: 12 - Batch: 12224 - Loss: 8.373560 - Time:28.434653282165527\n",
      "Epoch: 12 - Batch: 16320 - Loss: 7.399544 - Time:37.847766160964966\n",
      "Epoch: 12 - Batch: 20416 - Loss: 7.376148 - Time:47.2623553276062\n",
      "Epoch: 12 - Batch: 24512 - Loss: 8.143971 - Time:56.67693901062012\n",
      "Epoch: 12 - Batch: 28608 - Loss: 6.220679 - Time:66.11117267608643\n",
      "Epoch: 12 - Batch: 32704 - Loss: 6.931732 - Time:75.52697801589966\n",
      "Epoch: 12 - Batch: 36800 - Loss: 6.966055 - Time:84.94271278381348\n",
      "Epoch: 12 - Batch: 40896 - Loss: 6.499400 - Time:94.37659311294556\n",
      "Epoch: 12 - Batch: 44992 - Loss: 7.363165 - Time:103.79016327857971\n",
      "Epoch: 12 - Batch: 49088 - Loss: 6.882293 - Time:113.20469379425049\n",
      "Epoch: 12 - Batch: 53184 - Loss: 7.836961 - Time:122.61751389503479\n",
      "Epoch: 12 - Batch: 57280 - Loss: 6.869437 - Time:132.02972030639648\n",
      "Epoch: 12 - Batch: 61376 - Loss: 7.270239 - Time:141.44432616233826\n",
      "Epoch: 12 - Batch: 65472 - Loss: 7.102948 - Time:150.85939860343933\n",
      "Epoch: 12 - Batch: 69568 - Loss: 6.301341 - Time:160.2724688053131\n",
      "Epoch: 12 - Batch: 73664 - Loss: 8.031156 - Time:169.6987645626068\n",
      "Epoch: 12 - Batch: 77760 - Loss: 7.750340 - Time:179.10994696617126\n",
      "Epoch: 12 - Batch: 81856 - Loss: 7.824444 - Time:188.52343916893005\n",
      "Epoch: 12 - Batch: 85952 - Loss: 7.366971 - Time:197.93939208984375\n",
      "Epoch: 12 - Batch: 90048 - Loss: 5.818139 - Time:207.36804294586182\n",
      "Epoch: 12 - Batch: 94144 - Loss: 6.401137 - Time:216.78078174591064\n",
      "Epoch: 12 - Batch: 98240 - Loss: 7.266043 - Time:226.19347286224365\n",
      "Epoch: 12 - Batch: 102336 - Loss: 6.249043 - Time:235.62306427955627\n",
      "Epoch: 12 - Batch: 106432 - Loss: 6.867425 - Time:245.03562450408936\n",
      "Epoch: 12 - Batch: 110528 - Loss: 6.871722 - Time:254.44879746437073\n",
      "Epoch: 12 - Batch: 114624 - Loss: 8.668449 - Time:263.86049699783325\n",
      "Epoch: 12 - Batch: 118720 - Loss: 7.096096 - Time:273.2730641365051\n",
      "Epoch: 12 - Batch: 122816 - Loss: 5.955497 - Time:282.68557620048523\n",
      "Epoch: 12 - Batch: 126912 - Loss: 5.641048 - Time:292.09770584106445\n",
      "Epoch: 12 - Batch: 131008 - Loss: 6.646005 - Time:301.51080656051636\n",
      "Epoch: 12 - Batch: 135104 - Loss: 6.482344 - Time:310.93934893608093\n",
      "Epoch: 12 - Batch: 139200 - Loss: 6.913397 - Time:320.35328483581543\n",
      "Epoch: 12 - Batch: 143296 - Loss: 7.309202 - Time:329.7644233703613\n",
      "Epoch: 12 - Batch: 147392 - Loss: 6.747641 - Time:339.173180103302\n",
      "Epoch: 12 - Batch: 151488 - Loss: 6.924725 - Time:348.60065746307373\n",
      "Epoch: 13 - Batch: 4032 - Loss: 6.359837 - Time:9.587037563323975\n",
      "Epoch: 13 - Batch: 8128 - Loss: 6.417781 - Time:19.003804445266724\n",
      "Epoch: 13 - Batch: 12224 - Loss: 6.496174 - Time:28.417622804641724\n",
      "Epoch: 13 - Batch: 16320 - Loss: 6.343061 - Time:37.85860514640808\n",
      "Epoch: 13 - Batch: 20416 - Loss: 5.781253 - Time:47.27168798446655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 - Batch: 24512 - Loss: 5.069403 - Time:56.7032904624939\n",
      "Epoch: 13 - Batch: 28608 - Loss: 5.381323 - Time:66.11729669570923\n",
      "Epoch: 13 - Batch: 32704 - Loss: 5.294456 - Time:75.53414964675903\n",
      "Epoch: 13 - Batch: 36800 - Loss: 5.588585 - Time:84.94756746292114\n",
      "Epoch: 13 - Batch: 40896 - Loss: 5.970428 - Time:94.362065076828\n",
      "Epoch: 13 - Batch: 44992 - Loss: 6.013079 - Time:103.77665567398071\n",
      "Epoch: 13 - Batch: 49088 - Loss: 5.722341 - Time:113.19064140319824\n",
      "Epoch: 13 - Batch: 53184 - Loss: 5.843303 - Time:122.60154056549072\n",
      "Epoch: 13 - Batch: 57280 - Loss: 6.255595 - Time:132.01451325416565\n",
      "Epoch: 13 - Batch: 61376 - Loss: 5.309368 - Time:141.44344806671143\n",
      "Epoch: 13 - Batch: 65472 - Loss: 6.269670 - Time:150.85584020614624\n",
      "Epoch: 13 - Batch: 69568 - Loss: 5.813092 - Time:160.26840376853943\n",
      "Epoch: 13 - Batch: 73664 - Loss: 5.518933 - Time:169.6813862323761\n",
      "Epoch: 13 - Batch: 77760 - Loss: 6.951519 - Time:179.11238193511963\n",
      "Epoch: 13 - Batch: 81856 - Loss: 6.446956 - Time:188.52339673042297\n",
      "Epoch: 13 - Batch: 85952 - Loss: 6.276387 - Time:197.9355342388153\n",
      "Epoch: 13 - Batch: 90048 - Loss: 5.408587 - Time:207.36764478683472\n",
      "Epoch: 13 - Batch: 94144 - Loss: 5.995861 - Time:216.7805347442627\n",
      "Epoch: 13 - Batch: 98240 - Loss: 5.898757 - Time:226.19198083877563\n",
      "Epoch: 13 - Batch: 102336 - Loss: 6.302268 - Time:235.6043198108673\n",
      "Epoch: 13 - Batch: 106432 - Loss: 5.105980 - Time:245.0167498588562\n",
      "Epoch: 13 - Batch: 110528 - Loss: 5.906260 - Time:254.42923164367676\n",
      "Epoch: 13 - Batch: 114624 - Loss: 5.684606 - Time:263.8404414653778\n",
      "Epoch: 13 - Batch: 118720 - Loss: 7.100164 - Time:273.2522943019867\n",
      "Epoch: 13 - Batch: 122816 - Loss: 5.547205 - Time:282.68155884742737\n",
      "Epoch: 13 - Batch: 126912 - Loss: 4.292961 - Time:292.09491777420044\n",
      "Epoch: 13 - Batch: 131008 - Loss: 6.987298 - Time:301.50506591796875\n",
      "Epoch: 13 - Batch: 135104 - Loss: 5.626431 - Time:310.9157590866089\n",
      "Epoch: 13 - Batch: 139200 - Loss: 5.505808 - Time:320.3477439880371\n",
      "Epoch: 13 - Batch: 143296 - Loss: 5.566812 - Time:329.7608380317688\n",
      "Epoch: 13 - Batch: 147392 - Loss: 6.025621 - Time:339.1768524646759\n",
      "Epoch: 13 - Batch: 151488 - Loss: 6.084768 - Time:348.6097300052643\n",
      "Epoch: 14 - Batch: 4032 - Loss: 4.566383 - Time:9.574609756469727\n",
      "Epoch: 14 - Batch: 8128 - Loss: 4.926308 - Time:18.988895654678345\n",
      "Epoch: 14 - Batch: 12224 - Loss: 5.008535 - Time:28.430906534194946\n",
      "Epoch: 14 - Batch: 16320 - Loss: 3.717042 - Time:37.842185258865356\n",
      "Epoch: 14 - Batch: 20416 - Loss: 4.705920 - Time:47.25496029853821\n",
      "Epoch: 14 - Batch: 24512 - Loss: 3.758452 - Time:56.665895223617554\n",
      "Epoch: 14 - Batch: 28608 - Loss: 4.586849 - Time:66.0765688419342\n",
      "Epoch: 14 - Batch: 32704 - Loss: 5.301286 - Time:75.48889136314392\n",
      "Epoch: 14 - Batch: 36800 - Loss: 4.152145 - Time:84.90020227432251\n",
      "Epoch: 14 - Batch: 40896 - Loss: 4.848084 - Time:94.31306552886963\n",
      "Epoch: 14 - Batch: 44992 - Loss: 4.621950 - Time:103.74300408363342\n",
      "Epoch: 14 - Batch: 49088 - Loss: 5.183700 - Time:113.15622568130493\n",
      "Epoch: 14 - Batch: 53184 - Loss: 5.113131 - Time:122.56926345825195\n",
      "Epoch: 14 - Batch: 57280 - Loss: 4.586425 - Time:131.98115038871765\n",
      "Epoch: 14 - Batch: 61376 - Loss: 3.791291 - Time:141.40816688537598\n",
      "Epoch: 14 - Batch: 65472 - Loss: 4.804745 - Time:150.82041931152344\n",
      "Epoch: 14 - Batch: 69568 - Loss: 4.358354 - Time:160.2348392009735\n",
      "Epoch: 14 - Batch: 73664 - Loss: 5.201843 - Time:169.6655833721161\n",
      "Epoch: 14 - Batch: 77760 - Loss: 4.788034 - Time:179.07899737358093\n",
      "Epoch: 14 - Batch: 81856 - Loss: 4.668164 - Time:188.4908697605133\n",
      "Epoch: 14 - Batch: 85952 - Loss: 6.091248 - Time:197.9028515815735\n",
      "Epoch: 14 - Batch: 90048 - Loss: 4.295897 - Time:207.3164792060852\n",
      "Epoch: 14 - Batch: 94144 - Loss: 4.200362 - Time:216.72727870941162\n",
      "Epoch: 14 - Batch: 98240 - Loss: 3.246064 - Time:226.13932633399963\n",
      "Epoch: 14 - Batch: 102336 - Loss: 5.041597 - Time:235.55101895332336\n",
      "Epoch: 14 - Batch: 106432 - Loss: 4.470007 - Time:244.98110723495483\n",
      "Epoch: 14 - Batch: 110528 - Loss: 4.255767 - Time:254.39142227172852\n",
      "Epoch: 14 - Batch: 114624 - Loss: 4.526490 - Time:263.80462098121643\n",
      "Epoch: 14 - Batch: 118720 - Loss: 3.209964 - Time:273.21751046180725\n",
      "Epoch: 14 - Batch: 122816 - Loss: 4.655325 - Time:282.63098526000977\n",
      "Epoch: 14 - Batch: 126912 - Loss: 5.602746 - Time:292.0593099594116\n",
      "Epoch: 14 - Batch: 131008 - Loss: 5.108634 - Time:301.474241733551\n",
      "Epoch: 14 - Batch: 135104 - Loss: 4.299121 - Time:310.9050941467285\n",
      "Epoch: 14 - Batch: 139200 - Loss: 3.689415 - Time:320.3194637298584\n",
      "Epoch: 14 - Batch: 143296 - Loss: 4.552417 - Time:329.73526263237\n",
      "Epoch: 14 - Batch: 147392 - Loss: 4.511653 - Time:339.14898586273193\n",
      "Epoch: 14 - Batch: 151488 - Loss: 3.940981 - Time:348.5655653476715\n",
      "Epoch: 15 - Batch: 4032 - Loss: 4.351609 - Time:9.578725814819336\n",
      "Epoch: 15 - Batch: 8128 - Loss: 4.109645 - Time:19.019383430480957\n",
      "Epoch: 15 - Batch: 12224 - Loss: 3.518292 - Time:28.433075189590454\n",
      "Epoch: 15 - Batch: 16320 - Loss: 3.673565 - Time:37.849804162979126\n",
      "Epoch: 15 - Batch: 20416 - Loss: 3.830301 - Time:47.26876521110535\n",
      "Epoch: 15 - Batch: 24512 - Loss: 3.893499 - Time:56.689871311187744\n",
      "Epoch: 15 - Batch: 28608 - Loss: 4.082713 - Time:66.10970067977905\n",
      "Epoch: 15 - Batch: 32704 - Loss: 4.014376 - Time:75.52953481674194\n",
      "Epoch: 15 - Batch: 36800 - Loss: 4.186227 - Time:84.95341348648071\n",
      "Epoch: 15 - Batch: 40896 - Loss: 4.813700 - Time:94.40123701095581\n",
      "Epoch: 15 - Batch: 44992 - Loss: 3.390210 - Time:103.81979703903198\n",
      "Epoch: 15 - Batch: 49088 - Loss: 4.938554 - Time:113.24170064926147\n",
      "Epoch: 15 - Batch: 53184 - Loss: 3.041022 - Time:122.66318941116333\n",
      "Epoch: 15 - Batch: 57280 - Loss: 3.926551 - Time:132.1072781085968\n",
      "Epoch: 15 - Batch: 61376 - Loss: 4.672355 - Time:141.53053188323975\n",
      "Epoch: 15 - Batch: 65472 - Loss: 4.883600 - Time:150.95712327957153\n",
      "Epoch: 15 - Batch: 69568 - Loss: 3.676917 - Time:160.3959767818451\n",
      "Epoch: 15 - Batch: 73664 - Loss: 4.671138 - Time:169.82137250900269\n",
      "Epoch: 15 - Batch: 77760 - Loss: 3.522498 - Time:179.24613571166992\n",
      "Epoch: 15 - Batch: 81856 - Loss: 3.089977 - Time:188.67281913757324\n",
      "Epoch: 15 - Batch: 85952 - Loss: 2.825161 - Time:198.09765696525574\n",
      "Epoch: 15 - Batch: 90048 - Loss: 4.335638 - Time:207.51155877113342\n",
      "Epoch: 15 - Batch: 94144 - Loss: 4.117980 - Time:216.92784428596497\n",
      "Epoch: 15 - Batch: 98240 - Loss: 4.528229 - Time:226.34155917167664\n",
      "Epoch: 15 - Batch: 102336 - Loss: 4.023829 - Time:235.7718164920807\n",
      "Epoch: 15 - Batch: 106432 - Loss: 3.676552 - Time:245.18625211715698\n",
      "Epoch: 15 - Batch: 110528 - Loss: 4.312806 - Time:254.59899520874023\n",
      "Epoch: 15 - Batch: 114624 - Loss: 3.466636 - Time:264.0126688480377\n",
      "Epoch: 15 - Batch: 118720 - Loss: 5.094338 - Time:273.44378089904785\n",
      "Epoch: 15 - Batch: 122816 - Loss: 3.315172 - Time:282.8570363521576\n",
      "Epoch: 15 - Batch: 126912 - Loss: 4.676991 - Time:292.2709639072418\n",
      "Epoch: 15 - Batch: 131008 - Loss: 5.059422 - Time:301.7022795677185\n",
      "Epoch: 15 - Batch: 135104 - Loss: 4.661280 - Time:311.1131308078766\n",
      "Epoch: 15 - Batch: 139200 - Loss: 4.005080 - Time:320.52537965774536\n",
      "Epoch: 15 - Batch: 143296 - Loss: 3.836203 - Time:329.93870186805725\n",
      "Epoch: 15 - Batch: 147392 - Loss: 3.235078 - Time:339.35006833076477\n",
      "Epoch: 15 - Batch: 151488 - Loss: 3.241012 - Time:348.76454067230225\n",
      "Epoch: 16 - Batch: 4032 - Loss: 3.873456 - Time:9.5785231590271\n",
      "Epoch: 16 - Batch: 8128 - Loss: 2.759558 - Time:18.995121717453003\n",
      "Epoch: 16 - Batch: 12224 - Loss: 2.959147 - Time:28.437212228775024\n",
      "Epoch: 16 - Batch: 16320 - Loss: 2.783517 - Time:37.850502729415894\n",
      "Epoch: 16 - Batch: 20416 - Loss: 3.450724 - Time:47.26495409011841\n",
      "Epoch: 16 - Batch: 24512 - Loss: 3.235317 - Time:56.67655920982361\n",
      "Epoch: 16 - Batch: 28608 - Loss: 2.504267 - Time:66.10763239860535\n",
      "Epoch: 16 - Batch: 32704 - Loss: 3.010802 - Time:75.52136635780334\n",
      "Epoch: 16 - Batch: 36800 - Loss: 3.601897 - Time:84.93350386619568\n",
      "Epoch: 16 - Batch: 40896 - Loss: 3.224944 - Time:94.36428904533386\n",
      "Epoch: 16 - Batch: 44992 - Loss: 3.651299 - Time:103.775634765625\n",
      "Epoch: 16 - Batch: 49088 - Loss: 2.827196 - Time:113.18730020523071\n",
      "Epoch: 16 - Batch: 53184 - Loss: 2.846769 - Time:122.59862804412842\n",
      "Epoch: 16 - Batch: 57280 - Loss: 2.275806 - Time:132.01082134246826\n",
      "Epoch: 16 - Batch: 61376 - Loss: 4.341958 - Time:141.4238338470459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 - Batch: 65472 - Loss: 3.159977 - Time:150.83567190170288\n",
      "Epoch: 16 - Batch: 69568 - Loss: 3.606112 - Time:160.24748754501343\n",
      "Epoch: 16 - Batch: 73664 - Loss: 3.143519 - Time:169.67763447761536\n",
      "Epoch: 16 - Batch: 77760 - Loss: 2.918243 - Time:179.09138679504395\n",
      "Epoch: 16 - Batch: 81856 - Loss: 3.427268 - Time:188.50407528877258\n",
      "Epoch: 16 - Batch: 85952 - Loss: 3.082279 - Time:197.91556906700134\n",
      "Epoch: 16 - Batch: 90048 - Loss: 3.936981 - Time:207.34457969665527\n",
      "Epoch: 16 - Batch: 94144 - Loss: 3.468993 - Time:216.75860333442688\n",
      "Epoch: 16 - Batch: 98240 - Loss: 4.309787 - Time:226.17281222343445\n",
      "Epoch: 16 - Batch: 102336 - Loss: 2.335515 - Time:235.6035656929016\n",
      "Epoch: 16 - Batch: 106432 - Loss: 2.560688 - Time:245.0161852836609\n",
      "Epoch: 16 - Batch: 110528 - Loss: 2.868934 - Time:254.43081951141357\n",
      "Epoch: 16 - Batch: 114624 - Loss: 3.114891 - Time:263.84309577941895\n",
      "Epoch: 16 - Batch: 118720 - Loss: 1.928477 - Time:273.2589952945709\n",
      "Epoch: 16 - Batch: 122816 - Loss: 3.011430 - Time:282.6720681190491\n",
      "Epoch: 16 - Batch: 126912 - Loss: 4.264810 - Time:292.0838432312012\n",
      "Epoch: 16 - Batch: 131008 - Loss: 3.344337 - Time:301.4973838329315\n",
      "Epoch: 16 - Batch: 135104 - Loss: 2.205114 - Time:310.9273056983948\n",
      "Epoch: 16 - Batch: 139200 - Loss: 3.205678 - Time:320.33919310569763\n",
      "Epoch: 16 - Batch: 143296 - Loss: 2.704974 - Time:329.7511336803436\n",
      "Epoch: 16 - Batch: 147392 - Loss: 3.289773 - Time:339.1654989719391\n",
      "Epoch: 16 - Batch: 151488 - Loss: 3.650899 - Time:348.59400367736816\n",
      "Epoch: 17 - Batch: 4032 - Loss: 1.753134 - Time:9.575272560119629\n",
      "Epoch: 17 - Batch: 8128 - Loss: 2.150308 - Time:18.990326642990112\n",
      "Epoch: 17 - Batch: 12224 - Loss: 2.198474 - Time:28.405802488327026\n",
      "Epoch: 17 - Batch: 16320 - Loss: 2.234978 - Time:37.84842538833618\n",
      "Epoch: 17 - Batch: 20416 - Loss: 2.071750 - Time:47.26424503326416\n",
      "Epoch: 17 - Batch: 24512 - Loss: 2.060228 - Time:56.67866802215576\n",
      "Epoch: 17 - Batch: 28608 - Loss: 2.596404 - Time:66.1113760471344\n",
      "Epoch: 17 - Batch: 32704 - Loss: 2.562606 - Time:75.52524280548096\n",
      "Epoch: 17 - Batch: 36800 - Loss: 2.126644 - Time:84.94002842903137\n",
      "Epoch: 17 - Batch: 40896 - Loss: 1.883043 - Time:94.35739946365356\n",
      "Epoch: 17 - Batch: 44992 - Loss: 2.816837 - Time:103.7741105556488\n",
      "Epoch: 17 - Batch: 49088 - Loss: 2.486100 - Time:113.18832564353943\n",
      "Epoch: 17 - Batch: 53184 - Loss: 2.193818 - Time:122.60407137870789\n",
      "Epoch: 17 - Batch: 57280 - Loss: 3.564123 - Time:132.01563835144043\n",
      "Epoch: 17 - Batch: 61376 - Loss: 2.357243 - Time:141.4470202922821\n",
      "Epoch: 17 - Batch: 65472 - Loss: 3.294202 - Time:150.861168384552\n",
      "Epoch: 17 - Batch: 69568 - Loss: 1.719997 - Time:160.27465867996216\n",
      "Epoch: 17 - Batch: 73664 - Loss: 1.836720 - Time:169.68691444396973\n",
      "Epoch: 17 - Batch: 77760 - Loss: 2.523715 - Time:179.1176233291626\n",
      "Epoch: 17 - Batch: 81856 - Loss: 2.587029 - Time:188.53126502037048\n",
      "Epoch: 17 - Batch: 85952 - Loss: 2.367524 - Time:197.9459149837494\n",
      "Epoch: 17 - Batch: 90048 - Loss: 3.420563 - Time:207.377991437912\n",
      "Epoch: 17 - Batch: 94144 - Loss: 1.946970 - Time:216.7905249595642\n",
      "Epoch: 17 - Batch: 98240 - Loss: 3.385180 - Time:226.20728731155396\n",
      "Epoch: 17 - Batch: 102336 - Loss: 2.681110 - Time:235.62032985687256\n",
      "Epoch: 17 - Batch: 106432 - Loss: 3.562268 - Time:245.03455543518066\n",
      "Epoch: 17 - Batch: 110528 - Loss: 2.961135 - Time:254.44850707054138\n",
      "Epoch: 17 - Batch: 114624 - Loss: 2.545281 - Time:263.86228370666504\n",
      "Epoch: 17 - Batch: 118720 - Loss: 3.353451 - Time:273.27622079849243\n",
      "Epoch: 17 - Batch: 122816 - Loss: 2.321262 - Time:282.70596957206726\n",
      "Epoch: 17 - Batch: 126912 - Loss: 2.113698 - Time:292.1212100982666\n",
      "Epoch: 17 - Batch: 131008 - Loss: 2.950632 - Time:301.5354459285736\n",
      "Epoch: 17 - Batch: 135104 - Loss: 2.833501 - Time:310.94915747642517\n",
      "Epoch: 17 - Batch: 139200 - Loss: 2.508557 - Time:320.38314867019653\n",
      "Epoch: 17 - Batch: 143296 - Loss: 3.317814 - Time:329.797194480896\n",
      "Epoch: 17 - Batch: 147392 - Loss: 2.926733 - Time:339.2097053527832\n",
      "Epoch: 17 - Batch: 151488 - Loss: 1.924740 - Time:348.6402986049652\n",
      "Epoch: 18 - Batch: 4032 - Loss: 1.579326 - Time:9.579246044158936\n",
      "Epoch: 18 - Batch: 8128 - Loss: 2.198837 - Time:18.996607303619385\n",
      "Epoch: 18 - Batch: 12224 - Loss: 1.999234 - Time:28.440306425094604\n",
      "Epoch: 18 - Batch: 16320 - Loss: 1.927440 - Time:37.854695320129395\n",
      "Epoch: 18 - Batch: 20416 - Loss: 2.302402 - Time:47.26779294013977\n",
      "Epoch: 18 - Batch: 24512 - Loss: 1.810170 - Time:56.67854404449463\n",
      "Epoch: 18 - Batch: 28608 - Loss: 2.588180 - Time:66.09055685997009\n",
      "Epoch: 18 - Batch: 32704 - Loss: 2.120229 - Time:75.50444746017456\n",
      "Epoch: 18 - Batch: 36800 - Loss: 2.058155 - Time:84.91784167289734\n",
      "Epoch: 18 - Batch: 40896 - Loss: 2.213118 - Time:94.32830858230591\n",
      "Epoch: 18 - Batch: 44992 - Loss: 1.559783 - Time:103.75708246231079\n",
      "Epoch: 18 - Batch: 49088 - Loss: 1.434476 - Time:113.16813397407532\n",
      "Epoch: 18 - Batch: 53184 - Loss: 1.886268 - Time:122.58162808418274\n",
      "Epoch: 18 - Batch: 57280 - Loss: 2.118577 - Time:131.9936487674713\n",
      "Epoch: 18 - Batch: 61376 - Loss: 2.443987 - Time:141.42252326011658\n",
      "Epoch: 18 - Batch: 65472 - Loss: 1.665645 - Time:150.8333718776703\n",
      "Epoch: 18 - Batch: 69568 - Loss: 3.085902 - Time:160.24313974380493\n",
      "Epoch: 18 - Batch: 73664 - Loss: 1.226548 - Time:169.67243194580078\n",
      "Epoch: 18 - Batch: 77760 - Loss: 1.413971 - Time:179.08421730995178\n",
      "Epoch: 18 - Batch: 81856 - Loss: 2.748596 - Time:188.4960699081421\n",
      "Epoch: 18 - Batch: 85952 - Loss: 2.335212 - Time:197.90800189971924\n",
      "Epoch: 18 - Batch: 90048 - Loss: 2.121092 - Time:207.31952953338623\n",
      "Epoch: 18 - Batch: 94144 - Loss: 2.119196 - Time:216.73086380958557\n",
      "Epoch: 18 - Batch: 98240 - Loss: 2.018308 - Time:226.14416432380676\n",
      "Epoch: 18 - Batch: 102336 - Loss: 1.258843 - Time:235.5564422607422\n",
      "Epoch: 18 - Batch: 106432 - Loss: 2.573864 - Time:244.9857976436615\n",
      "Epoch: 18 - Batch: 110528 - Loss: 3.160675 - Time:254.397358417511\n",
      "Epoch: 18 - Batch: 114624 - Loss: 2.608143 - Time:263.80976247787476\n",
      "Epoch: 18 - Batch: 118720 - Loss: 2.685635 - Time:273.2224450111389\n",
      "Epoch: 18 - Batch: 122816 - Loss: 2.692612 - Time:282.63589906692505\n",
      "Epoch: 18 - Batch: 126912 - Loss: 1.617664 - Time:292.0649471282959\n",
      "Epoch: 18 - Batch: 131008 - Loss: 3.195625 - Time:301.4761817455292\n",
      "Epoch: 18 - Batch: 135104 - Loss: 1.843506 - Time:310.90684747695923\n",
      "Epoch: 18 - Batch: 139200 - Loss: 2.505125 - Time:320.3208785057068\n",
      "Epoch: 18 - Batch: 143296 - Loss: 1.655066 - Time:329.734920501709\n",
      "Epoch: 18 - Batch: 147392 - Loss: 1.387739 - Time:339.1462926864624\n",
      "Epoch: 18 - Batch: 151488 - Loss: 1.831536 - Time:348.55907225608826\n",
      "Epoch: 19 - Batch: 4032 - Loss: 1.565663 - Time:9.584456443786621\n",
      "Epoch: 19 - Batch: 8128 - Loss: 1.841046 - Time:19.028459072113037\n",
      "Epoch: 19 - Batch: 12224 - Loss: 1.276083 - Time:28.442020893096924\n",
      "Epoch: 19 - Batch: 16320 - Loss: 1.147285 - Time:37.855674266815186\n",
      "Epoch: 19 - Batch: 20416 - Loss: 2.049452 - Time:47.26900005340576\n",
      "Epoch: 19 - Batch: 24512 - Loss: 1.192171 - Time:56.68297290802002\n",
      "Epoch: 19 - Batch: 28608 - Loss: 1.917909 - Time:66.09592700004578\n",
      "Epoch: 19 - Batch: 32704 - Loss: 1.183728 - Time:75.50735116004944\n",
      "Epoch: 19 - Batch: 36800 - Loss: 1.263024 - Time:84.91880083084106\n",
      "Epoch: 19 - Batch: 40896 - Loss: 1.211663 - Time:94.34605360031128\n",
      "Epoch: 19 - Batch: 44992 - Loss: 2.164002 - Time:103.7577555179596\n",
      "Epoch: 19 - Batch: 49088 - Loss: 2.386854 - Time:113.17197489738464\n",
      "Epoch: 19 - Batch: 53184 - Loss: 1.936501 - Time:122.58301663398743\n",
      "Epoch: 19 - Batch: 57280 - Loss: 1.742536 - Time:132.01211833953857\n",
      "Epoch: 19 - Batch: 61376 - Loss: 1.759786 - Time:141.42404460906982\n",
      "Epoch: 19 - Batch: 65472 - Loss: 1.573398 - Time:150.83582520484924\n",
      "Epoch: 19 - Batch: 69568 - Loss: 1.829238 - Time:160.26413488388062\n",
      "Epoch: 19 - Batch: 73664 - Loss: 2.509344 - Time:169.6764783859253\n",
      "Epoch: 19 - Batch: 77760 - Loss: 2.386050 - Time:179.0896599292755\n",
      "Epoch: 19 - Batch: 81856 - Loss: 1.602090 - Time:188.50451970100403\n",
      "Epoch: 19 - Batch: 85952 - Loss: 1.958145 - Time:197.91688632965088\n",
      "Epoch: 19 - Batch: 90048 - Loss: 1.925890 - Time:207.33071208000183\n",
      "Epoch: 19 - Batch: 94144 - Loss: 2.034395 - Time:216.74244141578674\n",
      "Epoch: 19 - Batch: 98240 - Loss: 1.818579 - Time:226.15359592437744\n",
      "Epoch: 19 - Batch: 102336 - Loss: 0.950412 - Time:235.5832462310791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 - Batch: 106432 - Loss: 0.835564 - Time:244.99451160430908\n",
      "Epoch: 19 - Batch: 110528 - Loss: 1.303545 - Time:254.40492963790894\n",
      "Epoch: 19 - Batch: 114624 - Loss: 1.418380 - Time:263.81529426574707\n",
      "Epoch: 19 - Batch: 118720 - Loss: 2.116535 - Time:273.2439534664154\n",
      "Epoch: 19 - Batch: 122816 - Loss: 2.078464 - Time:282.65595269203186\n",
      "Epoch: 19 - Batch: 126912 - Loss: 2.438918 - Time:292.06975650787354\n",
      "Epoch: 19 - Batch: 131008 - Loss: 2.648544 - Time:301.5005478858948\n",
      "Epoch: 19 - Batch: 135104 - Loss: 1.488886 - Time:310.9134991168976\n",
      "Epoch: 19 - Batch: 139200 - Loss: 2.214748 - Time:320.32718682289124\n",
      "Epoch: 19 - Batch: 143296 - Loss: 1.615309 - Time:329.73978066444397\n",
      "Epoch: 19 - Batch: 147392 - Loss: 2.038936 - Time:339.1528398990631\n",
      "Epoch: 19 - Batch: 151488 - Loss: 1.055172 - Time:348.56596279144287\n",
      "Epoch: 20 - Batch: 4032 - Loss: 1.640983 - Time:9.581170797348022\n",
      "Epoch: 20 - Batch: 8128 - Loss: 1.029488 - Time:18.998889207839966\n",
      "Epoch: 20 - Batch: 12224 - Loss: 1.968578 - Time:28.44264030456543\n",
      "Epoch: 20 - Batch: 16320 - Loss: 1.375566 - Time:37.85717225074768\n",
      "Epoch: 20 - Batch: 20416 - Loss: 0.794902 - Time:47.273019552230835\n",
      "Epoch: 20 - Batch: 24512 - Loss: 1.208701 - Time:56.68848443031311\n",
      "Epoch: 20 - Batch: 28608 - Loss: 1.354690 - Time:66.11966323852539\n",
      "Epoch: 20 - Batch: 32704 - Loss: 2.060730 - Time:75.53561401367188\n",
      "Epoch: 20 - Batch: 36800 - Loss: 1.548153 - Time:84.95043683052063\n",
      "Epoch: 20 - Batch: 40896 - Loss: 1.158827 - Time:94.3822250366211\n",
      "Epoch: 20 - Batch: 44992 - Loss: 1.213319 - Time:103.79341769218445\n",
      "Epoch: 20 - Batch: 49088 - Loss: 1.635751 - Time:113.20613932609558\n",
      "Epoch: 20 - Batch: 53184 - Loss: 1.223218 - Time:122.61788654327393\n",
      "Epoch: 20 - Batch: 57280 - Loss: 1.134764 - Time:132.02850103378296\n",
      "Epoch: 20 - Batch: 61376 - Loss: 1.181203 - Time:141.44117140769958\n",
      "Epoch: 20 - Batch: 65472 - Loss: 1.398799 - Time:150.85382199287415\n",
      "Epoch: 20 - Batch: 69568 - Loss: 1.394556 - Time:160.2663631439209\n",
      "Epoch: 20 - Batch: 73664 - Loss: 1.121668 - Time:169.6957130432129\n",
      "Epoch: 20 - Batch: 77760 - Loss: 2.802640 - Time:179.1064531803131\n",
      "Epoch: 20 - Batch: 81856 - Loss: 1.223806 - Time:188.51704907417297\n",
      "Epoch: 20 - Batch: 85952 - Loss: 1.358686 - Time:197.92808437347412\n",
      "Epoch: 20 - Batch: 90048 - Loss: 2.697721 - Time:207.35581922531128\n",
      "Epoch: 20 - Batch: 94144 - Loss: 1.304057 - Time:216.76551413536072\n",
      "Epoch: 20 - Batch: 98240 - Loss: 0.821317 - Time:226.17563104629517\n",
      "Epoch: 20 - Batch: 102336 - Loss: 1.849382 - Time:235.60627031326294\n",
      "Epoch: 20 - Batch: 106432 - Loss: 1.318852 - Time:245.0193362236023\n",
      "Epoch: 20 - Batch: 110528 - Loss: 2.181242 - Time:254.4315791130066\n",
      "Epoch: 20 - Batch: 114624 - Loss: 1.529121 - Time:263.8448519706726\n",
      "Epoch: 20 - Batch: 118720 - Loss: 1.157025 - Time:273.2606439590454\n",
      "Epoch: 20 - Batch: 122816 - Loss: 0.976693 - Time:282.67388129234314\n",
      "Epoch: 20 - Batch: 126912 - Loss: 1.672077 - Time:292.08902311325073\n",
      "Epoch: 20 - Batch: 131008 - Loss: 1.094499 - Time:301.5029444694519\n",
      "Epoch: 20 - Batch: 135104 - Loss: 1.422201 - Time:310.9341244697571\n",
      "Epoch: 20 - Batch: 139200 - Loss: 1.366418 - Time:320.3487560749054\n",
      "Epoch: 20 - Batch: 143296 - Loss: 0.968600 - Time:329.7608835697174\n",
      "Epoch: 20 - Batch: 147392 - Loss: 1.905543 - Time:339.17462968826294\n",
      "Epoch: 20 - Batch: 151488 - Loss: 1.680997 - Time:348.60272097587585\n",
      "Epoch: 21 - Batch: 4032 - Loss: 0.892362 - Time:9.572569370269775\n",
      "Epoch: 21 - Batch: 8128 - Loss: 1.102063 - Time:18.986891508102417\n",
      "Epoch: 21 - Batch: 12224 - Loss: 1.327755 - Time:28.404387950897217\n",
      "Epoch: 21 - Batch: 16320 - Loss: 1.105660 - Time:37.846227169036865\n",
      "Epoch: 21 - Batch: 20416 - Loss: 1.122515 - Time:47.26169395446777\n",
      "Epoch: 21 - Batch: 24512 - Loss: 1.412518 - Time:56.67603778839111\n",
      "Epoch: 21 - Batch: 28608 - Loss: 1.551809 - Time:66.10398030281067\n",
      "Epoch: 21 - Batch: 32704 - Loss: 1.214413 - Time:75.51857686042786\n",
      "Epoch: 21 - Batch: 36800 - Loss: 1.540814 - Time:84.93334627151489\n",
      "Epoch: 21 - Batch: 40896 - Loss: 1.423788 - Time:94.3484423160553\n",
      "Epoch: 21 - Batch: 44992 - Loss: 0.962945 - Time:103.76365971565247\n",
      "Epoch: 21 - Batch: 49088 - Loss: 1.386898 - Time:113.17468166351318\n",
      "Epoch: 21 - Batch: 53184 - Loss: 0.925601 - Time:122.58798623085022\n",
      "Epoch: 21 - Batch: 57280 - Loss: 1.372495 - Time:132.00083231925964\n",
      "Epoch: 21 - Batch: 61376 - Loss: 0.730040 - Time:141.430419921875\n",
      "Epoch: 21 - Batch: 65472 - Loss: 1.993140 - Time:150.84535217285156\n",
      "Epoch: 21 - Batch: 69568 - Loss: 1.993952 - Time:160.2573344707489\n",
      "Epoch: 21 - Batch: 73664 - Loss: 0.980373 - Time:169.6688768863678\n",
      "Epoch: 21 - Batch: 77760 - Loss: 1.241617 - Time:179.09811663627625\n",
      "Epoch: 21 - Batch: 81856 - Loss: 1.616211 - Time:188.51091957092285\n",
      "Epoch: 21 - Batch: 85952 - Loss: 1.069659 - Time:197.92421293258667\n",
      "Epoch: 21 - Batch: 90048 - Loss: 1.836685 - Time:207.3569254875183\n",
      "Epoch: 21 - Batch: 94144 - Loss: 1.255967 - Time:216.77091073989868\n",
      "Epoch: 21 - Batch: 98240 - Loss: 0.449311 - Time:226.18311285972595\n",
      "Epoch: 21 - Batch: 102336 - Loss: 1.761904 - Time:235.5960874557495\n",
      "Epoch: 21 - Batch: 106432 - Loss: 1.047077 - Time:245.00939631462097\n",
      "Epoch: 21 - Batch: 110528 - Loss: 1.055452 - Time:254.42202281951904\n",
      "Epoch: 21 - Batch: 114624 - Loss: 2.196809 - Time:263.8328950405121\n",
      "Epoch: 21 - Batch: 118720 - Loss: 1.248029 - Time:273.2440550327301\n",
      "Epoch: 21 - Batch: 122816 - Loss: 1.426685 - Time:282.6756911277771\n",
      "Epoch: 21 - Batch: 126912 - Loss: 1.037905 - Time:292.0896339416504\n",
      "Epoch: 21 - Batch: 131008 - Loss: 1.063526 - Time:301.5043911933899\n",
      "Epoch: 21 - Batch: 135104 - Loss: 1.920933 - Time:310.9189555644989\n",
      "Epoch: 21 - Batch: 139200 - Loss: 1.961808 - Time:320.3509900569916\n",
      "Epoch: 21 - Batch: 143296 - Loss: 1.949865 - Time:329.764151096344\n",
      "Epoch: 21 - Batch: 147392 - Loss: 2.306307 - Time:339.1791961193085\n",
      "Epoch: 21 - Batch: 151488 - Loss: 1.616316 - Time:348.61086225509644\n",
      "Epoch: 22 - Batch: 4032 - Loss: 0.560988 - Time:9.5815908908844\n",
      "Epoch: 22 - Batch: 8128 - Loss: 1.207309 - Time:18.995067358016968\n",
      "Epoch: 22 - Batch: 12224 - Loss: 0.772348 - Time:28.435765266418457\n",
      "Epoch: 22 - Batch: 16320 - Loss: 0.593514 - Time:37.847280502319336\n",
      "Epoch: 22 - Batch: 20416 - Loss: 0.748026 - Time:47.25954580307007\n",
      "Epoch: 22 - Batch: 24512 - Loss: 1.215881 - Time:56.67172884941101\n",
      "Epoch: 22 - Batch: 28608 - Loss: 0.946723 - Time:66.08448791503906\n",
      "Epoch: 22 - Batch: 32704 - Loss: 0.737545 - Time:75.49476456642151\n",
      "Epoch: 22 - Batch: 36800 - Loss: 0.976462 - Time:84.90776228904724\n",
      "Epoch: 22 - Batch: 40896 - Loss: 1.297601 - Time:94.31996059417725\n",
      "Epoch: 22 - Batch: 44992 - Loss: 1.515658 - Time:103.74811697006226\n",
      "Epoch: 22 - Batch: 49088 - Loss: 0.627527 - Time:113.15939593315125\n",
      "Epoch: 22 - Batch: 53184 - Loss: 0.904476 - Time:122.57000064849854\n",
      "Epoch: 22 - Batch: 57280 - Loss: 1.574325 - Time:131.9839837551117\n",
      "Epoch: 22 - Batch: 61376 - Loss: 1.033429 - Time:141.41280841827393\n",
      "Epoch: 22 - Batch: 65472 - Loss: 0.416213 - Time:150.82622003555298\n",
      "Epoch: 22 - Batch: 69568 - Loss: 1.172858 - Time:160.24102759361267\n",
      "Epoch: 22 - Batch: 73664 - Loss: 1.126248 - Time:169.67298364639282\n",
      "Epoch: 22 - Batch: 77760 - Loss: 1.161298 - Time:179.0849950313568\n",
      "Epoch: 22 - Batch: 81856 - Loss: 1.426653 - Time:188.49649024009705\n",
      "Epoch: 22 - Batch: 85952 - Loss: 1.444195 - Time:197.90881872177124\n",
      "Epoch: 22 - Batch: 90048 - Loss: 0.639846 - Time:207.32089281082153\n",
      "Epoch: 22 - Batch: 94144 - Loss: 1.021898 - Time:216.7333800792694\n",
      "Epoch: 22 - Batch: 98240 - Loss: 1.149097 - Time:226.14495086669922\n",
      "Epoch: 22 - Batch: 102336 - Loss: 0.564580 - Time:235.5560073852539\n",
      "Epoch: 22 - Batch: 106432 - Loss: 1.680933 - Time:244.98617362976074\n",
      "Epoch: 22 - Batch: 110528 - Loss: 1.683239 - Time:254.39779114723206\n",
      "Epoch: 22 - Batch: 114624 - Loss: 1.310365 - Time:263.81012415885925\n",
      "Epoch: 22 - Batch: 118720 - Loss: 0.948550 - Time:273.220472574234\n",
      "Epoch: 22 - Batch: 122816 - Loss: 1.927916 - Time:282.63150572776794\n",
      "Epoch: 22 - Batch: 126912 - Loss: 1.328997 - Time:292.0612509250641\n",
      "Epoch: 22 - Batch: 131008 - Loss: 0.984529 - Time:301.47362995147705\n",
      "Epoch: 22 - Batch: 135104 - Loss: 1.135433 - Time:310.88619470596313\n",
      "Epoch: 22 - Batch: 139200 - Loss: 0.993149 - Time:320.3164715766907\n",
      "Epoch: 22 - Batch: 143296 - Loss: 1.426161 - Time:329.7275376319885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 - Batch: 147392 - Loss: 1.353192 - Time:339.139931678772\n",
      "Epoch: 22 - Batch: 151488 - Loss: 1.416389 - Time:348.5506749153137\n",
      "Epoch: 23 - Batch: 4032 - Loss: 0.411205 - Time:9.57662057876587\n",
      "Epoch: 23 - Batch: 8128 - Loss: 0.449981 - Time:19.01742434501648\n",
      "Epoch: 23 - Batch: 12224 - Loss: 0.815330 - Time:28.43161368370056\n",
      "Epoch: 23 - Batch: 16320 - Loss: 0.672078 - Time:37.8434784412384\n",
      "Epoch: 23 - Batch: 20416 - Loss: 0.613766 - Time:47.25804829597473\n",
      "Epoch: 23 - Batch: 24512 - Loss: 0.803376 - Time:56.67078733444214\n",
      "Epoch: 23 - Batch: 28608 - Loss: 0.643861 - Time:66.08401775360107\n",
      "Epoch: 23 - Batch: 32704 - Loss: 0.538981 - Time:75.49615550041199\n",
      "Epoch: 23 - Batch: 36800 - Loss: 0.703201 - Time:84.90878987312317\n",
      "Epoch: 23 - Batch: 40896 - Loss: 0.677135 - Time:94.33743214607239\n",
      "Epoch: 23 - Batch: 44992 - Loss: 1.236181 - Time:103.75177311897278\n",
      "Epoch: 23 - Batch: 49088 - Loss: 1.131207 - Time:113.16298842430115\n",
      "Epoch: 23 - Batch: 53184 - Loss: 0.890714 - Time:122.57470345497131\n",
      "Epoch: 23 - Batch: 57280 - Loss: 1.233886 - Time:132.00366759300232\n",
      "Epoch: 23 - Batch: 61376 - Loss: 0.745661 - Time:141.4154109954834\n",
      "Epoch: 23 - Batch: 65472 - Loss: 0.447787 - Time:150.82513761520386\n",
      "Epoch: 23 - Batch: 69568 - Loss: 1.145900 - Time:160.25240397453308\n",
      "Epoch: 23 - Batch: 73664 - Loss: 0.827535 - Time:169.6641561985016\n",
      "Epoch: 23 - Batch: 77760 - Loss: 1.161885 - Time:179.0762152671814\n",
      "Epoch: 23 - Batch: 81856 - Loss: 1.829291 - Time:188.49013805389404\n",
      "Epoch: 23 - Batch: 85952 - Loss: 1.007526 - Time:197.90073323249817\n",
      "Epoch: 23 - Batch: 90048 - Loss: 0.383114 - Time:207.31208086013794\n",
      "Epoch: 23 - Batch: 94144 - Loss: 0.638019 - Time:216.72396302223206\n",
      "Epoch: 23 - Batch: 98240 - Loss: 1.162150 - Time:226.1366422176361\n",
      "Epoch: 23 - Batch: 102336 - Loss: 0.900645 - Time:235.56896901130676\n",
      "Epoch: 23 - Batch: 106432 - Loss: 0.917168 - Time:244.98074769973755\n",
      "Epoch: 23 - Batch: 110528 - Loss: 1.657119 - Time:254.3914020061493\n",
      "Epoch: 23 - Batch: 114624 - Loss: 0.864242 - Time:263.80468940734863\n",
      "Epoch: 23 - Batch: 118720 - Loss: 0.785752 - Time:273.23537707328796\n",
      "Epoch: 23 - Batch: 122816 - Loss: 1.010427 - Time:282.64749789237976\n",
      "Epoch: 23 - Batch: 126912 - Loss: 1.548201 - Time:292.0598750114441\n",
      "Epoch: 23 - Batch: 131008 - Loss: 0.995475 - Time:301.4883153438568\n",
      "Epoch: 23 - Batch: 135104 - Loss: 0.848985 - Time:310.9016966819763\n",
      "Epoch: 23 - Batch: 139200 - Loss: 0.637002 - Time:320.3149778842926\n",
      "Epoch: 23 - Batch: 143296 - Loss: 1.074778 - Time:329.72843408584595\n",
      "Epoch: 23 - Batch: 147392 - Loss: 1.335738 - Time:339.14345598220825\n",
      "Epoch: 23 - Batch: 151488 - Loss: 0.873716 - Time:348.55550813674927\n",
      "Epoch: 24 - Batch: 4032 - Loss: 0.504665 - Time:9.580453395843506\n",
      "Epoch: 24 - Batch: 8128 - Loss: 0.322630 - Time:18.999006271362305\n",
      "Epoch: 24 - Batch: 12224 - Loss: 1.478817 - Time:28.443819046020508\n",
      "Epoch: 24 - Batch: 16320 - Loss: 0.997768 - Time:37.85834741592407\n",
      "Epoch: 24 - Batch: 20416 - Loss: 0.884381 - Time:47.27514147758484\n",
      "Epoch: 24 - Batch: 24512 - Loss: 0.825030 - Time:56.689772844314575\n",
      "Epoch: 24 - Batch: 28608 - Loss: 0.631683 - Time:66.12136507034302\n",
      "Epoch: 24 - Batch: 32704 - Loss: 0.792059 - Time:75.53336811065674\n",
      "Epoch: 24 - Batch: 36800 - Loss: 0.443982 - Time:84.94657301902771\n",
      "Epoch: 24 - Batch: 40896 - Loss: 0.742314 - Time:94.37545967102051\n",
      "Epoch: 24 - Batch: 44992 - Loss: 0.500209 - Time:103.78748798370361\n",
      "Epoch: 24 - Batch: 49088 - Loss: 0.855995 - Time:113.20049023628235\n",
      "Epoch: 24 - Batch: 53184 - Loss: 0.801378 - Time:122.61435651779175\n",
      "Epoch: 24 - Batch: 57280 - Loss: 0.918361 - Time:132.03166937828064\n",
      "Epoch: 24 - Batch: 61376 - Loss: 0.639422 - Time:141.44533228874207\n",
      "Epoch: 24 - Batch: 65472 - Loss: 0.953227 - Time:150.85869812965393\n",
      "Epoch: 24 - Batch: 69568 - Loss: 0.361121 - Time:160.26925945281982\n",
      "Epoch: 24 - Batch: 73664 - Loss: 1.045347 - Time:169.6978304386139\n",
      "Epoch: 24 - Batch: 77760 - Loss: 1.043706 - Time:179.11036348342896\n",
      "Epoch: 24 - Batch: 81856 - Loss: 1.701923 - Time:188.52390551567078\n",
      "Epoch: 24 - Batch: 85952 - Loss: 0.707908 - Time:197.93858456611633\n",
      "Epoch: 24 - Batch: 90048 - Loss: 0.684527 - Time:207.36930203437805\n",
      "Epoch: 24 - Batch: 94144 - Loss: 0.781304 - Time:216.78110694885254\n",
      "Epoch: 24 - Batch: 98240 - Loss: 0.727289 - Time:226.1945357322693\n",
      "Epoch: 24 - Batch: 102336 - Loss: 0.478681 - Time:235.62443780899048\n",
      "Epoch: 24 - Batch: 106432 - Loss: 1.504166 - Time:245.0370705127716\n",
      "Epoch: 24 - Batch: 110528 - Loss: 1.272694 - Time:254.4505467414856\n",
      "Epoch: 24 - Batch: 114624 - Loss: 0.588365 - Time:263.8633346557617\n",
      "Epoch: 24 - Batch: 118720 - Loss: 1.133034 - Time:273.2772796154022\n",
      "Epoch: 24 - Batch: 122816 - Loss: 0.990520 - Time:282.69332695007324\n",
      "Epoch: 24 - Batch: 126912 - Loss: 0.975563 - Time:292.1053376197815\n",
      "Epoch: 24 - Batch: 131008 - Loss: 0.904097 - Time:301.52148365974426\n",
      "Epoch: 24 - Batch: 135104 - Loss: 0.731361 - Time:310.950879573822\n",
      "Epoch: 24 - Batch: 139200 - Loss: 0.677550 - Time:320.36427998542786\n",
      "Epoch: 24 - Batch: 143296 - Loss: 0.798643 - Time:329.7764461040497\n",
      "Epoch: 24 - Batch: 147392 - Loss: 1.074070 - Time:339.1893217563629\n",
      "Epoch: 24 - Batch: 151488 - Loss: 0.809751 - Time:348.61804270744324\n",
      "Epoch: 25 - Batch: 4032 - Loss: 0.995817 - Time:9.572167158126831\n",
      "Epoch: 25 - Batch: 8128 - Loss: 1.183318 - Time:18.988831996917725\n",
      "Epoch: 25 - Batch: 12224 - Loss: 0.949097 - Time:28.404935121536255\n",
      "Epoch: 25 - Batch: 16320 - Loss: 0.506882 - Time:37.84646654129028\n",
      "Epoch: 25 - Batch: 20416 - Loss: 0.451930 - Time:47.25888395309448\n",
      "Epoch: 25 - Batch: 24512 - Loss: 0.630307 - Time:56.669498920440674\n",
      "Epoch: 25 - Batch: 28608 - Loss: 0.846707 - Time:66.09952020645142\n",
      "Epoch: 25 - Batch: 32704 - Loss: 0.695224 - Time:75.51096606254578\n",
      "Epoch: 25 - Batch: 36800 - Loss: 0.865551 - Time:84.92508888244629\n",
      "Epoch: 25 - Batch: 40896 - Loss: 1.128123 - Time:94.33762454986572\n",
      "Epoch: 25 - Batch: 44992 - Loss: 0.530408 - Time:103.75113320350647\n",
      "Epoch: 25 - Batch: 49088 - Loss: 0.297078 - Time:113.16359686851501\n",
      "Epoch: 25 - Batch: 53184 - Loss: 0.643047 - Time:122.57556939125061\n",
      "Epoch: 25 - Batch: 57280 - Loss: 1.904688 - Time:131.99004793167114\n",
      "Epoch: 25 - Batch: 61376 - Loss: 0.783832 - Time:141.42028880119324\n",
      "Epoch: 25 - Batch: 65472 - Loss: 0.489051 - Time:150.83302450180054\n",
      "Epoch: 25 - Batch: 69568 - Loss: 0.758154 - Time:160.24626421928406\n",
      "Epoch: 25 - Batch: 73664 - Loss: 0.378233 - Time:169.65978360176086\n",
      "Epoch: 25 - Batch: 77760 - Loss: 1.088024 - Time:179.08983063697815\n",
      "Epoch: 25 - Batch: 81856 - Loss: 0.986787 - Time:188.50517463684082\n",
      "Epoch: 25 - Batch: 85952 - Loss: 1.300565 - Time:197.92004203796387\n",
      "Epoch: 25 - Batch: 90048 - Loss: 0.594889 - Time:207.35091280937195\n",
      "Epoch: 25 - Batch: 94144 - Loss: 0.413581 - Time:216.7636444568634\n",
      "Epoch: 25 - Batch: 98240 - Loss: 1.114851 - Time:226.178715467453\n",
      "Epoch: 25 - Batch: 102336 - Loss: 0.890900 - Time:235.5922703742981\n",
      "Epoch: 25 - Batch: 106432 - Loss: 1.350116 - Time:245.00578022003174\n",
      "Epoch: 25 - Batch: 110528 - Loss: 0.487939 - Time:254.4186451435089\n",
      "Epoch: 25 - Batch: 114624 - Loss: 0.757499 - Time:263.8312702178955\n",
      "Epoch: 25 - Batch: 118720 - Loss: 1.172593 - Time:273.24286794662476\n",
      "Epoch: 25 - Batch: 122816 - Loss: 0.433987 - Time:282.6709496974945\n",
      "Epoch: 25 - Batch: 126912 - Loss: 0.480170 - Time:292.0870292186737\n",
      "Epoch: 25 - Batch: 131008 - Loss: 0.828315 - Time:301.50074338912964\n",
      "Epoch: 25 - Batch: 135104 - Loss: 1.137382 - Time:310.9118638038635\n",
      "Epoch: 25 - Batch: 139200 - Loss: 0.606523 - Time:320.34329533576965\n",
      "Epoch: 25 - Batch: 143296 - Loss: 0.618944 - Time:329.7578320503235\n",
      "Epoch: 25 - Batch: 147392 - Loss: 0.647830 - Time:339.1708993911743\n",
      "Epoch: 25 - Batch: 151488 - Loss: 1.027897 - Time:348.6021873950958\n",
      "Epoch: 26 - Batch: 4032 - Loss: 0.656272 - Time:9.577970504760742\n",
      "Epoch: 26 - Batch: 8128 - Loss: 0.451942 - Time:18.99606704711914\n",
      "Epoch: 26 - Batch: 12224 - Loss: 0.809574 - Time:28.41306495666504\n",
      "Epoch: 26 - Batch: 16320 - Loss: 0.594436 - Time:37.828304052352905\n",
      "Epoch: 26 - Batch: 20416 - Loss: 0.504949 - Time:47.24093317985535\n",
      "Epoch: 26 - Batch: 24512 - Loss: 0.173525 - Time:56.65481877326965\n",
      "Epoch: 26 - Batch: 28608 - Loss: 0.747513 - Time:66.07035779953003\n",
      "Epoch: 26 - Batch: 32704 - Loss: 0.298277 - Time:75.51208996772766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 - Batch: 36800 - Loss: 0.213233 - Time:84.9249427318573\n",
      "Epoch: 26 - Batch: 40896 - Loss: 0.204348 - Time:94.33822536468506\n",
      "Epoch: 26 - Batch: 44992 - Loss: 0.867444 - Time:103.7510404586792\n",
      "Epoch: 26 - Batch: 49088 - Loss: 0.543518 - Time:113.18038368225098\n",
      "Epoch: 26 - Batch: 53184 - Loss: 0.869075 - Time:122.59416437149048\n",
      "Epoch: 26 - Batch: 57280 - Loss: 1.093373 - Time:132.00580716133118\n",
      "Epoch: 26 - Batch: 61376 - Loss: 0.491525 - Time:141.4367160797119\n",
      "Epoch: 26 - Batch: 65472 - Loss: 0.752262 - Time:150.85140562057495\n",
      "Epoch: 26 - Batch: 69568 - Loss: 0.586724 - Time:160.26539993286133\n",
      "Epoch: 26 - Batch: 73664 - Loss: 0.485907 - Time:169.6777491569519\n",
      "Epoch: 26 - Batch: 77760 - Loss: 1.025091 - Time:179.08951234817505\n",
      "Epoch: 26 - Batch: 81856 - Loss: 0.652439 - Time:188.50204730033875\n",
      "Epoch: 26 - Batch: 85952 - Loss: 0.843876 - Time:197.91531348228455\n",
      "Epoch: 26 - Batch: 90048 - Loss: 0.737067 - Time:207.33092641830444\n",
      "Epoch: 26 - Batch: 94144 - Loss: 0.785155 - Time:216.7626757621765\n",
      "Epoch: 26 - Batch: 98240 - Loss: 0.578075 - Time:226.17656064033508\n",
      "Epoch: 26 - Batch: 102336 - Loss: 0.602930 - Time:235.5880424976349\n",
      "Epoch: 26 - Batch: 106432 - Loss: 0.976559 - Time:245.00097823143005\n",
      "Epoch: 26 - Batch: 110528 - Loss: 0.695012 - Time:254.43090534210205\n",
      "Epoch: 26 - Batch: 114624 - Loss: 0.503141 - Time:263.8408718109131\n",
      "Epoch: 26 - Batch: 118720 - Loss: 0.556052 - Time:273.25360798835754\n",
      "Epoch: 26 - Batch: 122816 - Loss: 0.573083 - Time:282.68241810798645\n",
      "Epoch: 26 - Batch: 126912 - Loss: 0.626469 - Time:292.0982675552368\n",
      "Epoch: 26 - Batch: 131008 - Loss: 1.072345 - Time:301.5128071308136\n",
      "Epoch: 26 - Batch: 135104 - Loss: 1.187909 - Time:310.927907705307\n",
      "Epoch: 26 - Batch: 139200 - Loss: 0.450118 - Time:320.3394422531128\n",
      "Epoch: 26 - Batch: 143296 - Loss: 0.931172 - Time:329.75193643569946\n",
      "Epoch: 26 - Batch: 147392 - Loss: 0.712911 - Time:339.1641004085541\n",
      "Epoch: 26 - Batch: 151488 - Loss: 0.683595 - Time:348.57701325416565\n",
      "Epoch: 27 - Batch: 4032 - Loss: 0.299360 - Time:9.572869062423706\n",
      "Epoch: 27 - Batch: 8128 - Loss: 0.729568 - Time:18.99013352394104\n",
      "Epoch: 27 - Batch: 12224 - Loss: 0.474572 - Time:28.40392303466797\n",
      "Epoch: 27 - Batch: 16320 - Loss: 0.512588 - Time:37.81837606430054\n",
      "Epoch: 27 - Batch: 20416 - Loss: 0.535103 - Time:47.256524324417114\n",
      "Epoch: 27 - Batch: 24512 - Loss: 0.428411 - Time:56.666207790374756\n",
      "Epoch: 27 - Batch: 28608 - Loss: 0.520923 - Time:66.07704734802246\n",
      "Epoch: 27 - Batch: 32704 - Loss: 0.572407 - Time:75.5042335987091\n",
      "Epoch: 27 - Batch: 36800 - Loss: 0.721749 - Time:84.91677713394165\n",
      "Epoch: 27 - Batch: 40896 - Loss: 0.554586 - Time:94.33036041259766\n",
      "Epoch: 27 - Batch: 44992 - Loss: 0.431500 - Time:103.74126648902893\n",
      "Epoch: 27 - Batch: 49088 - Loss: 0.705197 - Time:113.15404963493347\n",
      "Epoch: 27 - Batch: 53184 - Loss: 0.546975 - Time:122.56488227844238\n",
      "Epoch: 27 - Batch: 57280 - Loss: 0.662174 - Time:131.9753725528717\n",
      "Epoch: 27 - Batch: 61376 - Loss: 0.761549 - Time:141.3881378173828\n",
      "Epoch: 27 - Batch: 65472 - Loss: 0.742484 - Time:150.81679773330688\n",
      "Epoch: 27 - Batch: 69568 - Loss: 0.762304 - Time:160.22928380966187\n",
      "Epoch: 27 - Batch: 73664 - Loss: 0.357425 - Time:169.64414525032043\n",
      "Epoch: 27 - Batch: 77760 - Loss: 0.824575 - Time:179.05714678764343\n",
      "Epoch: 27 - Batch: 81856 - Loss: 0.523336 - Time:188.48743414878845\n",
      "Epoch: 27 - Batch: 85952 - Loss: 0.605432 - Time:197.9017994403839\n",
      "Epoch: 27 - Batch: 90048 - Loss: 0.523394 - Time:207.31280279159546\n",
      "Epoch: 27 - Batch: 94144 - Loss: 0.398042 - Time:216.74514508247375\n",
      "Epoch: 27 - Batch: 98240 - Loss: 0.414717 - Time:226.16515493392944\n",
      "Epoch: 27 - Batch: 102336 - Loss: 0.428535 - Time:235.5800700187683\n",
      "Epoch: 27 - Batch: 106432 - Loss: 0.818826 - Time:244.99222207069397\n",
      "Epoch: 27 - Batch: 110528 - Loss: 0.824131 - Time:254.40862488746643\n",
      "Epoch: 27 - Batch: 114624 - Loss: 0.463246 - Time:263.8216083049774\n",
      "Epoch: 27 - Batch: 118720 - Loss: 1.252454 - Time:273.2350263595581\n",
      "Epoch: 27 - Batch: 122816 - Loss: 0.225938 - Time:282.6480128765106\n",
      "Epoch: 27 - Batch: 126912 - Loss: 0.573551 - Time:292.07546615600586\n",
      "Epoch: 27 - Batch: 131008 - Loss: 1.020638 - Time:301.48768043518066\n",
      "Epoch: 27 - Batch: 135104 - Loss: 1.077197 - Time:310.89844608306885\n",
      "Epoch: 27 - Batch: 139200 - Loss: 0.648206 - Time:320.3108515739441\n",
      "Epoch: 27 - Batch: 143296 - Loss: 0.658925 - Time:329.7404432296753\n",
      "Epoch: 27 - Batch: 147392 - Loss: 0.711987 - Time:339.15296387672424\n",
      "Epoch: 27 - Batch: 151488 - Loss: 0.374612 - Time:348.56788516044617\n",
      "Epoch: 28 - Batch: 4032 - Loss: 0.326117 - Time:9.602717876434326\n",
      "Epoch: 28 - Batch: 8128 - Loss: 0.294305 - Time:19.020987033843994\n",
      "Epoch: 28 - Batch: 12224 - Loss: 0.647121 - Time:28.43709659576416\n",
      "Epoch: 28 - Batch: 16320 - Loss: 0.588513 - Time:37.85162615776062\n",
      "Epoch: 28 - Batch: 20416 - Loss: 0.240320 - Time:47.26527976989746\n",
      "Epoch: 28 - Batch: 24512 - Loss: 0.738484 - Time:56.67863440513611\n",
      "Epoch: 28 - Batch: 28608 - Loss: 0.758212 - Time:66.0917797088623\n",
      "Epoch: 28 - Batch: 32704 - Loss: 1.032065 - Time:75.50636792182922\n",
      "Epoch: 28 - Batch: 36800 - Loss: 0.140979 - Time:84.94565677642822\n",
      "Epoch: 28 - Batch: 40896 - Loss: 0.417183 - Time:94.35537672042847\n",
      "Epoch: 28 - Batch: 44992 - Loss: 0.572056 - Time:103.76738095283508\n",
      "Epoch: 28 - Batch: 49088 - Loss: 0.578009 - Time:113.17952990531921\n",
      "Epoch: 28 - Batch: 53184 - Loss: 0.531816 - Time:122.61028027534485\n",
      "Epoch: 28 - Batch: 57280 - Loss: 0.591199 - Time:132.02262425422668\n",
      "Epoch: 28 - Batch: 61376 - Loss: 0.687896 - Time:141.43511533737183\n",
      "Epoch: 28 - Batch: 65472 - Loss: 0.682062 - Time:150.86366748809814\n",
      "Epoch: 28 - Batch: 69568 - Loss: 0.457922 - Time:160.27569770812988\n",
      "Epoch: 28 - Batch: 73664 - Loss: 1.304838 - Time:169.6868417263031\n",
      "Epoch: 28 - Batch: 77760 - Loss: 0.606771 - Time:179.09922981262207\n",
      "Epoch: 28 - Batch: 81856 - Loss: 1.285995 - Time:188.51305747032166\n",
      "Epoch: 28 - Batch: 85952 - Loss: 0.787790 - Time:197.92254734039307\n",
      "Epoch: 28 - Batch: 90048 - Loss: 0.362194 - Time:207.33539032936096\n",
      "Epoch: 28 - Batch: 94144 - Loss: 0.620459 - Time:216.7495846748352\n",
      "Epoch: 28 - Batch: 98240 - Loss: 0.738917 - Time:226.17972993850708\n",
      "Epoch: 28 - Batch: 102336 - Loss: 0.944980 - Time:235.592866897583\n",
      "Epoch: 28 - Batch: 106432 - Loss: 0.682288 - Time:245.0056824684143\n",
      "Epoch: 28 - Batch: 110528 - Loss: 0.883569 - Time:254.41742300987244\n",
      "Epoch: 28 - Batch: 114624 - Loss: 0.830686 - Time:263.84569096565247\n",
      "Epoch: 28 - Batch: 118720 - Loss: 0.729264 - Time:273.2553720474243\n",
      "Epoch: 28 - Batch: 122816 - Loss: 0.997485 - Time:282.66910099983215\n",
      "Epoch: 28 - Batch: 126912 - Loss: 0.987403 - Time:292.0974452495575\n",
      "Epoch: 28 - Batch: 131008 - Loss: 0.767691 - Time:301.5084385871887\n",
      "Epoch: 28 - Batch: 135104 - Loss: 1.406674 - Time:310.9214689731598\n",
      "Epoch: 28 - Batch: 139200 - Loss: 0.450141 - Time:320.3359031677246\n",
      "Epoch: 28 - Batch: 143296 - Loss: 0.825231 - Time:329.74912095069885\n",
      "Epoch: 28 - Batch: 147392 - Loss: 0.790291 - Time:339.1621515750885\n",
      "Epoch: 28 - Batch: 151488 - Loss: 0.809891 - Time:348.5753815174103\n",
      "Epoch: 29 - Batch: 4032 - Loss: 0.233563 - Time:9.607709407806396\n",
      "Epoch: 29 - Batch: 8128 - Loss: 0.942971 - Time:19.02215003967285\n",
      "Epoch: 29 - Batch: 12224 - Loss: 0.672731 - Time:28.437195539474487\n",
      "Epoch: 29 - Batch: 16320 - Loss: 0.086784 - Time:37.85096049308777\n",
      "Epoch: 29 - Batch: 20416 - Loss: 1.198886 - Time:47.265620708465576\n",
      "Epoch: 29 - Batch: 24512 - Loss: 0.379337 - Time:56.67978882789612\n",
      "Epoch: 29 - Batch: 28608 - Loss: 0.488494 - Time:66.09105229377747\n",
      "Epoch: 29 - Batch: 32704 - Loss: 0.317836 - Time:75.50030374526978\n",
      "Epoch: 29 - Batch: 36800 - Loss: 0.625478 - Time:84.93111395835876\n",
      "Epoch: 29 - Batch: 40896 - Loss: 0.761736 - Time:94.34190607070923\n",
      "Epoch: 29 - Batch: 44992 - Loss: 0.339994 - Time:103.75434517860413\n",
      "Epoch: 29 - Batch: 49088 - Loss: 0.597754 - Time:113.16552305221558\n",
      "Epoch: 29 - Batch: 53184 - Loss: 0.763527 - Time:122.59470629692078\n",
      "Epoch: 29 - Batch: 57280 - Loss: 0.429052 - Time:132.00570678710938\n",
      "Epoch: 29 - Batch: 61376 - Loss: 0.806980 - Time:141.41613745689392\n",
      "Epoch: 29 - Batch: 65472 - Loss: 0.951322 - Time:150.8473162651062\n",
      "Epoch: 29 - Batch: 69568 - Loss: 0.742696 - Time:160.25883889198303\n",
      "Epoch: 29 - Batch: 73664 - Loss: 0.219880 - Time:169.67033696174622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 - Batch: 77760 - Loss: 0.616160 - Time:179.082355260849\n",
      "Epoch: 29 - Batch: 81856 - Loss: 0.453958 - Time:188.4936158657074\n",
      "Epoch: 29 - Batch: 85952 - Loss: 0.924420 - Time:197.90556383132935\n",
      "Epoch: 29 - Batch: 90048 - Loss: 1.208270 - Time:207.31717014312744\n",
      "Epoch: 29 - Batch: 94144 - Loss: 0.801770 - Time:216.72801899909973\n",
      "Epoch: 29 - Batch: 98240 - Loss: 0.533206 - Time:226.1541051864624\n",
      "Epoch: 29 - Batch: 102336 - Loss: 0.153522 - Time:235.56451201438904\n",
      "Epoch: 29 - Batch: 106432 - Loss: 0.515065 - Time:244.97466707229614\n",
      "Epoch: 29 - Batch: 110528 - Loss: 0.250482 - Time:254.38782715797424\n",
      "Epoch: 29 - Batch: 114624 - Loss: 0.935582 - Time:263.8163917064667\n",
      "Epoch: 29 - Batch: 118720 - Loss: 0.943489 - Time:273.2274899482727\n",
      "Epoch: 29 - Batch: 122816 - Loss: 0.729786 - Time:282.6408348083496\n",
      "Epoch: 29 - Batch: 126912 - Loss: 0.582377 - Time:292.0699028968811\n",
      "Epoch: 29 - Batch: 131008 - Loss: 0.557575 - Time:301.4814410209656\n",
      "Epoch: 29 - Batch: 135104 - Loss: 0.568845 - Time:310.89422488212585\n",
      "Epoch: 29 - Batch: 139200 - Loss: 0.616178 - Time:320.3075134754181\n",
      "Epoch: 29 - Batch: 143296 - Loss: 0.860761 - Time:329.71895384788513\n",
      "Epoch: 29 - Batch: 147392 - Loss: 0.363459 - Time:339.1326677799225\n",
      "Epoch: 29 - Batch: 151488 - Loss: 0.452238 - Time:348.5475957393646\n",
      "Epoch: 30 - Batch: 4032 - Loss: 0.352122 - Time:9.607086181640625\n",
      "Epoch: 30 - Batch: 8128 - Loss: 0.478558 - Time:19.020515203475952\n",
      "Epoch: 30 - Batch: 12224 - Loss: 0.453252 - Time:28.43457841873169\n",
      "Epoch: 30 - Batch: 16320 - Loss: 0.194521 - Time:37.844831466674805\n",
      "Epoch: 30 - Batch: 20416 - Loss: 0.401125 - Time:47.256911754608154\n",
      "Epoch: 30 - Batch: 24512 - Loss: 0.419329 - Time:56.66774845123291\n",
      "Epoch: 30 - Batch: 28608 - Loss: 0.635741 - Time:66.07931661605835\n",
      "Epoch: 30 - Batch: 32704 - Loss: 0.326455 - Time:75.49280095100403\n",
      "Epoch: 30 - Batch: 36800 - Loss: 0.117535 - Time:84.92005753517151\n",
      "Epoch: 30 - Batch: 40896 - Loss: 0.162519 - Time:94.33049321174622\n",
      "Epoch: 30 - Batch: 44992 - Loss: 0.502795 - Time:103.74386239051819\n",
      "Epoch: 30 - Batch: 49088 - Loss: 0.597524 - Time:113.15535926818848\n",
      "Epoch: 30 - Batch: 53184 - Loss: 0.319656 - Time:122.58384490013123\n",
      "Epoch: 30 - Batch: 57280 - Loss: 0.574556 - Time:131.99428606033325\n",
      "Epoch: 30 - Batch: 61376 - Loss: 0.500594 - Time:141.40563988685608\n",
      "Epoch: 30 - Batch: 65472 - Loss: 0.788904 - Time:150.83185124397278\n",
      "Epoch: 30 - Batch: 69568 - Loss: 0.693908 - Time:160.24292182922363\n",
      "Epoch: 30 - Batch: 73664 - Loss: 0.331625 - Time:169.6562521457672\n",
      "Epoch: 30 - Batch: 77760 - Loss: 0.390250 - Time:179.07023525238037\n",
      "Epoch: 30 - Batch: 81856 - Loss: 0.586603 - Time:188.48227787017822\n",
      "Epoch: 30 - Batch: 85952 - Loss: 0.447828 - Time:197.89504504203796\n",
      "Epoch: 30 - Batch: 90048 - Loss: 0.378799 - Time:207.30709528923035\n",
      "Epoch: 30 - Batch: 94144 - Loss: 0.849533 - Time:216.71969151496887\n",
      "Epoch: 30 - Batch: 98240 - Loss: 0.386401 - Time:226.14900541305542\n",
      "Epoch: 30 - Batch: 102336 - Loss: 0.378656 - Time:235.56098985671997\n",
      "Epoch: 30 - Batch: 106432 - Loss: 0.383792 - Time:244.97322273254395\n",
      "Epoch: 30 - Batch: 110528 - Loss: 1.001051 - Time:254.38554739952087\n",
      "Epoch: 30 - Batch: 114624 - Loss: 0.330166 - Time:263.7984025478363\n",
      "Epoch: 30 - Batch: 118720 - Loss: 0.813847 - Time:273.2276554107666\n",
      "Epoch: 30 - Batch: 122816 - Loss: 0.984844 - Time:282.64079546928406\n",
      "Epoch: 30 - Batch: 126912 - Loss: 0.733778 - Time:292.07259821891785\n",
      "Epoch: 30 - Batch: 131008 - Loss: 0.234859 - Time:301.4855725765228\n",
      "Epoch: 30 - Batch: 135104 - Loss: 0.186141 - Time:310.8975176811218\n",
      "Epoch: 30 - Batch: 139200 - Loss: 0.317019 - Time:320.3112931251526\n",
      "Epoch: 30 - Batch: 143296 - Loss: 0.566100 - Time:329.7220537662506\n",
      "Epoch: 30 - Batch: 147392 - Loss: 0.899765 - Time:339.1329650878906\n",
      "Epoch: 30 - Batch: 151488 - Loss: 0.619371 - Time:348.54370307922363\n",
      "Epoch: 31 - Batch: 4032 - Loss: 0.273243 - Time:9.603269338607788\n",
      "Epoch: 31 - Batch: 8128 - Loss: 0.377733 - Time:19.016122341156006\n",
      "Epoch: 31 - Batch: 12224 - Loss: 0.415512 - Time:28.4277765750885\n",
      "Epoch: 31 - Batch: 16320 - Loss: 0.597109 - Time:37.841025829315186\n",
      "Epoch: 31 - Batch: 20416 - Loss: 0.465878 - Time:47.25060033798218\n",
      "Epoch: 31 - Batch: 24512 - Loss: 0.383018 - Time:56.664323568344116\n",
      "Epoch: 31 - Batch: 28608 - Loss: 0.240313 - Time:66.07639479637146\n",
      "Epoch: 31 - Batch: 32704 - Loss: 0.654780 - Time:75.48856949806213\n",
      "Epoch: 31 - Batch: 36800 - Loss: 0.422071 - Time:84.92014122009277\n",
      "Epoch: 31 - Batch: 40896 - Loss: 0.417195 - Time:94.33184170722961\n",
      "Epoch: 31 - Batch: 44992 - Loss: 0.568730 - Time:103.74554681777954\n",
      "Epoch: 31 - Batch: 49088 - Loss: 0.665115 - Time:113.1585853099823\n",
      "Epoch: 31 - Batch: 53184 - Loss: 0.208810 - Time:122.58767628669739\n",
      "Epoch: 31 - Batch: 57280 - Loss: 0.415360 - Time:132.0026831626892\n",
      "Epoch: 31 - Batch: 61376 - Loss: 0.434121 - Time:141.41567087173462\n",
      "Epoch: 31 - Batch: 65472 - Loss: 0.588930 - Time:150.85029768943787\n",
      "Epoch: 31 - Batch: 69568 - Loss: 0.389830 - Time:160.26434969902039\n",
      "Epoch: 31 - Batch: 73664 - Loss: 0.608642 - Time:169.6784906387329\n",
      "Epoch: 31 - Batch: 77760 - Loss: 0.760703 - Time:179.09199357032776\n",
      "Epoch: 31 - Batch: 81856 - Loss: 0.356096 - Time:188.5036005973816\n",
      "Epoch: 31 - Batch: 85952 - Loss: 0.599461 - Time:197.91674184799194\n",
      "Epoch: 31 - Batch: 90048 - Loss: 0.565729 - Time:207.33063197135925\n",
      "Epoch: 31 - Batch: 94144 - Loss: 0.983029 - Time:216.74525952339172\n",
      "Epoch: 31 - Batch: 98240 - Loss: 0.689262 - Time:226.1761758327484\n",
      "Epoch: 31 - Batch: 102336 - Loss: 0.558103 - Time:235.58889603614807\n",
      "Epoch: 31 - Batch: 106432 - Loss: 0.723703 - Time:245.00238919258118\n",
      "Epoch: 31 - Batch: 110528 - Loss: 0.281874 - Time:254.4158229827881\n",
      "Epoch: 31 - Batch: 114624 - Loss: 1.204255 - Time:263.8311276435852\n",
      "Epoch: 31 - Batch: 118720 - Loss: 0.868863 - Time:273.2589461803436\n",
      "Epoch: 31 - Batch: 122816 - Loss: 0.734841 - Time:282.67025923728943\n",
      "Epoch: 31 - Batch: 126912 - Loss: 0.751774 - Time:292.1004183292389\n",
      "Epoch: 31 - Batch: 131008 - Loss: 0.317626 - Time:301.51434898376465\n",
      "Epoch: 31 - Batch: 135104 - Loss: 0.358655 - Time:310.9256098270416\n",
      "Epoch: 31 - Batch: 139200 - Loss: 0.564116 - Time:320.33929800987244\n",
      "Epoch: 31 - Batch: 143296 - Loss: 0.384983 - Time:329.7499256134033\n",
      "Epoch: 31 - Batch: 147392 - Loss: 0.401303 - Time:339.16009545326233\n",
      "Epoch: 31 - Batch: 151488 - Loss: 0.659591 - Time:348.57094621658325\n",
      "Epoch: 32 - Batch: 4032 - Loss: 0.259947 - Time:9.57215929031372\n",
      "Epoch: 32 - Batch: 8128 - Loss: 0.377875 - Time:19.01836657524109\n",
      "Epoch: 32 - Batch: 12224 - Loss: 0.460986 - Time:28.43029808998108\n",
      "Epoch: 32 - Batch: 16320 - Loss: 0.462827 - Time:37.84458374977112\n",
      "Epoch: 32 - Batch: 20416 - Loss: 0.303683 - Time:47.258352279663086\n",
      "Epoch: 32 - Batch: 24512 - Loss: 0.267584 - Time:56.68808436393738\n",
      "Epoch: 32 - Batch: 28608 - Loss: 0.158674 - Time:66.10042810440063\n",
      "Epoch: 32 - Batch: 32704 - Loss: 0.386201 - Time:75.5120096206665\n",
      "Epoch: 32 - Batch: 36800 - Loss: 0.359820 - Time:84.9406988620758\n",
      "Epoch: 32 - Batch: 40896 - Loss: 0.360676 - Time:94.34876942634583\n",
      "Epoch: 32 - Batch: 44992 - Loss: 0.304530 - Time:103.7594096660614\n",
      "Epoch: 32 - Batch: 49088 - Loss: 0.374953 - Time:113.17003750801086\n",
      "Epoch: 32 - Batch: 53184 - Loss: 0.431156 - Time:122.58306002616882\n",
      "Epoch: 32 - Batch: 57280 - Loss: 0.304289 - Time:131.99989986419678\n",
      "Epoch: 32 - Batch: 61376 - Loss: 0.385888 - Time:141.41416692733765\n",
      "Epoch: 32 - Batch: 65472 - Loss: 0.603813 - Time:150.82773566246033\n",
      "Epoch: 32 - Batch: 69568 - Loss: 0.388388 - Time:160.26004481315613\n",
      "Epoch: 32 - Batch: 73664 - Loss: 0.399172 - Time:169.67140936851501\n",
      "Epoch: 32 - Batch: 77760 - Loss: 0.321650 - Time:179.08636212348938\n",
      "Epoch: 32 - Batch: 81856 - Loss: 0.581956 - Time:188.49985766410828\n",
      "Epoch: 32 - Batch: 85952 - Loss: 0.474223 - Time:197.93244433403015\n",
      "Epoch: 32 - Batch: 90048 - Loss: 0.444864 - Time:207.345365524292\n",
      "Epoch: 32 - Batch: 94144 - Loss: 0.913600 - Time:216.7570996284485\n",
      "Epoch: 32 - Batch: 98240 - Loss: 0.606033 - Time:226.18773365020752\n",
      "Epoch: 32 - Batch: 102336 - Loss: 0.341956 - Time:235.60035943984985\n",
      "Epoch: 32 - Batch: 106432 - Loss: 0.684214 - Time:245.01158595085144\n",
      "Epoch: 32 - Batch: 110528 - Loss: 0.829392 - Time:254.42291831970215\n",
      "Epoch: 32 - Batch: 114624 - Loss: 0.998178 - Time:263.83647656440735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 - Batch: 118720 - Loss: 0.295053 - Time:273.2486526966095\n",
      "Epoch: 32 - Batch: 122816 - Loss: 0.684572 - Time:282.66249322891235\n",
      "Epoch: 32 - Batch: 126912 - Loss: 0.728923 - Time:292.077917098999\n",
      "Epoch: 32 - Batch: 131008 - Loss: 0.692843 - Time:301.5122721195221\n",
      "Epoch: 32 - Batch: 135104 - Loss: 0.705307 - Time:310.9241602420807\n",
      "Epoch: 32 - Batch: 139200 - Loss: 0.407440 - Time:320.3360140323639\n",
      "Epoch: 32 - Batch: 143296 - Loss: 1.250526 - Time:329.7504360675812\n",
      "Epoch: 32 - Batch: 147392 - Loss: 0.271700 - Time:339.16427278518677\n",
      "Epoch: 32 - Batch: 151488 - Loss: 0.836684 - Time:348.5953269004822\n",
      "Epoch: 33 - Batch: 4032 - Loss: 0.110978 - Time:9.578471660614014\n",
      "Epoch: 33 - Batch: 8128 - Loss: 0.158629 - Time:18.995579957962036\n",
      "Epoch: 33 - Batch: 12224 - Loss: 0.347536 - Time:28.43748950958252\n",
      "Epoch: 33 - Batch: 16320 - Loss: 0.330044 - Time:37.851792335510254\n",
      "Epoch: 33 - Batch: 20416 - Loss: 0.234396 - Time:47.28148531913757\n",
      "Epoch: 33 - Batch: 24512 - Loss: 0.162823 - Time:56.6924774646759\n",
      "Epoch: 33 - Batch: 28608 - Loss: 0.224041 - Time:66.10544729232788\n",
      "Epoch: 33 - Batch: 32704 - Loss: 0.226550 - Time:75.51804900169373\n",
      "Epoch: 33 - Batch: 36800 - Loss: 0.608882 - Time:84.9282796382904\n",
      "Epoch: 33 - Batch: 40896 - Loss: 0.505109 - Time:94.33869099617004\n",
      "Epoch: 33 - Batch: 44992 - Loss: 0.235073 - Time:103.74976706504822\n",
      "Epoch: 33 - Batch: 49088 - Loss: 0.364435 - Time:113.1631269454956\n",
      "Epoch: 33 - Batch: 53184 - Loss: 0.576172 - Time:122.57504057884216\n",
      "Epoch: 33 - Batch: 57280 - Loss: 0.788963 - Time:132.00645470619202\n",
      "Epoch: 33 - Batch: 61376 - Loss: 0.801345 - Time:141.42183685302734\n",
      "Epoch: 33 - Batch: 65472 - Loss: 0.414198 - Time:150.8375165462494\n",
      "Epoch: 33 - Batch: 69568 - Loss: 0.486066 - Time:160.25145816802979\n",
      "Epoch: 33 - Batch: 73664 - Loss: 0.287938 - Time:169.6824173927307\n",
      "Epoch: 33 - Batch: 77760 - Loss: 0.149333 - Time:179.0956473350525\n",
      "Epoch: 33 - Batch: 81856 - Loss: 0.555627 - Time:188.50846195220947\n",
      "Epoch: 33 - Batch: 85952 - Loss: 0.582824 - Time:197.9391222000122\n",
      "Epoch: 33 - Batch: 90048 - Loss: 0.290484 - Time:207.35345935821533\n",
      "Epoch: 33 - Batch: 94144 - Loss: 0.289512 - Time:216.76565957069397\n",
      "Epoch: 33 - Batch: 98240 - Loss: 0.243819 - Time:226.1781678199768\n",
      "Epoch: 33 - Batch: 102336 - Loss: 0.659591 - Time:235.58921313285828\n",
      "Epoch: 33 - Batch: 106432 - Loss: 1.240961 - Time:245.00068402290344\n",
      "Epoch: 33 - Batch: 110528 - Loss: 0.654674 - Time:254.41225600242615\n",
      "Epoch: 33 - Batch: 114624 - Loss: 0.651018 - Time:263.824782371521\n",
      "Epoch: 33 - Batch: 118720 - Loss: 0.512622 - Time:273.25478291511536\n",
      "Epoch: 33 - Batch: 122816 - Loss: 0.294097 - Time:282.6671071052551\n",
      "Epoch: 33 - Batch: 126912 - Loss: 0.537514 - Time:292.07939314842224\n",
      "Epoch: 33 - Batch: 131008 - Loss: 0.399086 - Time:301.4922697544098\n",
      "Epoch: 33 - Batch: 135104 - Loss: 0.454121 - Time:310.9230737686157\n",
      "Epoch: 33 - Batch: 139200 - Loss: 0.611732 - Time:320.3349812030792\n",
      "Epoch: 33 - Batch: 143296 - Loss: 0.468307 - Time:329.7472393512726\n",
      "Epoch: 33 - Batch: 147392 - Loss: 0.403962 - Time:339.1771550178528\n",
      "Epoch: 33 - Batch: 151488 - Loss: 0.334079 - Time:348.5903103351593\n",
      "Epoch: 34 - Batch: 4032 - Loss: 0.499021 - Time:9.576887130737305\n",
      "Epoch: 34 - Batch: 8128 - Loss: 0.328222 - Time:18.990209579467773\n",
      "Epoch: 34 - Batch: 12224 - Loss: 0.120122 - Time:28.43286156654358\n",
      "Epoch: 34 - Batch: 16320 - Loss: 0.434072 - Time:37.846126317977905\n",
      "Epoch: 34 - Batch: 20416 - Loss: 0.376682 - Time:47.25800132751465\n",
      "Epoch: 34 - Batch: 24512 - Loss: 0.279583 - Time:56.67012310028076\n",
      "Epoch: 34 - Batch: 28608 - Loss: 0.403190 - Time:66.08314609527588\n",
      "Epoch: 34 - Batch: 32704 - Loss: 0.317860 - Time:75.49707913398743\n",
      "Epoch: 34 - Batch: 36800 - Loss: 0.403842 - Time:84.90958261489868\n",
      "Epoch: 34 - Batch: 40896 - Loss: 0.295951 - Time:94.3210780620575\n",
      "Epoch: 34 - Batch: 44992 - Loss: 0.335191 - Time:103.74946284294128\n",
      "Epoch: 34 - Batch: 49088 - Loss: 0.218126 - Time:113.15945029258728\n",
      "Epoch: 34 - Batch: 53184 - Loss: 0.478007 - Time:122.5703055858612\n",
      "Epoch: 34 - Batch: 57280 - Loss: 0.637975 - Time:131.98208284378052\n",
      "Epoch: 34 - Batch: 61376 - Loss: 0.352580 - Time:141.41174483299255\n",
      "Epoch: 34 - Batch: 65472 - Loss: 0.641856 - Time:150.82246041297913\n",
      "Epoch: 34 - Batch: 69568 - Loss: 0.471020 - Time:160.2335650920868\n",
      "Epoch: 34 - Batch: 73664 - Loss: 0.614875 - Time:169.6611568927765\n",
      "Epoch: 34 - Batch: 77760 - Loss: 0.673083 - Time:179.0713620185852\n",
      "Epoch: 34 - Batch: 81856 - Loss: 0.771430 - Time:188.48271822929382\n",
      "Epoch: 34 - Batch: 85952 - Loss: 0.157597 - Time:197.89270401000977\n",
      "Epoch: 34 - Batch: 90048 - Loss: 0.778529 - Time:207.30557513237\n",
      "Epoch: 34 - Batch: 94144 - Loss: 0.581687 - Time:216.71847486495972\n",
      "Epoch: 34 - Batch: 98240 - Loss: 0.344160 - Time:226.13064193725586\n",
      "Epoch: 34 - Batch: 102336 - Loss: 0.134898 - Time:235.54239177703857\n",
      "Epoch: 34 - Batch: 106432 - Loss: 0.589062 - Time:244.97155499458313\n",
      "Epoch: 34 - Batch: 110528 - Loss: 0.672278 - Time:254.38213443756104\n",
      "Epoch: 34 - Batch: 114624 - Loss: 0.586373 - Time:263.79334688186646\n",
      "Epoch: 34 - Batch: 118720 - Loss: 0.688170 - Time:273.20726227760315\n",
      "Epoch: 34 - Batch: 122816 - Loss: 0.493830 - Time:282.63931107521057\n",
      "Epoch: 34 - Batch: 126912 - Loss: 0.476622 - Time:292.0529479980469\n",
      "Epoch: 34 - Batch: 131008 - Loss: 0.294709 - Time:301.46465730667114\n",
      "Epoch: 34 - Batch: 135104 - Loss: 0.204327 - Time:310.8946077823639\n",
      "Epoch: 34 - Batch: 139200 - Loss: 0.763150 - Time:320.3078374862671\n",
      "Epoch: 34 - Batch: 143296 - Loss: 0.203943 - Time:329.7219829559326\n",
      "Epoch: 34 - Batch: 147392 - Loss: 0.532614 - Time:339.1376338005066\n",
      "Epoch: 34 - Batch: 151488 - Loss: 0.283726 - Time:348.5512137413025\n",
      "Epoch: 35 - Batch: 4032 - Loss: 0.358745 - Time:9.574310779571533\n",
      "Epoch: 35 - Batch: 8128 - Loss: 0.581782 - Time:19.01660180091858\n",
      "Epoch: 35 - Batch: 12224 - Loss: 0.131260 - Time:28.430821895599365\n",
      "Epoch: 35 - Batch: 16320 - Loss: 0.158320 - Time:37.8424117565155\n",
      "Epoch: 35 - Batch: 20416 - Loss: 0.200012 - Time:47.25477480888367\n",
      "Epoch: 35 - Batch: 24512 - Loss: 0.281835 - Time:56.667564392089844\n",
      "Epoch: 35 - Batch: 28608 - Loss: 0.150211 - Time:66.08020091056824\n",
      "Epoch: 35 - Batch: 32704 - Loss: 0.521169 - Time:75.49503755569458\n",
      "Epoch: 35 - Batch: 36800 - Loss: 0.263497 - Time:84.90896129608154\n",
      "Epoch: 35 - Batch: 40896 - Loss: 0.352824 - Time:94.33916234970093\n",
      "Epoch: 35 - Batch: 44992 - Loss: 0.276672 - Time:103.7512788772583\n",
      "Epoch: 35 - Batch: 49088 - Loss: 0.122599 - Time:113.16137957572937\n",
      "Epoch: 35 - Batch: 53184 - Loss: 0.208712 - Time:122.5740954875946\n",
      "Epoch: 35 - Batch: 57280 - Loss: 0.198383 - Time:132.00182914733887\n",
      "Epoch: 35 - Batch: 61376 - Loss: 0.422313 - Time:141.4147915840149\n",
      "Epoch: 35 - Batch: 65472 - Loss: 0.222967 - Time:150.82736158370972\n",
      "Epoch: 35 - Batch: 69568 - Loss: 0.449043 - Time:160.2552695274353\n",
      "Epoch: 35 - Batch: 73664 - Loss: 0.420004 - Time:169.66608548164368\n",
      "Epoch: 35 - Batch: 77760 - Loss: 0.293534 - Time:179.07704877853394\n",
      "Epoch: 35 - Batch: 81856 - Loss: 0.165149 - Time:188.48882913589478\n",
      "Epoch: 35 - Batch: 85952 - Loss: 0.251854 - Time:197.903169631958\n",
      "Epoch: 35 - Batch: 90048 - Loss: 0.145948 - Time:207.31614899635315\n",
      "Epoch: 35 - Batch: 94144 - Loss: 0.515632 - Time:216.73016238212585\n",
      "Epoch: 35 - Batch: 98240 - Loss: 0.462641 - Time:226.14440894126892\n",
      "Epoch: 35 - Batch: 102336 - Loss: 0.434484 - Time:235.57501602172852\n",
      "Epoch: 35 - Batch: 106432 - Loss: 0.616307 - Time:244.98950219154358\n",
      "Epoch: 35 - Batch: 110528 - Loss: 0.607807 - Time:254.4039602279663\n",
      "Epoch: 35 - Batch: 114624 - Loss: 0.460304 - Time:263.8170907497406\n",
      "Epoch: 35 - Batch: 118720 - Loss: 0.248826 - Time:273.2483229637146\n",
      "Epoch: 35 - Batch: 122816 - Loss: 0.416733 - Time:282.66104435920715\n",
      "Epoch: 35 - Batch: 126912 - Loss: 0.344442 - Time:292.0727961063385\n",
      "Epoch: 35 - Batch: 131008 - Loss: 1.044270 - Time:301.50280809402466\n",
      "Epoch: 35 - Batch: 135104 - Loss: 0.377951 - Time:310.9154863357544\n",
      "Epoch: 35 - Batch: 139200 - Loss: 0.825157 - Time:320.32931184768677\n",
      "Epoch: 35 - Batch: 143296 - Loss: 0.156812 - Time:329.7419307231903\n",
      "Epoch: 35 - Batch: 147392 - Loss: 0.838204 - Time:339.15403151512146\n",
      "Epoch: 35 - Batch: 151488 - Loss: 0.201277 - Time:348.5660126209259\n",
      "Epoch: 36 - Batch: 4032 - Loss: 0.196171 - Time:9.58602523803711\n",
      "Epoch: 36 - Batch: 8128 - Loss: 0.209764 - Time:19.002585887908936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 - Batch: 12224 - Loss: 0.690880 - Time:28.447436571121216\n",
      "Epoch: 36 - Batch: 16320 - Loss: 0.465502 - Time:37.861512184143066\n",
      "Epoch: 36 - Batch: 20416 - Loss: 0.180896 - Time:47.2772262096405\n",
      "Epoch: 36 - Batch: 24512 - Loss: 0.451564 - Time:56.69436192512512\n",
      "Epoch: 36 - Batch: 28608 - Loss: 0.513117 - Time:66.12439179420471\n",
      "Epoch: 36 - Batch: 32704 - Loss: 0.701317 - Time:75.53666472434998\n",
      "Epoch: 36 - Batch: 36800 - Loss: 0.690417 - Time:84.95170640945435\n",
      "Epoch: 36 - Batch: 40896 - Loss: 0.329115 - Time:94.37968468666077\n",
      "Epoch: 36 - Batch: 44992 - Loss: 0.639112 - Time:103.79253602027893\n",
      "Epoch: 36 - Batch: 49088 - Loss: 0.138084 - Time:113.2064151763916\n",
      "Epoch: 36 - Batch: 53184 - Loss: 0.314027 - Time:122.61642074584961\n",
      "Epoch: 36 - Batch: 57280 - Loss: 0.537393 - Time:132.0286123752594\n",
      "Epoch: 36 - Batch: 61376 - Loss: 0.292070 - Time:141.44454288482666\n",
      "Epoch: 36 - Batch: 65472 - Loss: 0.231553 - Time:150.85862016677856\n",
      "Epoch: 36 - Batch: 69568 - Loss: 0.737411 - Time:160.27358865737915\n",
      "Epoch: 36 - Batch: 73664 - Loss: 0.285279 - Time:169.7026731967926\n",
      "Epoch: 36 - Batch: 77760 - Loss: 0.520316 - Time:179.11477160453796\n",
      "Epoch: 36 - Batch: 81856 - Loss: 0.323436 - Time:188.52751660346985\n",
      "Epoch: 36 - Batch: 85952 - Loss: 0.294959 - Time:197.94009685516357\n",
      "Epoch: 36 - Batch: 90048 - Loss: 0.841841 - Time:207.368239402771\n",
      "Epoch: 36 - Batch: 94144 - Loss: 0.431850 - Time:216.77883315086365\n",
      "Epoch: 36 - Batch: 98240 - Loss: 0.284853 - Time:226.19000101089478\n",
      "Epoch: 36 - Batch: 102336 - Loss: 0.710122 - Time:235.6187310218811\n",
      "Epoch: 36 - Batch: 106432 - Loss: 0.259328 - Time:245.03135538101196\n",
      "Epoch: 36 - Batch: 110528 - Loss: 0.347537 - Time:254.4448480606079\n",
      "Epoch: 36 - Batch: 114624 - Loss: 0.222907 - Time:263.85765290260315\n",
      "Epoch: 36 - Batch: 118720 - Loss: 0.390939 - Time:273.26979327201843\n",
      "Epoch: 36 - Batch: 122816 - Loss: 0.471457 - Time:282.6847348213196\n",
      "Epoch: 36 - Batch: 126912 - Loss: 0.309899 - Time:292.09967970848083\n",
      "Epoch: 36 - Batch: 131008 - Loss: 0.269736 - Time:301.5130259990692\n",
      "Epoch: 36 - Batch: 135104 - Loss: 0.245337 - Time:310.94338965415955\n",
      "Epoch: 36 - Batch: 139200 - Loss: 0.537909 - Time:320.3592092990875\n",
      "Epoch: 36 - Batch: 143296 - Loss: 0.275942 - Time:329.7733459472656\n",
      "Epoch: 36 - Batch: 147392 - Loss: 0.448312 - Time:339.1868498325348\n",
      "Epoch: 36 - Batch: 151488 - Loss: 0.933807 - Time:348.61473989486694\n",
      "Epoch: 37 - Batch: 4032 - Loss: 0.397672 - Time:9.571620225906372\n",
      "Epoch: 37 - Batch: 8128 - Loss: 0.068818 - Time:18.98825478553772\n",
      "Epoch: 37 - Batch: 12224 - Loss: 0.308578 - Time:28.401492834091187\n",
      "Epoch: 37 - Batch: 16320 - Loss: 0.258550 - Time:37.84426259994507\n",
      "Epoch: 37 - Batch: 20416 - Loss: 0.333893 - Time:47.25851655006409\n",
      "Epoch: 37 - Batch: 24512 - Loss: 0.145082 - Time:56.688772678375244\n",
      "Epoch: 37 - Batch: 28608 - Loss: 0.298130 - Time:66.10327219963074\n",
      "Epoch: 37 - Batch: 32704 - Loss: 0.297689 - Time:75.51628589630127\n",
      "Epoch: 37 - Batch: 36800 - Loss: 0.597552 - Time:84.92917466163635\n",
      "Epoch: 37 - Batch: 40896 - Loss: 0.640975 - Time:94.34187626838684\n",
      "Epoch: 37 - Batch: 44992 - Loss: 0.235729 - Time:103.75455093383789\n",
      "Epoch: 37 - Batch: 49088 - Loss: 0.526096 - Time:113.16719174385071\n",
      "Epoch: 37 - Batch: 53184 - Loss: 0.433868 - Time:122.58241677284241\n",
      "Epoch: 37 - Batch: 57280 - Loss: 0.088161 - Time:131.99707102775574\n",
      "Epoch: 37 - Batch: 61376 - Loss: 0.345760 - Time:141.4267237186432\n",
      "Epoch: 37 - Batch: 65472 - Loss: 0.453548 - Time:150.8394696712494\n",
      "Epoch: 37 - Batch: 69568 - Loss: 0.233216 - Time:160.25016593933105\n",
      "Epoch: 37 - Batch: 73664 - Loss: 0.546669 - Time:169.66383910179138\n",
      "Epoch: 37 - Batch: 77760 - Loss: 0.850322 - Time:179.0931122303009\n",
      "Epoch: 37 - Batch: 81856 - Loss: 0.389060 - Time:188.50303053855896\n",
      "Epoch: 37 - Batch: 85952 - Loss: 0.171659 - Time:197.91798567771912\n",
      "Epoch: 37 - Batch: 90048 - Loss: 0.296800 - Time:207.34916257858276\n",
      "Epoch: 37 - Batch: 94144 - Loss: 0.407724 - Time:216.7618546485901\n",
      "Epoch: 37 - Batch: 98240 - Loss: 0.335218 - Time:226.1756672859192\n",
      "Epoch: 37 - Batch: 102336 - Loss: 0.730150 - Time:235.58960819244385\n",
      "Epoch: 37 - Batch: 106432 - Loss: 0.223918 - Time:245.00397944450378\n",
      "Epoch: 37 - Batch: 110528 - Loss: 0.326348 - Time:254.41708421707153\n",
      "Epoch: 37 - Batch: 114624 - Loss: 0.501493 - Time:263.8293356895447\n",
      "Epoch: 37 - Batch: 118720 - Loss: 0.361601 - Time:273.24408864974976\n",
      "Epoch: 37 - Batch: 122816 - Loss: 0.258086 - Time:282.6742877960205\n",
      "Epoch: 37 - Batch: 126912 - Loss: 0.264142 - Time:292.08903980255127\n",
      "Epoch: 37 - Batch: 131008 - Loss: 0.269135 - Time:301.5039699077606\n",
      "Epoch: 37 - Batch: 135104 - Loss: 1.002215 - Time:310.9161846637726\n",
      "Epoch: 37 - Batch: 139200 - Loss: 0.593283 - Time:320.3440999984741\n",
      "Epoch: 37 - Batch: 143296 - Loss: 0.469171 - Time:329.7573125362396\n",
      "Epoch: 37 - Batch: 147392 - Loss: 0.495690 - Time:339.1671817302704\n",
      "Epoch: 37 - Batch: 151488 - Loss: 0.407770 - Time:348.59536266326904\n",
      "Epoch: 38 - Batch: 4032 - Loss: 0.332262 - Time:9.57954478263855\n",
      "Epoch: 38 - Batch: 8128 - Loss: 0.232379 - Time:18.99318265914917\n",
      "Epoch: 38 - Batch: 12224 - Loss: 0.427987 - Time:28.436593532562256\n",
      "Epoch: 38 - Batch: 16320 - Loss: 0.678388 - Time:37.85081624984741\n",
      "Epoch: 38 - Batch: 20416 - Loss: 0.642723 - Time:47.26300120353699\n",
      "Epoch: 38 - Batch: 24512 - Loss: 0.602805 - Time:56.67638540267944\n",
      "Epoch: 38 - Batch: 28608 - Loss: 0.303300 - Time:66.08958721160889\n",
      "Epoch: 38 - Batch: 32704 - Loss: 0.496712 - Time:75.50120639801025\n",
      "Epoch: 38 - Batch: 36800 - Loss: 0.192201 - Time:84.9122474193573\n",
      "Epoch: 38 - Batch: 40896 - Loss: 0.222352 - Time:94.32354164123535\n",
      "Epoch: 38 - Batch: 44992 - Loss: 0.269827 - Time:103.75326657295227\n",
      "Epoch: 38 - Batch: 49088 - Loss: 0.483470 - Time:113.17829036712646\n",
      "Epoch: 38 - Batch: 53184 - Loss: 0.388808 - Time:122.64079976081848\n",
      "Epoch: 38 - Batch: 57280 - Loss: 0.430462 - Time:132.05459570884705\n",
      "Epoch: 38 - Batch: 61376 - Loss: 0.116809 - Time:141.48401618003845\n",
      "Epoch: 38 - Batch: 65472 - Loss: 0.143154 - Time:150.89884424209595\n",
      "Epoch: 38 - Batch: 69568 - Loss: 0.315341 - Time:160.31124305725098\n",
      "Epoch: 38 - Batch: 73664 - Loss: 0.310371 - Time:169.73997569084167\n",
      "Epoch: 38 - Batch: 77760 - Loss: 0.286945 - Time:179.15256023406982\n",
      "Epoch: 38 - Batch: 81856 - Loss: 0.370685 - Time:188.56493830680847\n",
      "Epoch: 38 - Batch: 85952 - Loss: 0.633898 - Time:197.97913217544556\n",
      "Epoch: 38 - Batch: 90048 - Loss: 0.552983 - Time:207.3922266960144\n",
      "Epoch: 38 - Batch: 94144 - Loss: 0.186638 - Time:216.80470728874207\n",
      "Epoch: 38 - Batch: 98240 - Loss: 0.419317 - Time:226.21736979484558\n",
      "Epoch: 38 - Batch: 102336 - Loss: 0.069976 - Time:235.63137936592102\n",
      "Epoch: 38 - Batch: 106432 - Loss: 0.554950 - Time:245.06399536132812\n",
      "Epoch: 38 - Batch: 110528 - Loss: 0.688498 - Time:254.47800660133362\n",
      "Epoch: 38 - Batch: 114624 - Loss: 0.490842 - Time:263.8906855583191\n",
      "Epoch: 38 - Batch: 118720 - Loss: 0.512234 - Time:273.3021237850189\n",
      "Epoch: 38 - Batch: 122816 - Loss: 0.736804 - Time:282.71349453926086\n",
      "Epoch: 38 - Batch: 126912 - Loss: 0.167444 - Time:292.14334321022034\n",
      "Epoch: 38 - Batch: 131008 - Loss: 0.168368 - Time:301.5550627708435\n",
      "Epoch: 38 - Batch: 135104 - Loss: 0.219997 - Time:310.9854462146759\n",
      "Epoch: 38 - Batch: 139200 - Loss: 0.364579 - Time:320.39913511276245\n",
      "Epoch: 38 - Batch: 143296 - Loss: 0.126966 - Time:329.8138289451599\n",
      "Epoch: 38 - Batch: 147392 - Loss: 0.306816 - Time:339.2269141674042\n",
      "Epoch: 38 - Batch: 151488 - Loss: 0.242605 - Time:348.64315533638\n",
      "Epoch: 39 - Batch: 4032 - Loss: 0.066135 - Time:9.57409405708313\n",
      "Epoch: 39 - Batch: 8128 - Loss: 0.362434 - Time:19.019201040267944\n",
      "Epoch: 39 - Batch: 12224 - Loss: 0.146634 - Time:28.432801246643066\n",
      "Epoch: 39 - Batch: 16320 - Loss: 0.489667 - Time:37.847140312194824\n",
      "Epoch: 39 - Batch: 20416 - Loss: 0.406270 - Time:47.26066827774048\n",
      "Epoch: 39 - Batch: 24512 - Loss: 0.168952 - Time:56.67413520812988\n",
      "Epoch: 39 - Batch: 28608 - Loss: 0.167069 - Time:66.0867063999176\n",
      "Epoch: 39 - Batch: 32704 - Loss: 0.166530 - Time:75.4991044998169\n",
      "Epoch: 39 - Batch: 36800 - Loss: 0.122185 - Time:84.91256809234619\n",
      "Epoch: 39 - Batch: 40896 - Loss: 0.318868 - Time:94.34466004371643\n",
      "Epoch: 39 - Batch: 44992 - Loss: 0.067645 - Time:103.75773811340332\n",
      "Epoch: 39 - Batch: 49088 - Loss: 0.304710 - Time:113.17293930053711\n",
      "Epoch: 39 - Batch: 53184 - Loss: 0.605840 - Time:122.5862364768982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 - Batch: 57280 - Loss: 0.538790 - Time:132.01464176177979\n",
      "Epoch: 39 - Batch: 61376 - Loss: 0.443541 - Time:141.42846632003784\n",
      "Epoch: 39 - Batch: 65472 - Loss: 0.110108 - Time:150.84142565727234\n",
      "Epoch: 39 - Batch: 69568 - Loss: 0.312474 - Time:160.27061319351196\n",
      "Epoch: 39 - Batch: 73664 - Loss: 0.343237 - Time:169.68198561668396\n",
      "Epoch: 39 - Batch: 77760 - Loss: 0.578046 - Time:179.0971643924713\n",
      "Epoch: 39 - Batch: 81856 - Loss: 0.063395 - Time:188.51327300071716\n",
      "Epoch: 39 - Batch: 85952 - Loss: 0.390243 - Time:197.92824745178223\n",
      "Epoch: 39 - Batch: 90048 - Loss: 0.442798 - Time:207.34460616111755\n",
      "Epoch: 39 - Batch: 94144 - Loss: 0.233981 - Time:216.75875234603882\n",
      "Epoch: 39 - Batch: 98240 - Loss: 0.406427 - Time:226.17365908622742\n",
      "Epoch: 39 - Batch: 102336 - Loss: 0.066316 - Time:235.60363101959229\n",
      "Epoch: 39 - Batch: 106432 - Loss: 0.206101 - Time:245.01649689674377\n",
      "Epoch: 39 - Batch: 110528 - Loss: 0.286912 - Time:254.4290735721588\n",
      "Epoch: 39 - Batch: 114624 - Loss: 0.260761 - Time:263.84338879585266\n",
      "Epoch: 39 - Batch: 118720 - Loss: 0.116638 - Time:273.27516627311707\n",
      "Epoch: 39 - Batch: 122816 - Loss: 0.357090 - Time:282.6920828819275\n",
      "Epoch: 39 - Batch: 126912 - Loss: 0.459374 - Time:292.1079752445221\n",
      "Epoch: 39 - Batch: 131008 - Loss: 0.465537 - Time:301.53998827934265\n",
      "Epoch: 39 - Batch: 135104 - Loss: 0.401812 - Time:310.9552686214447\n",
      "Epoch: 39 - Batch: 139200 - Loss: 0.195084 - Time:320.3683297634125\n",
      "Epoch: 39 - Batch: 143296 - Loss: 0.428010 - Time:329.7810711860657\n",
      "Epoch: 39 - Batch: 147392 - Loss: 0.309279 - Time:339.1983618736267\n",
      "Epoch: 39 - Batch: 151488 - Loss: 0.513973 - Time:348.6123206615448\n",
      "Epoch: 40 - Batch: 4032 - Loss: 0.333496 - Time:9.583025932312012\n",
      "Epoch: 40 - Batch: 8128 - Loss: 0.867412 - Time:18.999500513076782\n",
      "Epoch: 40 - Batch: 12224 - Loss: 0.112972 - Time:28.44335412979126\n",
      "Epoch: 40 - Batch: 16320 - Loss: 0.199763 - Time:37.86033320426941\n",
      "Epoch: 40 - Batch: 20416 - Loss: 0.239730 - Time:47.275389671325684\n",
      "Epoch: 40 - Batch: 24512 - Loss: 0.306491 - Time:56.69148373603821\n",
      "Epoch: 40 - Batch: 28608 - Loss: 0.151227 - Time:66.12420916557312\n",
      "Epoch: 40 - Batch: 32704 - Loss: 0.175333 - Time:75.5387315750122\n",
      "Epoch: 40 - Batch: 36800 - Loss: 0.316911 - Time:84.95236039161682\n",
      "Epoch: 40 - Batch: 40896 - Loss: 0.210709 - Time:94.3822455406189\n",
      "Epoch: 40 - Batch: 44992 - Loss: 0.205615 - Time:103.79561853408813\n",
      "Epoch: 40 - Batch: 49088 - Loss: 0.577333 - Time:113.20710921287537\n",
      "Epoch: 40 - Batch: 53184 - Loss: 0.255918 - Time:122.62022423744202\n",
      "Epoch: 40 - Batch: 57280 - Loss: 0.722039 - Time:132.03428530693054\n",
      "Epoch: 40 - Batch: 61376 - Loss: 0.642339 - Time:141.4480857849121\n",
      "Epoch: 40 - Batch: 65472 - Loss: 0.574688 - Time:150.8595118522644\n",
      "Epoch: 40 - Batch: 69568 - Loss: 0.255615 - Time:160.27157735824585\n",
      "Epoch: 40 - Batch: 73664 - Loss: 0.214444 - Time:169.70245242118835\n",
      "Epoch: 40 - Batch: 77760 - Loss: 0.503483 - Time:179.11500120162964\n",
      "Epoch: 40 - Batch: 81856 - Loss: 0.446669 - Time:188.52909088134766\n",
      "Epoch: 40 - Batch: 85952 - Loss: 0.183241 - Time:197.94307160377502\n",
      "Epoch: 40 - Batch: 90048 - Loss: 0.331146 - Time:207.37255358695984\n",
      "Epoch: 40 - Batch: 94144 - Loss: 0.257250 - Time:216.78658986091614\n",
      "Epoch: 40 - Batch: 98240 - Loss: 0.478353 - Time:226.20028519630432\n",
      "Epoch: 40 - Batch: 102336 - Loss: 0.329377 - Time:235.63253903388977\n",
      "Epoch: 40 - Batch: 106432 - Loss: 0.810349 - Time:245.04748034477234\n",
      "Epoch: 40 - Batch: 110528 - Loss: 0.453489 - Time:254.46085238456726\n",
      "Epoch: 40 - Batch: 114624 - Loss: 0.136715 - Time:263.87537026405334\n",
      "Epoch: 40 - Batch: 118720 - Loss: 0.513024 - Time:273.2933533191681\n",
      "Epoch: 40 - Batch: 122816 - Loss: 0.453287 - Time:282.70884466171265\n",
      "Epoch: 40 - Batch: 126912 - Loss: 0.446940 - Time:292.1236193180084\n",
      "Epoch: 40 - Batch: 131008 - Loss: 0.200770 - Time:301.53871273994446\n",
      "Epoch: 40 - Batch: 135104 - Loss: 0.603659 - Time:310.9701361656189\n",
      "Epoch: 40 - Batch: 139200 - Loss: 0.406409 - Time:320.383588552475\n",
      "Epoch: 40 - Batch: 143296 - Loss: 0.291831 - Time:329.79758191108704\n",
      "Epoch: 40 - Batch: 147392 - Loss: 0.159441 - Time:339.21385526657104\n",
      "Epoch: 40 - Batch: 151488 - Loss: 0.379504 - Time:348.6444203853607\n",
      "Epoch: 41 - Batch: 4032 - Loss: 0.158210 - Time:9.596467018127441\n",
      "Epoch: 41 - Batch: 8128 - Loss: 0.296765 - Time:19.014613389968872\n",
      "Epoch: 41 - Batch: 12224 - Loss: 0.097138 - Time:28.431819200515747\n",
      "Epoch: 41 - Batch: 16320 - Loss: 0.106155 - Time:37.876153230667114\n",
      "Epoch: 41 - Batch: 20416 - Loss: 0.484413 - Time:47.29105806350708\n",
      "Epoch: 41 - Batch: 24512 - Loss: 0.172984 - Time:56.70573925971985\n",
      "Epoch: 41 - Batch: 28608 - Loss: 0.305106 - Time:66.138112783432\n",
      "Epoch: 41 - Batch: 32704 - Loss: 0.356203 - Time:75.55264234542847\n",
      "Epoch: 41 - Batch: 36800 - Loss: 0.818841 - Time:84.9660153388977\n",
      "Epoch: 41 - Batch: 40896 - Loss: 0.319365 - Time:94.37801861763\n",
      "Epoch: 41 - Batch: 44992 - Loss: 0.219110 - Time:103.79405808448792\n",
      "Epoch: 41 - Batch: 49088 - Loss: 0.598484 - Time:113.20731019973755\n",
      "Epoch: 41 - Batch: 53184 - Loss: 0.260606 - Time:122.62112879753113\n",
      "Epoch: 41 - Batch: 57280 - Loss: 0.521455 - Time:132.03381204605103\n",
      "Epoch: 41 - Batch: 61376 - Loss: 0.445857 - Time:141.46693563461304\n",
      "Epoch: 41 - Batch: 65472 - Loss: 0.285087 - Time:150.88023400306702\n",
      "Epoch: 41 - Batch: 69568 - Loss: 0.299496 - Time:160.29358196258545\n",
      "Epoch: 41 - Batch: 73664 - Loss: 0.482061 - Time:169.70841073989868\n",
      "Epoch: 41 - Batch: 77760 - Loss: 0.206375 - Time:179.1384789943695\n",
      "Epoch: 41 - Batch: 81856 - Loss: 0.248839 - Time:188.55209803581238\n",
      "Epoch: 41 - Batch: 85952 - Loss: 0.180991 - Time:197.96590065956116\n",
      "Epoch: 41 - Batch: 90048 - Loss: 0.506354 - Time:207.39500427246094\n",
      "Epoch: 41 - Batch: 94144 - Loss: 0.305190 - Time:216.8077838420868\n",
      "Epoch: 41 - Batch: 98240 - Loss: 0.607504 - Time:226.22164249420166\n",
      "Epoch: 41 - Batch: 102336 - Loss: 0.274456 - Time:235.636714220047\n",
      "Epoch: 41 - Batch: 106432 - Loss: 0.630530 - Time:245.05057668685913\n",
      "Epoch: 41 - Batch: 110528 - Loss: 0.255317 - Time:254.4660999774933\n",
      "Epoch: 41 - Batch: 114624 - Loss: 0.407480 - Time:263.8788752555847\n",
      "Epoch: 41 - Batch: 118720 - Loss: 0.553000 - Time:273.2925293445587\n",
      "Epoch: 41 - Batch: 122816 - Loss: 0.473546 - Time:282.72580313682556\n",
      "Epoch: 41 - Batch: 126912 - Loss: 0.297158 - Time:292.1401240825653\n",
      "Epoch: 41 - Batch: 131008 - Loss: 0.249229 - Time:301.5553798675537\n",
      "Epoch: 41 - Batch: 135104 - Loss: 0.674738 - Time:310.9699773788452\n",
      "Epoch: 41 - Batch: 139200 - Loss: 0.339410 - Time:320.40296721458435\n",
      "Epoch: 41 - Batch: 143296 - Loss: 0.215479 - Time:329.8168032169342\n",
      "Epoch: 41 - Batch: 147392 - Loss: 0.453432 - Time:339.2307937145233\n",
      "Epoch: 41 - Batch: 151488 - Loss: 0.717610 - Time:348.66131949424744\n",
      "Epoch: 42 - Batch: 4032 - Loss: 0.099596 - Time:9.582261800765991\n",
      "Epoch: 42 - Batch: 8128 - Loss: 0.064071 - Time:18.99907922744751\n",
      "Epoch: 42 - Batch: 12224 - Loss: 0.259644 - Time:28.44183611869812\n",
      "Epoch: 42 - Batch: 16320 - Loss: 0.171890 - Time:37.85622501373291\n",
      "Epoch: 42 - Batch: 20416 - Loss: 0.326420 - Time:47.270002603530884\n",
      "Epoch: 42 - Batch: 24512 - Loss: 0.138861 - Time:56.68366718292236\n",
      "Epoch: 42 - Batch: 28608 - Loss: 0.093788 - Time:66.09874510765076\n",
      "Epoch: 42 - Batch: 32704 - Loss: 0.178642 - Time:75.51413440704346\n",
      "Epoch: 42 - Batch: 36800 - Loss: 0.069864 - Time:84.92741632461548\n",
      "Epoch: 42 - Batch: 40896 - Loss: 0.116341 - Time:94.34324622154236\n",
      "Epoch: 42 - Batch: 44992 - Loss: 0.352738 - Time:103.77276229858398\n",
      "Epoch: 42 - Batch: 49088 - Loss: 0.079337 - Time:113.18703889846802\n",
      "Epoch: 42 - Batch: 53184 - Loss: 0.265892 - Time:122.60064482688904\n",
      "Epoch: 42 - Batch: 57280 - Loss: 0.431107 - Time:132.01093435287476\n",
      "Epoch: 42 - Batch: 61376 - Loss: 0.194366 - Time:141.43917274475098\n",
      "Epoch: 42 - Batch: 65472 - Loss: 0.080566 - Time:150.85225915908813\n",
      "Epoch: 42 - Batch: 69568 - Loss: 0.200860 - Time:160.2651126384735\n",
      "Epoch: 42 - Batch: 73664 - Loss: 0.315052 - Time:169.6978189945221\n",
      "Epoch: 42 - Batch: 77760 - Loss: 0.201209 - Time:179.1114227771759\n",
      "Epoch: 42 - Batch: 81856 - Loss: 0.436624 - Time:188.52555131912231\n",
      "Epoch: 42 - Batch: 85952 - Loss: 0.288443 - Time:197.93643832206726\n",
      "Epoch: 42 - Batch: 90048 - Loss: 0.226573 - Time:207.3482894897461\n",
      "Epoch: 42 - Batch: 94144 - Loss: 0.640467 - Time:216.76196026802063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 - Batch: 98240 - Loss: 0.335286 - Time:226.17415928840637\n",
      "Epoch: 42 - Batch: 102336 - Loss: 0.395035 - Time:235.5899577140808\n",
      "Epoch: 42 - Batch: 106432 - Loss: 0.324294 - Time:245.0223684310913\n",
      "Epoch: 42 - Batch: 110528 - Loss: 0.316070 - Time:254.43609285354614\n",
      "Epoch: 42 - Batch: 114624 - Loss: 0.143723 - Time:263.8492579460144\n",
      "Epoch: 42 - Batch: 118720 - Loss: 0.549884 - Time:273.2622261047363\n",
      "Epoch: 42 - Batch: 122816 - Loss: 0.223054 - Time:282.6743948459625\n",
      "Epoch: 42 - Batch: 126912 - Loss: 0.440284 - Time:292.10527086257935\n",
      "Epoch: 42 - Batch: 131008 - Loss: 0.521491 - Time:301.5192937850952\n",
      "Epoch: 42 - Batch: 135104 - Loss: 0.401594 - Time:310.93150544166565\n",
      "Epoch: 42 - Batch: 139200 - Loss: 0.163647 - Time:320.3633396625519\n",
      "Epoch: 42 - Batch: 143296 - Loss: 0.570047 - Time:329.77575278282166\n",
      "Epoch: 42 - Batch: 147392 - Loss: 0.653323 - Time:339.18806767463684\n",
      "Epoch: 42 - Batch: 151488 - Loss: 0.475293 - Time:348.60111236572266\n",
      "Epoch: 43 - Batch: 4032 - Loss: 0.515800 - Time:9.574177742004395\n",
      "Epoch: 43 - Batch: 8128 - Loss: 0.071133 - Time:19.018741369247437\n",
      "Epoch: 43 - Batch: 12224 - Loss: 0.387906 - Time:28.433404684066772\n",
      "Epoch: 43 - Batch: 16320 - Loss: 0.048492 - Time:37.84722185134888\n",
      "Epoch: 43 - Batch: 20416 - Loss: 0.169569 - Time:47.26049447059631\n",
      "Epoch: 43 - Batch: 24512 - Loss: 0.179570 - Time:56.67352294921875\n",
      "Epoch: 43 - Batch: 28608 - Loss: 0.086041 - Time:66.08613395690918\n",
      "Epoch: 43 - Batch: 32704 - Loss: 0.292058 - Time:75.50176858901978\n",
      "Epoch: 43 - Batch: 36800 - Loss: 0.595504 - Time:84.91730237007141\n",
      "Epoch: 43 - Batch: 40896 - Loss: 0.373758 - Time:94.35184144973755\n",
      "Epoch: 43 - Batch: 44992 - Loss: 0.065442 - Time:103.76629185676575\n",
      "Epoch: 43 - Batch: 49088 - Loss: 0.629231 - Time:113.18018674850464\n",
      "Epoch: 43 - Batch: 53184 - Loss: 0.491255 - Time:122.59517478942871\n",
      "Epoch: 43 - Batch: 57280 - Loss: 0.177830 - Time:132.0248646736145\n",
      "Epoch: 43 - Batch: 61376 - Loss: 0.256663 - Time:141.43816089630127\n",
      "Epoch: 43 - Batch: 65472 - Loss: 0.108532 - Time:150.85243940353394\n",
      "Epoch: 43 - Batch: 69568 - Loss: 0.197509 - Time:160.282817363739\n",
      "Epoch: 43 - Batch: 73664 - Loss: 0.248554 - Time:169.6975131034851\n",
      "Epoch: 43 - Batch: 77760 - Loss: 0.415884 - Time:179.11055946350098\n",
      "Epoch: 43 - Batch: 81856 - Loss: 0.201079 - Time:188.52287030220032\n",
      "Epoch: 43 - Batch: 85952 - Loss: 0.201017 - Time:197.9368131160736\n",
      "Epoch: 43 - Batch: 90048 - Loss: 0.574720 - Time:207.34899616241455\n",
      "Epoch: 43 - Batch: 94144 - Loss: 0.130201 - Time:216.7611584663391\n",
      "Epoch: 43 - Batch: 98240 - Loss: 0.420747 - Time:226.17359590530396\n",
      "Epoch: 43 - Batch: 102336 - Loss: 0.627619 - Time:235.6048448085785\n",
      "Epoch: 43 - Batch: 106432 - Loss: 0.409486 - Time:245.01932501792908\n",
      "Epoch: 43 - Batch: 110528 - Loss: 0.398672 - Time:254.4351212978363\n",
      "Epoch: 43 - Batch: 114624 - Loss: 0.230146 - Time:263.84997153282166\n",
      "Epoch: 43 - Batch: 118720 - Loss: 0.520118 - Time:273.28233790397644\n",
      "Epoch: 43 - Batch: 122816 - Loss: 0.129540 - Time:282.69545817375183\n",
      "Epoch: 43 - Batch: 126912 - Loss: 0.816211 - Time:292.10746216773987\n",
      "Epoch: 43 - Batch: 131008 - Loss: 0.456362 - Time:301.5375533103943\n",
      "Epoch: 43 - Batch: 135104 - Loss: 0.108703 - Time:310.9505798816681\n",
      "Epoch: 43 - Batch: 139200 - Loss: 0.265334 - Time:320.3619165420532\n",
      "Epoch: 43 - Batch: 143296 - Loss: 0.334397 - Time:329.7749788761139\n",
      "Epoch: 43 - Batch: 147392 - Loss: 0.423687 - Time:339.1888165473938\n",
      "Epoch: 43 - Batch: 151488 - Loss: 0.706108 - Time:348.60368251800537\n",
      "Epoch: 44 - Batch: 4032 - Loss: 0.336850 - Time:9.585747718811035\n",
      "Epoch: 44 - Batch: 8128 - Loss: 0.151759 - Time:19.005064010620117\n",
      "Epoch: 44 - Batch: 12224 - Loss: 0.173215 - Time:28.450136184692383\n",
      "Epoch: 44 - Batch: 16320 - Loss: 0.331945 - Time:37.86773753166199\n",
      "Epoch: 44 - Batch: 20416 - Loss: 0.161342 - Time:47.281468629837036\n",
      "Epoch: 44 - Batch: 24512 - Loss: 0.144018 - Time:56.69561982154846\n",
      "Epoch: 44 - Batch: 28608 - Loss: 0.250507 - Time:66.12773752212524\n",
      "Epoch: 44 - Batch: 32704 - Loss: 0.354437 - Time:75.5428638458252\n",
      "Epoch: 44 - Batch: 36800 - Loss: 0.117646 - Time:84.95703744888306\n",
      "Epoch: 44 - Batch: 40896 - Loss: 0.173976 - Time:94.38750433921814\n",
      "Epoch: 44 - Batch: 44992 - Loss: 0.153575 - Time:103.79971075057983\n",
      "Epoch: 44 - Batch: 49088 - Loss: 0.181487 - Time:113.2112398147583\n",
      "Epoch: 44 - Batch: 53184 - Loss: 0.188352 - Time:122.62790584564209\n",
      "Epoch: 44 - Batch: 57280 - Loss: 0.145135 - Time:132.04398226737976\n",
      "Epoch: 44 - Batch: 61376 - Loss: 0.241587 - Time:141.45979022979736\n",
      "Epoch: 44 - Batch: 65472 - Loss: 0.279841 - Time:150.8738191127777\n",
      "Epoch: 44 - Batch: 69568 - Loss: 0.152244 - Time:160.2904281616211\n",
      "Epoch: 44 - Batch: 73664 - Loss: 0.146317 - Time:169.72152614593506\n",
      "Epoch: 44 - Batch: 77760 - Loss: 0.241193 - Time:179.13561940193176\n",
      "Epoch: 44 - Batch: 81856 - Loss: 0.132437 - Time:188.54790902137756\n",
      "Epoch: 44 - Batch: 85952 - Loss: 0.509044 - Time:197.96010208129883\n",
      "Epoch: 44 - Batch: 90048 - Loss: 0.377722 - Time:207.38828206062317\n",
      "Epoch: 44 - Batch: 94144 - Loss: 0.111377 - Time:216.80056238174438\n",
      "Epoch: 44 - Batch: 98240 - Loss: 0.485321 - Time:226.21411991119385\n",
      "Epoch: 44 - Batch: 102336 - Loss: 0.418907 - Time:235.64433884620667\n",
      "Epoch: 44 - Batch: 106432 - Loss: 0.510385 - Time:245.05702447891235\n",
      "Epoch: 44 - Batch: 110528 - Loss: 0.120449 - Time:254.4683015346527\n",
      "Epoch: 44 - Batch: 114624 - Loss: 0.476130 - Time:263.8807201385498\n",
      "Epoch: 44 - Batch: 118720 - Loss: 0.119553 - Time:273.29232120513916\n",
      "Epoch: 44 - Batch: 122816 - Loss: 0.197531 - Time:282.7043807506561\n",
      "Epoch: 44 - Batch: 126912 - Loss: 0.078241 - Time:292.117374420166\n",
      "Epoch: 44 - Batch: 131008 - Loss: 0.355761 - Time:301.5319936275482\n",
      "Epoch: 44 - Batch: 135104 - Loss: 0.365042 - Time:310.96331334114075\n",
      "Epoch: 44 - Batch: 139200 - Loss: 0.302701 - Time:320.3762540817261\n",
      "Epoch: 44 - Batch: 143296 - Loss: 0.257855 - Time:329.7877748012543\n",
      "Epoch: 44 - Batch: 147392 - Loss: 0.630454 - Time:339.2010474205017\n",
      "Epoch: 44 - Batch: 151488 - Loss: 0.710086 - Time:348.63110089302063\n",
      "Epoch: 45 - Batch: 4032 - Loss: 0.359483 - Time:9.574813842773438\n",
      "Epoch: 45 - Batch: 8128 - Loss: 0.236705 - Time:18.989748001098633\n",
      "Epoch: 45 - Batch: 12224 - Loss: 0.432940 - Time:28.405311346054077\n",
      "Epoch: 45 - Batch: 16320 - Loss: 0.053696 - Time:37.846638441085815\n",
      "Epoch: 45 - Batch: 20416 - Loss: 0.247306 - Time:47.262553691864014\n",
      "Epoch: 45 - Batch: 24512 - Loss: 0.120751 - Time:56.677263021469116\n",
      "Epoch: 45 - Batch: 28608 - Loss: 0.484049 - Time:66.10757398605347\n",
      "Epoch: 45 - Batch: 32704 - Loss: 0.190731 - Time:75.5225760936737\n",
      "Epoch: 45 - Batch: 36800 - Loss: 0.171747 - Time:84.9354338645935\n",
      "Epoch: 45 - Batch: 40896 - Loss: 0.069783 - Time:94.35047745704651\n",
      "Epoch: 45 - Batch: 44992 - Loss: 0.060141 - Time:103.76466655731201\n",
      "Epoch: 45 - Batch: 49088 - Loss: 0.155687 - Time:113.1778872013092\n",
      "Epoch: 45 - Batch: 53184 - Loss: 0.275520 - Time:122.59240388870239\n",
      "Epoch: 45 - Batch: 57280 - Loss: 0.071756 - Time:132.00778985023499\n",
      "Epoch: 45 - Batch: 61376 - Loss: 0.343254 - Time:141.43776726722717\n",
      "Epoch: 45 - Batch: 65472 - Loss: 0.277807 - Time:150.8513126373291\n",
      "Epoch: 45 - Batch: 69568 - Loss: 0.838025 - Time:160.26496934890747\n",
      "Epoch: 45 - Batch: 73664 - Loss: 0.068948 - Time:169.67715525627136\n",
      "Epoch: 45 - Batch: 77760 - Loss: 0.124605 - Time:179.10616064071655\n",
      "Epoch: 45 - Batch: 81856 - Loss: 0.407006 - Time:188.51735496520996\n",
      "Epoch: 45 - Batch: 85952 - Loss: 0.059691 - Time:197.93067693710327\n",
      "Epoch: 45 - Batch: 90048 - Loss: 0.120446 - Time:207.3580756187439\n",
      "Epoch: 45 - Batch: 94144 - Loss: 0.377420 - Time:216.76896834373474\n",
      "Epoch: 45 - Batch: 98240 - Loss: 0.181410 - Time:226.1814570426941\n",
      "Epoch: 45 - Batch: 102336 - Loss: 0.336525 - Time:235.59396290779114\n",
      "Epoch: 45 - Batch: 106432 - Loss: 0.799870 - Time:245.0072362422943\n",
      "Epoch: 45 - Batch: 110528 - Loss: 0.378103 - Time:254.4230763912201\n",
      "Epoch: 45 - Batch: 114624 - Loss: 0.331267 - Time:263.8362846374512\n",
      "Epoch: 45 - Batch: 118720 - Loss: 0.169089 - Time:273.2501571178436\n",
      "Epoch: 45 - Batch: 122816 - Loss: 0.169678 - Time:282.68201327323914\n",
      "Epoch: 45 - Batch: 126912 - Loss: 0.283910 - Time:292.09826612472534\n",
      "Epoch: 45 - Batch: 131008 - Loss: 0.376610 - Time:301.5126266479492\n",
      "Epoch: 45 - Batch: 135104 - Loss: 0.136195 - Time:310.92665433883667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 - Batch: 139200 - Loss: 0.177913 - Time:320.3547682762146\n",
      "Epoch: 45 - Batch: 143296 - Loss: 0.094697 - Time:329.7672815322876\n",
      "Epoch: 45 - Batch: 147392 - Loss: 0.561540 - Time:339.1820435523987\n",
      "Epoch: 45 - Batch: 151488 - Loss: 0.265378 - Time:348.6135003566742\n",
      "Epoch: 46 - Batch: 4032 - Loss: 0.315598 - Time:9.578479766845703\n",
      "Epoch: 46 - Batch: 8128 - Loss: 0.131865 - Time:18.99294877052307\n",
      "Epoch: 46 - Batch: 12224 - Loss: 0.116689 - Time:28.437442779541016\n",
      "Epoch: 46 - Batch: 16320 - Loss: 0.218095 - Time:37.85153079032898\n",
      "Epoch: 46 - Batch: 20416 - Loss: 0.047310 - Time:47.26694846153259\n",
      "Epoch: 46 - Batch: 24512 - Loss: 0.439109 - Time:56.68167543411255\n",
      "Epoch: 46 - Batch: 28608 - Loss: 0.222640 - Time:66.0932252407074\n",
      "Epoch: 46 - Batch: 32704 - Loss: 0.268714 - Time:75.50606894493103\n",
      "Epoch: 46 - Batch: 36800 - Loss: 0.062668 - Time:84.91934514045715\n",
      "Epoch: 46 - Batch: 40896 - Loss: 0.066300 - Time:94.33328604698181\n",
      "Epoch: 46 - Batch: 44992 - Loss: 0.073734 - Time:103.76333713531494\n",
      "Epoch: 46 - Batch: 49088 - Loss: 0.046801 - Time:113.17635321617126\n",
      "Epoch: 46 - Batch: 53184 - Loss: 0.309041 - Time:122.5899920463562\n",
      "Epoch: 46 - Batch: 57280 - Loss: 0.745575 - Time:132.00406646728516\n",
      "Epoch: 46 - Batch: 61376 - Loss: 0.090703 - Time:141.43289184570312\n",
      "Epoch: 46 - Batch: 65472 - Loss: 0.118476 - Time:150.84449529647827\n",
      "Epoch: 46 - Batch: 69568 - Loss: 0.227668 - Time:160.25310039520264\n",
      "Epoch: 46 - Batch: 73664 - Loss: 0.512490 - Time:169.68112301826477\n",
      "Epoch: 46 - Batch: 77760 - Loss: 0.126649 - Time:179.0919110774994\n",
      "Epoch: 46 - Batch: 81856 - Loss: 0.122793 - Time:188.50182700157166\n",
      "Epoch: 46 - Batch: 85952 - Loss: 0.364617 - Time:197.91366982460022\n",
      "Epoch: 46 - Batch: 90048 - Loss: 0.139798 - Time:207.3270969390869\n",
      "Epoch: 46 - Batch: 94144 - Loss: 0.274043 - Time:216.7405662536621\n",
      "Epoch: 46 - Batch: 98240 - Loss: 0.141161 - Time:226.15371108055115\n",
      "Epoch: 46 - Batch: 102336 - Loss: 0.264228 - Time:235.5646367073059\n",
      "Epoch: 46 - Batch: 106432 - Loss: 0.180358 - Time:244.9935200214386\n",
      "Epoch: 46 - Batch: 110528 - Loss: 0.321114 - Time:254.40301942825317\n",
      "Epoch: 46 - Batch: 114624 - Loss: 0.272394 - Time:263.8137364387512\n",
      "Epoch: 46 - Batch: 118720 - Loss: 0.203186 - Time:273.2254436016083\n",
      "Epoch: 46 - Batch: 122816 - Loss: 0.281349 - Time:282.63751745224\n",
      "Epoch: 46 - Batch: 126912 - Loss: 0.569685 - Time:292.0654718875885\n",
      "Epoch: 46 - Batch: 131008 - Loss: 0.371151 - Time:301.4770007133484\n",
      "Epoch: 46 - Batch: 135104 - Loss: 0.228645 - Time:310.8871400356293\n",
      "Epoch: 46 - Batch: 139200 - Loss: 0.717303 - Time:320.3152377605438\n",
      "Epoch: 46 - Batch: 143296 - Loss: 0.323251 - Time:329.72760462760925\n",
      "Epoch: 46 - Batch: 147392 - Loss: 0.307610 - Time:339.1407153606415\n",
      "Epoch: 46 - Batch: 151488 - Loss: 0.206606 - Time:348.5551161766052\n",
      "Epoch: 47 - Batch: 4032 - Loss: 0.220942 - Time:9.572919845581055\n",
      "Epoch: 47 - Batch: 8128 - Loss: 0.114276 - Time:19.018288612365723\n",
      "Epoch: 47 - Batch: 12224 - Loss: 0.063255 - Time:28.431723594665527\n",
      "Epoch: 47 - Batch: 16320 - Loss: 0.346534 - Time:37.84601378440857\n",
      "Epoch: 47 - Batch: 20416 - Loss: 0.375296 - Time:47.25820732116699\n",
      "Epoch: 47 - Batch: 24512 - Loss: 0.074602 - Time:56.670838832855225\n",
      "Epoch: 47 - Batch: 28608 - Loss: 0.239075 - Time:66.082674741745\n",
      "Epoch: 47 - Batch: 32704 - Loss: 0.509379 - Time:75.4973828792572\n",
      "Epoch: 47 - Batch: 36800 - Loss: 0.283841 - Time:84.91111779212952\n",
      "Epoch: 47 - Batch: 40896 - Loss: 0.239083 - Time:94.34472250938416\n",
      "Epoch: 47 - Batch: 44992 - Loss: 0.549402 - Time:103.76082062721252\n",
      "Epoch: 47 - Batch: 49088 - Loss: 0.920298 - Time:113.17623567581177\n",
      "Epoch: 47 - Batch: 53184 - Loss: 0.151846 - Time:122.59077525138855\n",
      "Epoch: 47 - Batch: 57280 - Loss: 0.630181 - Time:132.02282404899597\n",
      "Epoch: 47 - Batch: 61376 - Loss: 0.236160 - Time:141.43683171272278\n",
      "Epoch: 47 - Batch: 65472 - Loss: 0.431581 - Time:150.85205101966858\n",
      "Epoch: 47 - Batch: 69568 - Loss: 0.304965 - Time:160.28202986717224\n",
      "Epoch: 47 - Batch: 73664 - Loss: 0.551874 - Time:169.6945858001709\n",
      "Epoch: 47 - Batch: 77760 - Loss: 0.232759 - Time:179.1091079711914\n",
      "Epoch: 47 - Batch: 81856 - Loss: 0.168770 - Time:188.52352237701416\n",
      "Epoch: 47 - Batch: 85952 - Loss: 0.223239 - Time:197.93875312805176\n",
      "Epoch: 47 - Batch: 90048 - Loss: 0.127871 - Time:207.35225367546082\n",
      "Epoch: 47 - Batch: 94144 - Loss: 0.243732 - Time:216.76363062858582\n",
      "Epoch: 47 - Batch: 98240 - Loss: 0.070573 - Time:226.1782145500183\n",
      "Epoch: 47 - Batch: 102336 - Loss: 0.234319 - Time:235.609628200531\n",
      "Epoch: 47 - Batch: 106432 - Loss: 0.368197 - Time:245.02303981781006\n",
      "Epoch: 47 - Batch: 110528 - Loss: 0.239522 - Time:254.4384639263153\n",
      "Epoch: 47 - Batch: 114624 - Loss: 0.514615 - Time:263.8521685600281\n",
      "Epoch: 47 - Batch: 118720 - Loss: 0.206829 - Time:273.28459787368774\n",
      "Epoch: 47 - Batch: 122816 - Loss: 0.128773 - Time:282.6986265182495\n",
      "Epoch: 47 - Batch: 126912 - Loss: 0.286325 - Time:292.1142165660858\n",
      "Epoch: 47 - Batch: 131008 - Loss: 0.504888 - Time:301.5469570159912\n",
      "Epoch: 47 - Batch: 135104 - Loss: 0.362913 - Time:310.9593982696533\n",
      "Epoch: 47 - Batch: 139200 - Loss: 0.679087 - Time:320.37426590919495\n",
      "Epoch: 47 - Batch: 143296 - Loss: 0.057390 - Time:329.7881774902344\n",
      "Epoch: 47 - Batch: 147392 - Loss: 0.483841 - Time:339.20222663879395\n",
      "Epoch: 47 - Batch: 151488 - Loss: 0.297153 - Time:348.61534881591797\n",
      "Epoch: 48 - Batch: 4032 - Loss: 0.157650 - Time:9.574967861175537\n",
      "Epoch: 48 - Batch: 8128 - Loss: 0.280603 - Time:18.992198944091797\n",
      "Epoch: 48 - Batch: 12224 - Loss: 0.048955 - Time:28.435527563095093\n",
      "Epoch: 48 - Batch: 16320 - Loss: 0.328058 - Time:37.849562883377075\n",
      "Epoch: 48 - Batch: 20416 - Loss: 0.554670 - Time:47.2656352519989\n",
      "Epoch: 48 - Batch: 24512 - Loss: 0.070011 - Time:56.681103229522705\n",
      "Epoch: 48 - Batch: 28608 - Loss: 0.414240 - Time:66.1151692867279\n",
      "Epoch: 48 - Batch: 32704 - Loss: 0.115098 - Time:75.52971529960632\n",
      "Epoch: 48 - Batch: 36800 - Loss: 0.084633 - Time:84.94351291656494\n",
      "Epoch: 48 - Batch: 40896 - Loss: 0.115875 - Time:94.37401700019836\n",
      "Epoch: 48 - Batch: 44992 - Loss: 0.161777 - Time:103.78832411766052\n",
      "Epoch: 48 - Batch: 49088 - Loss: 0.241823 - Time:113.20275616645813\n",
      "Epoch: 48 - Batch: 53184 - Loss: 0.198132 - Time:122.61612939834595\n",
      "Epoch: 48 - Batch: 57280 - Loss: 0.090775 - Time:132.0266091823578\n",
      "Epoch: 48 - Batch: 61376 - Loss: 0.322383 - Time:141.43842720985413\n",
      "Epoch: 48 - Batch: 65472 - Loss: 0.179923 - Time:150.85227012634277\n",
      "Epoch: 48 - Batch: 69568 - Loss: 0.372575 - Time:160.26408553123474\n",
      "Epoch: 48 - Batch: 73664 - Loss: 0.235094 - Time:169.69523096084595\n",
      "Epoch: 48 - Batch: 77760 - Loss: 0.199036 - Time:179.10804414749146\n",
      "Epoch: 48 - Batch: 81856 - Loss: 0.147756 - Time:188.5217683315277\n",
      "Epoch: 48 - Batch: 85952 - Loss: 0.091618 - Time:197.93480706214905\n",
      "Epoch: 48 - Batch: 90048 - Loss: 0.396475 - Time:207.36415100097656\n",
      "Epoch: 48 - Batch: 94144 - Loss: 0.220250 - Time:216.77688026428223\n",
      "Epoch: 48 - Batch: 98240 - Loss: 0.408316 - Time:226.19167852401733\n",
      "Epoch: 48 - Batch: 102336 - Loss: 0.282265 - Time:235.6215660572052\n",
      "Epoch: 48 - Batch: 106432 - Loss: 0.186247 - Time:245.03571367263794\n",
      "Epoch: 48 - Batch: 110528 - Loss: 0.250082 - Time:254.44711828231812\n",
      "Epoch: 48 - Batch: 114624 - Loss: 0.091329 - Time:263.8622591495514\n",
      "Epoch: 48 - Batch: 118720 - Loss: 0.512902 - Time:273.27698802948\n",
      "Epoch: 48 - Batch: 122816 - Loss: 0.267933 - Time:282.6899936199188\n",
      "Epoch: 48 - Batch: 126912 - Loss: 0.152933 - Time:292.1058495044708\n",
      "Epoch: 48 - Batch: 131008 - Loss: 0.482225 - Time:301.52094554901123\n",
      "Epoch: 48 - Batch: 135104 - Loss: 0.343370 - Time:310.950665473938\n",
      "Epoch: 48 - Batch: 139200 - Loss: 0.112022 - Time:320.366014957428\n",
      "Epoch: 48 - Batch: 143296 - Loss: 0.187490 - Time:329.7777695655823\n",
      "Epoch: 48 - Batch: 147392 - Loss: 0.077611 - Time:339.1906991004944\n",
      "Epoch: 48 - Batch: 151488 - Loss: 0.356876 - Time:348.61935544013977\n",
      "Epoch: 49 - Batch: 4032 - Loss: 0.055549 - Time:9.577284097671509\n",
      "Epoch: 49 - Batch: 8128 - Loss: 0.279747 - Time:18.99364471435547\n",
      "Epoch: 49 - Batch: 12224 - Loss: 0.106459 - Time:28.409847021102905\n",
      "Epoch: 49 - Batch: 16320 - Loss: 0.360078 - Time:37.85160303115845\n",
      "Epoch: 49 - Batch: 20416 - Loss: 0.329042 - Time:47.2652313709259\n",
      "Epoch: 49 - Batch: 24512 - Loss: 0.083074 - Time:56.67931818962097\n",
      "Epoch: 49 - Batch: 28608 - Loss: 0.392899 - Time:66.11014175415039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 - Batch: 32704 - Loss: 0.166060 - Time:75.52520871162415\n",
      "Epoch: 49 - Batch: 36800 - Loss: 0.318452 - Time:84.94128108024597\n",
      "Epoch: 49 - Batch: 40896 - Loss: 0.068389 - Time:94.35427761077881\n",
      "Epoch: 49 - Batch: 44992 - Loss: 0.089074 - Time:103.76988363265991\n",
      "Epoch: 49 - Batch: 49088 - Loss: 0.308255 - Time:113.18480801582336\n",
      "Epoch: 49 - Batch: 53184 - Loss: 0.512153 - Time:122.59705567359924\n",
      "Epoch: 49 - Batch: 57280 - Loss: 0.293231 - Time:132.01085233688354\n",
      "Epoch: 49 - Batch: 61376 - Loss: 0.437559 - Time:141.43921732902527\n",
      "Epoch: 49 - Batch: 65472 - Loss: 0.171124 - Time:150.85573434829712\n",
      "Epoch: 49 - Batch: 69568 - Loss: 0.096773 - Time:160.27287602424622\n",
      "Epoch: 49 - Batch: 73664 - Loss: 0.099031 - Time:169.68811964988708\n",
      "Epoch: 49 - Batch: 77760 - Loss: 0.471875 - Time:179.1209077835083\n",
      "Epoch: 49 - Batch: 81856 - Loss: 0.148106 - Time:188.53679752349854\n",
      "Epoch: 49 - Batch: 85952 - Loss: 0.153985 - Time:197.95207142829895\n",
      "Epoch: 49 - Batch: 90048 - Loss: 0.212091 - Time:207.3835141658783\n",
      "Epoch: 49 - Batch: 94144 - Loss: 0.151880 - Time:216.79868459701538\n",
      "Epoch: 49 - Batch: 98240 - Loss: 0.468048 - Time:226.2122404575348\n",
      "Epoch: 49 - Batch: 102336 - Loss: 0.237882 - Time:235.6251084804535\n",
      "Epoch: 49 - Batch: 106432 - Loss: 0.332443 - Time:245.03967475891113\n",
      "Epoch: 49 - Batch: 110528 - Loss: 0.423086 - Time:254.45504307746887\n",
      "Epoch: 49 - Batch: 114624 - Loss: 0.055337 - Time:263.8719940185547\n",
      "Epoch: 49 - Batch: 118720 - Loss: 0.087717 - Time:273.28714537620544\n",
      "Epoch: 49 - Batch: 122816 - Loss: 0.554940 - Time:282.71814918518066\n",
      "Epoch: 49 - Batch: 126912 - Loss: 0.310431 - Time:292.13325905799866\n",
      "Epoch: 49 - Batch: 131008 - Loss: 0.273394 - Time:301.54595398902893\n",
      "Epoch: 49 - Batch: 135104 - Loss: 0.141966 - Time:310.958642244339\n",
      "Epoch: 49 - Batch: 139200 - Loss: 0.211821 - Time:320.3904552459717\n",
      "Epoch: 49 - Batch: 143296 - Loss: 0.152430 - Time:329.80595421791077\n",
      "Epoch: 49 - Batch: 147392 - Loss: 0.180064 - Time:339.22164130210876\n",
      "Epoch: 49 - Batch: 151488 - Loss: 0.154002 - Time:348.6570055484772\n",
      "Epoch: 50 - Batch: 4032 - Loss: 0.209195 - Time:9.578403234481812\n",
      "Epoch: 50 - Batch: 8128 - Loss: 0.167028 - Time:18.994126081466675\n",
      "Epoch: 50 - Batch: 12224 - Loss: 0.322033 - Time:28.437424659729004\n",
      "Epoch: 50 - Batch: 16320 - Loss: 0.227015 - Time:37.85422492027283\n",
      "Epoch: 50 - Batch: 20416 - Loss: 0.213211 - Time:47.26846671104431\n",
      "Epoch: 50 - Batch: 24512 - Loss: 0.092573 - Time:56.683257818222046\n",
      "Epoch: 50 - Batch: 28608 - Loss: 0.231264 - Time:66.10040950775146\n",
      "Epoch: 50 - Batch: 32704 - Loss: 0.653834 - Time:75.51547241210938\n",
      "Epoch: 50 - Batch: 36800 - Loss: 0.108816 - Time:84.92908930778503\n",
      "Epoch: 50 - Batch: 40896 - Loss: 0.134910 - Time:94.34205651283264\n",
      "Epoch: 50 - Batch: 44992 - Loss: 0.076178 - Time:103.77269315719604\n",
      "Epoch: 50 - Batch: 49088 - Loss: 0.209554 - Time:113.1851110458374\n",
      "Epoch: 50 - Batch: 53184 - Loss: 0.145086 - Time:122.59676361083984\n",
      "Epoch: 50 - Batch: 57280 - Loss: 0.169444 - Time:132.00825953483582\n",
      "Epoch: 50 - Batch: 61376 - Loss: 0.533739 - Time:141.43801832199097\n",
      "Epoch: 50 - Batch: 65472 - Loss: 0.215229 - Time:150.85178303718567\n",
      "Epoch: 50 - Batch: 69568 - Loss: 0.349327 - Time:160.26349639892578\n",
      "Epoch: 50 - Batch: 73664 - Loss: 0.223610 - Time:169.6928265094757\n",
      "Epoch: 50 - Batch: 77760 - Loss: 0.046005 - Time:179.10471057891846\n",
      "Epoch: 50 - Batch: 81856 - Loss: 0.178231 - Time:188.5170180797577\n",
      "Epoch: 50 - Batch: 85952 - Loss: 0.314918 - Time:197.92809414863586\n",
      "Epoch: 50 - Batch: 90048 - Loss: 0.172143 - Time:207.34010648727417\n",
      "Epoch: 50 - Batch: 94144 - Loss: 0.079457 - Time:216.75398683547974\n",
      "Epoch: 50 - Batch: 98240 - Loss: 0.295587 - Time:226.1668107509613\n",
      "Epoch: 50 - Batch: 102336 - Loss: 0.110210 - Time:235.57809114456177\n",
      "Epoch: 50 - Batch: 106432 - Loss: 0.113906 - Time:245.00957036018372\n",
      "Epoch: 50 - Batch: 110528 - Loss: 0.160461 - Time:254.42281293869019\n",
      "Epoch: 50 - Batch: 114624 - Loss: 0.493187 - Time:263.8371067047119\n",
      "Epoch: 50 - Batch: 118720 - Loss: 0.345943 - Time:273.251318693161\n",
      "Epoch: 50 - Batch: 122816 - Loss: 0.116152 - Time:282.66332745552063\n",
      "Epoch: 50 - Batch: 126912 - Loss: 0.159519 - Time:292.0917377471924\n",
      "Epoch: 50 - Batch: 131008 - Loss: 0.165179 - Time:301.50439620018005\n",
      "Epoch: 50 - Batch: 135104 - Loss: 0.132464 - Time:310.9183189868927\n",
      "Epoch: 50 - Batch: 139200 - Loss: 0.323250 - Time:320.3491837978363\n",
      "Epoch: 50 - Batch: 143296 - Loss: 0.242178 - Time:329.763667345047\n",
      "Epoch: 50 - Batch: 147392 - Loss: 0.468307 - Time:339.1759178638458\n",
      "Epoch: 50 - Batch: 151488 - Loss: 0.071192 - Time:348.58914709091187\n",
      "Epoch: 51 - Batch: 4032 - Loss: 0.041461 - Time:9.574143171310425\n",
      "Epoch: 51 - Batch: 8128 - Loss: 0.109221 - Time:19.016644954681396\n",
      "Epoch: 51 - Batch: 12224 - Loss: 0.037843 - Time:28.431734800338745\n",
      "Epoch: 51 - Batch: 16320 - Loss: 0.279723 - Time:37.8480064868927\n",
      "Epoch: 51 - Batch: 20416 - Loss: 0.125653 - Time:47.262988805770874\n",
      "Epoch: 51 - Batch: 24512 - Loss: 0.281506 - Time:56.67684030532837\n",
      "Epoch: 51 - Batch: 28608 - Loss: 0.394161 - Time:66.08997774124146\n",
      "Epoch: 51 - Batch: 32704 - Loss: 0.444539 - Time:75.50255608558655\n",
      "Epoch: 51 - Batch: 36800 - Loss: 0.098472 - Time:84.91608309745789\n",
      "Epoch: 51 - Batch: 40896 - Loss: 0.170946 - Time:94.34604334831238\n",
      "Epoch: 51 - Batch: 44992 - Loss: 0.195013 - Time:103.75991368293762\n",
      "Epoch: 51 - Batch: 49088 - Loss: 0.184713 - Time:113.17525029182434\n",
      "Epoch: 51 - Batch: 53184 - Loss: 0.181799 - Time:122.58866286277771\n",
      "Epoch: 51 - Batch: 57280 - Loss: 0.160088 - Time:132.0186583995819\n",
      "Epoch: 51 - Batch: 61376 - Loss: 0.173359 - Time:141.43073415756226\n",
      "Epoch: 51 - Batch: 65472 - Loss: 0.047275 - Time:150.84277153015137\n",
      "Epoch: 51 - Batch: 69568 - Loss: 0.941754 - Time:160.2741575241089\n",
      "Epoch: 51 - Batch: 73664 - Loss: 0.283564 - Time:169.68789553642273\n",
      "Epoch: 51 - Batch: 77760 - Loss: 0.150905 - Time:179.1000635623932\n",
      "Epoch: 51 - Batch: 81856 - Loss: 0.228533 - Time:188.51453590393066\n",
      "Epoch: 51 - Batch: 85952 - Loss: 0.085996 - Time:197.92567348480225\n",
      "Epoch: 51 - Batch: 90048 - Loss: 0.159906 - Time:207.33841156959534\n",
      "Epoch: 51 - Batch: 94144 - Loss: 0.153811 - Time:216.74951577186584\n",
      "Epoch: 51 - Batch: 98240 - Loss: 0.122960 - Time:226.16187119483948\n",
      "Epoch: 51 - Batch: 102336 - Loss: 0.095814 - Time:235.59249711036682\n",
      "Epoch: 51 - Batch: 106432 - Loss: 0.321124 - Time:245.0064754486084\n",
      "Epoch: 51 - Batch: 110528 - Loss: 0.181015 - Time:254.4186496734619\n",
      "Epoch: 51 - Batch: 114624 - Loss: 0.310856 - Time:263.82947182655334\n",
      "Epoch: 51 - Batch: 118720 - Loss: 0.184134 - Time:273.25861978530884\n",
      "Epoch: 51 - Batch: 122816 - Loss: 0.307171 - Time:282.67103123664856\n",
      "Epoch: 51 - Batch: 126912 - Loss: 0.224489 - Time:292.0821838378906\n",
      "Epoch: 51 - Batch: 131008 - Loss: 0.147213 - Time:301.5101852416992\n",
      "Epoch: 51 - Batch: 135104 - Loss: 0.268238 - Time:310.9240508079529\n",
      "Epoch: 51 - Batch: 139200 - Loss: 0.230712 - Time:320.33819937705994\n",
      "Epoch: 51 - Batch: 143296 - Loss: 0.549943 - Time:329.75048208236694\n",
      "Epoch: 51 - Batch: 147392 - Loss: 0.094671 - Time:339.1626603603363\n",
      "Epoch: 51 - Batch: 151488 - Loss: 0.120256 - Time:348.57556915283203\n",
      "Epoch: 52 - Batch: 4032 - Loss: 0.078050 - Time:9.581156969070435\n",
      "Epoch: 52 - Batch: 8128 - Loss: 0.191226 - Time:18.998783588409424\n",
      "Epoch: 52 - Batch: 12224 - Loss: 0.298356 - Time:28.4418363571167\n",
      "Epoch: 52 - Batch: 16320 - Loss: 0.101324 - Time:37.85512185096741\n",
      "Epoch: 52 - Batch: 20416 - Loss: 0.113805 - Time:47.27080535888672\n",
      "Epoch: 52 - Batch: 24512 - Loss: 0.322264 - Time:56.68529963493347\n",
      "Epoch: 52 - Batch: 28608 - Loss: 0.079723 - Time:66.116464138031\n",
      "Epoch: 52 - Batch: 32704 - Loss: 0.081307 - Time:75.53301310539246\n",
      "Epoch: 52 - Batch: 36800 - Loss: 0.095053 - Time:84.94833850860596\n",
      "Epoch: 52 - Batch: 40896 - Loss: 0.196507 - Time:94.37986063957214\n",
      "Epoch: 52 - Batch: 44992 - Loss: 0.137465 - Time:103.79707336425781\n",
      "Epoch: 52 - Batch: 49088 - Loss: 0.161439 - Time:113.21337628364563\n",
      "Epoch: 52 - Batch: 53184 - Loss: 0.378505 - Time:122.63029885292053\n",
      "Epoch: 52 - Batch: 57280 - Loss: 0.230430 - Time:132.04606223106384\n",
      "Epoch: 52 - Batch: 61376 - Loss: 0.263526 - Time:141.46109294891357\n",
      "Epoch: 52 - Batch: 65472 - Loss: 0.214375 - Time:150.87365055084229\n",
      "Epoch: 52 - Batch: 69568 - Loss: 0.463755 - Time:160.28827786445618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 - Batch: 73664 - Loss: 0.172134 - Time:169.71992945671082\n",
      "Epoch: 52 - Batch: 77760 - Loss: 0.280694 - Time:179.13616609573364\n",
      "Epoch: 52 - Batch: 81856 - Loss: 0.098023 - Time:188.5492136478424\n",
      "Epoch: 52 - Batch: 85952 - Loss: 0.211604 - Time:197.96294283866882\n",
      "Epoch: 52 - Batch: 90048 - Loss: 0.259678 - Time:207.39760375022888\n",
      "Epoch: 52 - Batch: 94144 - Loss: 0.089419 - Time:216.81395888328552\n",
      "Epoch: 52 - Batch: 98240 - Loss: 0.176068 - Time:226.22949719429016\n",
      "Epoch: 52 - Batch: 102336 - Loss: 0.398504 - Time:235.66171669960022\n",
      "Epoch: 52 - Batch: 106432 - Loss: 0.280256 - Time:245.0760452747345\n",
      "Epoch: 52 - Batch: 110528 - Loss: 0.186768 - Time:254.48900270462036\n",
      "Epoch: 52 - Batch: 114624 - Loss: 0.294508 - Time:263.9018449783325\n",
      "Epoch: 52 - Batch: 118720 - Loss: 0.227215 - Time:273.31538438796997\n",
      "Epoch: 52 - Batch: 122816 - Loss: 0.317778 - Time:282.73163390159607\n",
      "Epoch: 52 - Batch: 126912 - Loss: 0.546275 - Time:292.14641857147217\n",
      "Epoch: 52 - Batch: 131008 - Loss: 0.449163 - Time:301.5586771965027\n",
      "Epoch: 52 - Batch: 135104 - Loss: 0.429156 - Time:310.9925410747528\n",
      "Epoch: 52 - Batch: 139200 - Loss: 0.060946 - Time:320.4086947441101\n",
      "Epoch: 52 - Batch: 143296 - Loss: 0.593834 - Time:329.8241021633148\n",
      "Epoch: 52 - Batch: 147392 - Loss: 0.129101 - Time:339.2399425506592\n",
      "Epoch: 52 - Batch: 151488 - Loss: 0.059347 - Time:348.67326307296753\n",
      "Epoch: 53 - Batch: 4032 - Loss: 0.178703 - Time:9.574782371520996\n",
      "Epoch: 53 - Batch: 8128 - Loss: 0.151618 - Time:18.991705179214478\n",
      "Epoch: 53 - Batch: 12224 - Loss: 0.551714 - Time:28.41051983833313\n",
      "Epoch: 53 - Batch: 16320 - Loss: 0.152927 - Time:37.85405969619751\n",
      "Epoch: 53 - Batch: 20416 - Loss: 0.120888 - Time:47.26996874809265\n",
      "Epoch: 53 - Batch: 24512 - Loss: 0.261002 - Time:56.68340826034546\n",
      "Epoch: 53 - Batch: 28608 - Loss: 0.246559 - Time:66.1160683631897\n",
      "Epoch: 53 - Batch: 32704 - Loss: 0.361920 - Time:75.5310230255127\n",
      "Epoch: 53 - Batch: 36800 - Loss: 0.033994 - Time:84.94578790664673\n",
      "Epoch: 53 - Batch: 40896 - Loss: 0.116991 - Time:94.35878252983093\n",
      "Epoch: 53 - Batch: 44992 - Loss: 0.230955 - Time:103.76942133903503\n",
      "Epoch: 53 - Batch: 49088 - Loss: 0.069024 - Time:113.18161487579346\n",
      "Epoch: 53 - Batch: 53184 - Loss: 0.128823 - Time:122.59440493583679\n",
      "Epoch: 53 - Batch: 57280 - Loss: 0.136410 - Time:132.0063807964325\n",
      "Epoch: 53 - Batch: 61376 - Loss: 0.219522 - Time:141.43797755241394\n",
      "Epoch: 53 - Batch: 65472 - Loss: 0.122000 - Time:150.84893822669983\n",
      "Epoch: 53 - Batch: 69568 - Loss: 0.160095 - Time:160.2590343952179\n",
      "Epoch: 53 - Batch: 73664 - Loss: 0.212600 - Time:169.67169880867004\n",
      "Epoch: 53 - Batch: 77760 - Loss: 0.320475 - Time:179.1012327671051\n",
      "Epoch: 53 - Batch: 81856 - Loss: 0.143295 - Time:188.51395559310913\n",
      "Epoch: 53 - Batch: 85952 - Loss: 0.169252 - Time:197.9243893623352\n",
      "Epoch: 53 - Batch: 90048 - Loss: 0.366106 - Time:207.3543462753296\n",
      "Epoch: 53 - Batch: 94144 - Loss: 0.199836 - Time:216.7652850151062\n",
      "Epoch: 53 - Batch: 98240 - Loss: 0.285957 - Time:226.1759295463562\n",
      "Epoch: 53 - Batch: 102336 - Loss: 0.298708 - Time:235.58813166618347\n",
      "Epoch: 53 - Batch: 106432 - Loss: 0.157004 - Time:245.00235223770142\n",
      "Epoch: 53 - Batch: 110528 - Loss: 0.434906 - Time:254.41397643089294\n",
      "Epoch: 53 - Batch: 114624 - Loss: 0.286039 - Time:263.82559847831726\n",
      "Epoch: 53 - Batch: 118720 - Loss: 0.071986 - Time:273.2380037307739\n",
      "Epoch: 53 - Batch: 122816 - Loss: 0.212726 - Time:282.66763067245483\n",
      "Epoch: 53 - Batch: 126912 - Loss: 0.214047 - Time:292.081241607666\n",
      "Epoch: 53 - Batch: 131008 - Loss: 0.295788 - Time:301.4939036369324\n",
      "Epoch: 53 - Batch: 135104 - Loss: 0.145950 - Time:310.905925989151\n",
      "Epoch: 53 - Batch: 139200 - Loss: 0.499058 - Time:320.3351616859436\n",
      "Epoch: 53 - Batch: 143296 - Loss: 0.163518 - Time:329.74623680114746\n",
      "Epoch: 53 - Batch: 147392 - Loss: 0.118481 - Time:339.1575002670288\n",
      "Epoch: 53 - Batch: 151488 - Loss: 0.384995 - Time:348.58936047554016\n",
      "Epoch: 54 - Batch: 4032 - Loss: 0.202188 - Time:9.5767240524292\n",
      "Epoch: 54 - Batch: 8128 - Loss: 0.172223 - Time:18.994001150131226\n",
      "Epoch: 54 - Batch: 12224 - Loss: 0.137026 - Time:28.435984134674072\n",
      "Epoch: 54 - Batch: 16320 - Loss: 0.109663 - Time:37.849276065826416\n",
      "Epoch: 54 - Batch: 20416 - Loss: 0.164828 - Time:47.264174699783325\n",
      "Epoch: 54 - Batch: 24512 - Loss: 0.343414 - Time:56.678035736083984\n",
      "Epoch: 54 - Batch: 28608 - Loss: 0.052708 - Time:66.0928795337677\n",
      "Epoch: 54 - Batch: 32704 - Loss: 0.142438 - Time:75.50726342201233\n",
      "Epoch: 54 - Batch: 36800 - Loss: 0.101091 - Time:84.91997957229614\n",
      "Epoch: 54 - Batch: 40896 - Loss: 0.031928 - Time:94.3333477973938\n",
      "Epoch: 54 - Batch: 44992 - Loss: 0.047604 - Time:103.7641294002533\n",
      "Epoch: 54 - Batch: 49088 - Loss: 0.065163 - Time:113.17915868759155\n",
      "Epoch: 54 - Batch: 53184 - Loss: 0.126062 - Time:122.59404158592224\n",
      "Epoch: 54 - Batch: 57280 - Loss: 0.160312 - Time:132.0075626373291\n",
      "Epoch: 54 - Batch: 61376 - Loss: 0.211348 - Time:141.44033861160278\n",
      "Epoch: 54 - Batch: 65472 - Loss: 0.088120 - Time:150.85217785835266\n",
      "Epoch: 54 - Batch: 69568 - Loss: 0.147724 - Time:160.26454377174377\n",
      "Epoch: 54 - Batch: 73664 - Loss: 0.430327 - Time:169.6938121318817\n",
      "Epoch: 54 - Batch: 77760 - Loss: 0.080179 - Time:179.1092734336853\n",
      "Epoch: 54 - Batch: 81856 - Loss: 0.222619 - Time:188.5247197151184\n",
      "Epoch: 54 - Batch: 85952 - Loss: 0.210030 - Time:197.93865323066711\n",
      "Epoch: 54 - Batch: 90048 - Loss: 0.164519 - Time:207.3529281616211\n",
      "Epoch: 54 - Batch: 94144 - Loss: 0.335536 - Time:216.76415300369263\n",
      "Epoch: 54 - Batch: 98240 - Loss: 0.189056 - Time:226.17724680900574\n",
      "Epoch: 54 - Batch: 102336 - Loss: 0.295884 - Time:235.59084582328796\n",
      "Epoch: 54 - Batch: 106432 - Loss: 0.073331 - Time:245.02323198318481\n",
      "Epoch: 54 - Batch: 110528 - Loss: 0.091110 - Time:254.4389843940735\n",
      "Epoch: 54 - Batch: 114624 - Loss: 0.075475 - Time:263.8521022796631\n",
      "Epoch: 54 - Batch: 118720 - Loss: 0.226143 - Time:273.2646052837372\n",
      "Epoch: 54 - Batch: 122816 - Loss: 0.398599 - Time:282.6763188838959\n",
      "Epoch: 54 - Batch: 126912 - Loss: 0.323030 - Time:292.10490226745605\n",
      "Epoch: 54 - Batch: 131008 - Loss: 0.429227 - Time:301.5151436328888\n",
      "Epoch: 54 - Batch: 135104 - Loss: 0.074893 - Time:310.9264121055603\n",
      "Epoch: 54 - Batch: 139200 - Loss: 0.044780 - Time:320.3548638820648\n",
      "Epoch: 54 - Batch: 143296 - Loss: 0.289214 - Time:329.76807379722595\n",
      "Epoch: 54 - Batch: 147392 - Loss: 0.142852 - Time:339.1798255443573\n",
      "Epoch: 54 - Batch: 151488 - Loss: 0.315570 - Time:348.5913074016571\n",
      "Epoch: 55 - Batch: 4032 - Loss: 0.315757 - Time:9.58626937866211\n",
      "Epoch: 55 - Batch: 8128 - Loss: 0.484885 - Time:19.02988600730896\n",
      "Epoch: 55 - Batch: 12224 - Loss: 0.177666 - Time:28.4431631565094\n",
      "Epoch: 55 - Batch: 16320 - Loss: 0.095720 - Time:37.856889724731445\n",
      "Epoch: 55 - Batch: 20416 - Loss: 0.124335 - Time:47.26768732070923\n",
      "Epoch: 55 - Batch: 24512 - Loss: 0.054497 - Time:56.68191885948181\n",
      "Epoch: 55 - Batch: 28608 - Loss: 0.077974 - Time:66.09443259239197\n",
      "Epoch: 55 - Batch: 32704 - Loss: 0.284913 - Time:75.50671052932739\n",
      "Epoch: 55 - Batch: 36800 - Loss: 0.093004 - Time:84.91844606399536\n",
      "Epoch: 55 - Batch: 40896 - Loss: 0.150447 - Time:94.3500030040741\n",
      "Epoch: 55 - Batch: 44992 - Loss: 0.126365 - Time:103.76250791549683\n",
      "Epoch: 55 - Batch: 49088 - Loss: 0.435482 - Time:113.17440056800842\n",
      "Epoch: 55 - Batch: 53184 - Loss: 0.182202 - Time:122.58750176429749\n",
      "Epoch: 55 - Batch: 57280 - Loss: 0.344851 - Time:132.01612424850464\n",
      "Epoch: 55 - Batch: 61376 - Loss: 0.278489 - Time:141.4276225566864\n",
      "Epoch: 55 - Batch: 65472 - Loss: 0.398166 - Time:150.83870387077332\n",
      "Epoch: 55 - Batch: 69568 - Loss: 0.095721 - Time:160.26585698127747\n",
      "Epoch: 55 - Batch: 73664 - Loss: 0.149840 - Time:169.67948985099792\n",
      "Epoch: 55 - Batch: 77760 - Loss: 0.040886 - Time:179.09310698509216\n",
      "Epoch: 55 - Batch: 81856 - Loss: 0.174055 - Time:188.5063214302063\n",
      "Epoch: 55 - Batch: 85952 - Loss: 0.405999 - Time:197.91711521148682\n",
      "Epoch: 55 - Batch: 90048 - Loss: 0.256125 - Time:207.33069825172424\n",
      "Epoch: 55 - Batch: 94144 - Loss: 0.188887 - Time:216.74348878860474\n",
      "Epoch: 55 - Batch: 98240 - Loss: 0.146913 - Time:226.15660333633423\n",
      "Epoch: 55 - Batch: 102336 - Loss: 0.159724 - Time:235.58585476875305\n",
      "Epoch: 55 - Batch: 106432 - Loss: 0.277844 - Time:244.99565935134888\n",
      "Epoch: 55 - Batch: 110528 - Loss: 0.180281 - Time:254.40838503837585\n",
      "Epoch: 55 - Batch: 114624 - Loss: 0.216957 - Time:263.8194262981415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 - Batch: 118720 - Loss: 0.375302 - Time:273.24765133857727\n",
      "Epoch: 55 - Batch: 122816 - Loss: 0.420737 - Time:282.65852332115173\n",
      "Epoch: 55 - Batch: 126912 - Loss: 0.264825 - Time:292.07089161872864\n",
      "Epoch: 55 - Batch: 131008 - Loss: 0.103738 - Time:301.505167722702\n",
      "Epoch: 55 - Batch: 135104 - Loss: 0.113727 - Time:310.9181215763092\n",
      "Epoch: 55 - Batch: 139200 - Loss: 0.433274 - Time:320.3312044143677\n",
      "Epoch: 55 - Batch: 143296 - Loss: 0.152693 - Time:329.74465441703796\n",
      "Epoch: 55 - Batch: 147392 - Loss: 0.193226 - Time:339.1552631855011\n",
      "Epoch: 55 - Batch: 151488 - Loss: 0.174932 - Time:348.5696656703949\n",
      "Epoch: 56 - Batch: 4032 - Loss: 0.224499 - Time:9.580031633377075\n",
      "Epoch: 56 - Batch: 8128 - Loss: 0.096057 - Time:18.998563528060913\n",
      "Epoch: 56 - Batch: 12224 - Loss: 0.573436 - Time:28.4412784576416\n",
      "Epoch: 56 - Batch: 16320 - Loss: 0.163467 - Time:37.85487699508667\n",
      "Epoch: 56 - Batch: 20416 - Loss: 0.239240 - Time:47.26740121841431\n",
      "Epoch: 56 - Batch: 24512 - Loss: 0.129938 - Time:56.6817991733551\n",
      "Epoch: 56 - Batch: 28608 - Loss: 0.070973 - Time:66.11494708061218\n",
      "Epoch: 56 - Batch: 32704 - Loss: 0.083241 - Time:75.53004741668701\n",
      "Epoch: 56 - Batch: 36800 - Loss: 0.176585 - Time:84.94530844688416\n",
      "Epoch: 56 - Batch: 40896 - Loss: 0.322302 - Time:94.37817907333374\n",
      "Epoch: 56 - Batch: 44992 - Loss: 0.544997 - Time:103.79565954208374\n",
      "Epoch: 56 - Batch: 49088 - Loss: 0.226401 - Time:113.20926356315613\n",
      "Epoch: 56 - Batch: 53184 - Loss: 0.454010 - Time:122.6249008178711\n",
      "Epoch: 56 - Batch: 57280 - Loss: 0.232686 - Time:132.03985738754272\n",
      "Epoch: 56 - Batch: 61376 - Loss: 0.275091 - Time:141.45580554008484\n",
      "Epoch: 56 - Batch: 65472 - Loss: 0.112226 - Time:150.87022924423218\n",
      "Epoch: 56 - Batch: 69568 - Loss: 0.293688 - Time:160.28364419937134\n",
      "Epoch: 56 - Batch: 73664 - Loss: 0.461876 - Time:169.71580004692078\n",
      "Epoch: 56 - Batch: 77760 - Loss: 0.217326 - Time:179.13266277313232\n",
      "Epoch: 56 - Batch: 81856 - Loss: 0.276638 - Time:188.54773688316345\n",
      "Epoch: 56 - Batch: 85952 - Loss: 0.307794 - Time:197.96312856674194\n",
      "Epoch: 56 - Batch: 90048 - Loss: 0.176563 - Time:207.39239478111267\n",
      "Epoch: 56 - Batch: 94144 - Loss: 0.145540 - Time:216.80482721328735\n",
      "Epoch: 56 - Batch: 98240 - Loss: 0.120551 - Time:226.21658301353455\n",
      "Epoch: 56 - Batch: 102336 - Loss: 0.250514 - Time:235.64556312561035\n",
      "Epoch: 56 - Batch: 106432 - Loss: 0.619678 - Time:245.06105518341064\n",
      "Epoch: 56 - Batch: 110528 - Loss: 0.306262 - Time:254.4776475429535\n",
      "Epoch: 56 - Batch: 114624 - Loss: 0.647948 - Time:263.8933095932007\n",
      "Epoch: 56 - Batch: 118720 - Loss: 0.331872 - Time:273.30927896499634\n",
      "Epoch: 56 - Batch: 122816 - Loss: 0.427198 - Time:282.7247414588928\n",
      "Epoch: 56 - Batch: 126912 - Loss: 0.050845 - Time:292.1391849517822\n",
      "Epoch: 56 - Batch: 131008 - Loss: 0.187227 - Time:301.55329418182373\n",
      "Epoch: 56 - Batch: 135104 - Loss: 0.148170 - Time:310.9827046394348\n",
      "Epoch: 56 - Batch: 139200 - Loss: 0.180949 - Time:320.39557361602783\n",
      "Epoch: 56 - Batch: 143296 - Loss: 0.276149 - Time:329.8104422092438\n",
      "Epoch: 56 - Batch: 147392 - Loss: 0.604397 - Time:339.2259621620178\n",
      "Epoch: 56 - Batch: 151488 - Loss: 0.385067 - Time:348.65745663642883\n",
      "Epoch: 57 - Batch: 4032 - Loss: 0.238629 - Time:9.582838535308838\n",
      "Epoch: 57 - Batch: 8128 - Loss: 0.211489 - Time:18.999495267868042\n",
      "Epoch: 57 - Batch: 12224 - Loss: 0.190347 - Time:28.41363763809204\n",
      "Epoch: 57 - Batch: 16320 - Loss: 0.150302 - Time:37.85706686973572\n",
      "Epoch: 57 - Batch: 20416 - Loss: 0.061870 - Time:47.271058797836304\n",
      "Epoch: 57 - Batch: 24512 - Loss: 0.059844 - Time:56.68565630912781\n",
      "Epoch: 57 - Batch: 28608 - Loss: 0.020873 - Time:66.11590909957886\n",
      "Epoch: 57 - Batch: 32704 - Loss: 0.072296 - Time:75.53011465072632\n",
      "Epoch: 57 - Batch: 36800 - Loss: 0.042413 - Time:84.94276690483093\n",
      "Epoch: 57 - Batch: 40896 - Loss: 0.145355 - Time:94.35251307487488\n",
      "Epoch: 57 - Batch: 44992 - Loss: 0.249543 - Time:103.76428413391113\n",
      "Epoch: 57 - Batch: 49088 - Loss: 0.508767 - Time:113.1737449169159\n",
      "Epoch: 57 - Batch: 53184 - Loss: 0.076328 - Time:122.58420753479004\n",
      "Epoch: 57 - Batch: 57280 - Loss: 0.078738 - Time:131.99568033218384\n",
      "Epoch: 57 - Batch: 61376 - Loss: 0.155933 - Time:141.4261302947998\n",
      "Epoch: 57 - Batch: 65472 - Loss: 0.533760 - Time:150.8377890586853\n",
      "Epoch: 57 - Batch: 69568 - Loss: 0.099980 - Time:160.2496521472931\n",
      "Epoch: 57 - Batch: 73664 - Loss: 0.128148 - Time:169.66169023513794\n",
      "Epoch: 57 - Batch: 77760 - Loss: 0.460291 - Time:179.0901961326599\n",
      "Epoch: 57 - Batch: 81856 - Loss: 0.071945 - Time:188.5038034915924\n",
      "Epoch: 57 - Batch: 85952 - Loss: 0.112230 - Time:197.91676688194275\n",
      "Epoch: 57 - Batch: 90048 - Loss: 0.084425 - Time:207.3461287021637\n",
      "Epoch: 57 - Batch: 94144 - Loss: 0.139793 - Time:216.75834846496582\n",
      "Epoch: 57 - Batch: 98240 - Loss: 0.258343 - Time:226.17119669914246\n",
      "Epoch: 57 - Batch: 102336 - Loss: 0.247145 - Time:235.5835108757019\n",
      "Epoch: 57 - Batch: 106432 - Loss: 0.171488 - Time:244.9961233139038\n",
      "Epoch: 57 - Batch: 110528 - Loss: 0.082051 - Time:254.40716314315796\n",
      "Epoch: 57 - Batch: 114624 - Loss: 0.285353 - Time:263.82028555870056\n",
      "Epoch: 57 - Batch: 118720 - Loss: 0.292030 - Time:273.2344937324524\n",
      "Epoch: 57 - Batch: 122816 - Loss: 0.261742 - Time:282.6625738143921\n",
      "Epoch: 57 - Batch: 126912 - Loss: 0.293886 - Time:292.0734190940857\n",
      "Epoch: 57 - Batch: 131008 - Loss: 0.165969 - Time:301.4838206768036\n",
      "Epoch: 57 - Batch: 135104 - Loss: 0.440051 - Time:310.8979654312134\n",
      "Epoch: 57 - Batch: 139200 - Loss: 0.351868 - Time:320.32759976387024\n",
      "Epoch: 57 - Batch: 143296 - Loss: 0.265533 - Time:329.73975014686584\n",
      "Epoch: 57 - Batch: 147392 - Loss: 0.242322 - Time:339.1533534526825\n",
      "Epoch: 57 - Batch: 151488 - Loss: 0.327911 - Time:348.5836365222931\n",
      "Epoch: 58 - Batch: 4032 - Loss: 0.385027 - Time:9.573072910308838\n",
      "Epoch: 58 - Batch: 8128 - Loss: 0.208050 - Time:18.9879207611084\n",
      "Epoch: 58 - Batch: 12224 - Loss: 0.226383 - Time:28.430724382400513\n",
      "Epoch: 58 - Batch: 16320 - Loss: 0.055260 - Time:37.843507289886475\n",
      "Epoch: 58 - Batch: 20416 - Loss: 0.056619 - Time:47.256521224975586\n",
      "Epoch: 58 - Batch: 24512 - Loss: 0.284066 - Time:56.67351698875427\n",
      "Epoch: 58 - Batch: 28608 - Loss: 0.186140 - Time:66.08810496330261\n",
      "Epoch: 58 - Batch: 32704 - Loss: 0.144123 - Time:75.50467729568481\n",
      "Epoch: 58 - Batch: 36800 - Loss: 0.040795 - Time:84.91926288604736\n",
      "Epoch: 58 - Batch: 40896 - Loss: 0.375890 - Time:94.33005881309509\n",
      "Epoch: 58 - Batch: 44992 - Loss: 0.053719 - Time:103.7615098953247\n",
      "Epoch: 58 - Batch: 49088 - Loss: 0.065480 - Time:113.17569041252136\n",
      "Epoch: 58 - Batch: 53184 - Loss: 0.288095 - Time:122.58871841430664\n",
      "Epoch: 58 - Batch: 57280 - Loss: 0.393289 - Time:132.00107336044312\n",
      "Epoch: 58 - Batch: 61376 - Loss: 0.207821 - Time:141.4292392730713\n",
      "Epoch: 58 - Batch: 65472 - Loss: 0.040788 - Time:150.84427094459534\n",
      "Epoch: 58 - Batch: 69568 - Loss: 0.369400 - Time:160.25843811035156\n",
      "Epoch: 58 - Batch: 73664 - Loss: 0.045918 - Time:169.68966007232666\n",
      "Epoch: 58 - Batch: 77760 - Loss: 0.290456 - Time:179.10162353515625\n",
      "Epoch: 58 - Batch: 81856 - Loss: 0.089975 - Time:188.51502919197083\n",
      "Epoch: 58 - Batch: 85952 - Loss: 0.370705 - Time:197.9275815486908\n",
      "Epoch: 58 - Batch: 90048 - Loss: 0.164567 - Time:207.34216904640198\n",
      "Epoch: 58 - Batch: 94144 - Loss: 0.209131 - Time:216.75495409965515\n",
      "Epoch: 58 - Batch: 98240 - Loss: 0.264773 - Time:226.16891741752625\n",
      "Epoch: 58 - Batch: 102336 - Loss: 0.204682 - Time:235.58268570899963\n",
      "Epoch: 58 - Batch: 106432 - Loss: 0.043963 - Time:245.01249241828918\n",
      "Epoch: 58 - Batch: 110528 - Loss: 0.053701 - Time:254.42565202713013\n",
      "Epoch: 58 - Batch: 114624 - Loss: 0.277235 - Time:263.84093499183655\n",
      "Epoch: 58 - Batch: 118720 - Loss: 0.070024 - Time:273.25251364707947\n",
      "Epoch: 58 - Batch: 122816 - Loss: 0.121042 - Time:282.6674530506134\n",
      "Epoch: 58 - Batch: 126912 - Loss: 0.266263 - Time:292.09987902641296\n",
      "Epoch: 58 - Batch: 131008 - Loss: 0.330617 - Time:301.51576113700867\n",
      "Epoch: 58 - Batch: 135104 - Loss: 0.297758 - Time:310.93135356903076\n",
      "Epoch: 58 - Batch: 139200 - Loss: 0.181187 - Time:320.36396956443787\n",
      "Epoch: 58 - Batch: 143296 - Loss: 0.205210 - Time:329.7782940864563\n",
      "Epoch: 58 - Batch: 147392 - Loss: 0.231424 - Time:339.19352436065674\n",
      "Epoch: 58 - Batch: 151488 - Loss: 0.359197 - Time:348.6091809272766\n",
      "Epoch: 59 - Batch: 4032 - Loss: 0.263196 - Time:9.575485467910767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 - Batch: 8128 - Loss: 0.114919 - Time:19.02117085456848\n",
      "Epoch: 59 - Batch: 12224 - Loss: 0.067775 - Time:28.437156438827515\n",
      "Epoch: 59 - Batch: 16320 - Loss: 0.096512 - Time:37.85534143447876\n",
      "Epoch: 59 - Batch: 20416 - Loss: 0.072673 - Time:47.271098613739014\n",
      "Epoch: 59 - Batch: 24512 - Loss: 0.086213 - Time:56.68697738647461\n",
      "Epoch: 59 - Batch: 28608 - Loss: 0.795159 - Time:66.10118436813354\n",
      "Epoch: 59 - Batch: 32704 - Loss: 0.088465 - Time:75.5132462978363\n",
      "Epoch: 59 - Batch: 36800 - Loss: 0.771438 - Time:84.92782545089722\n",
      "Epoch: 59 - Batch: 40896 - Loss: 0.055809 - Time:94.35743975639343\n",
      "Epoch: 59 - Batch: 44992 - Loss: 0.108136 - Time:103.7730541229248\n",
      "Epoch: 59 - Batch: 49088 - Loss: 0.028540 - Time:113.18804740905762\n",
      "Epoch: 59 - Batch: 53184 - Loss: 0.245526 - Time:122.60373282432556\n",
      "Epoch: 59 - Batch: 57280 - Loss: 0.165363 - Time:132.03424763679504\n",
      "Epoch: 59 - Batch: 61376 - Loss: 0.158783 - Time:141.44807052612305\n",
      "Epoch: 59 - Batch: 65472 - Loss: 0.169092 - Time:150.86361360549927\n",
      "Epoch: 59 - Batch: 69568 - Loss: 0.188987 - Time:160.29802775382996\n",
      "Epoch: 59 - Batch: 73664 - Loss: 0.053511 - Time:169.71292805671692\n",
      "Epoch: 59 - Batch: 77760 - Loss: 0.357536 - Time:179.1284384727478\n",
      "Epoch: 59 - Batch: 81856 - Loss: 0.089854 - Time:188.53965854644775\n",
      "Epoch: 59 - Batch: 85952 - Loss: 0.042126 - Time:197.95269799232483\n",
      "Epoch: 59 - Batch: 90048 - Loss: 0.160945 - Time:207.36767649650574\n",
      "Epoch: 59 - Batch: 94144 - Loss: 0.076143 - Time:216.78244996070862\n",
      "Epoch: 59 - Batch: 98240 - Loss: 0.075234 - Time:226.19547939300537\n",
      "Epoch: 59 - Batch: 102336 - Loss: 0.263552 - Time:235.62592840194702\n",
      "Epoch: 59 - Batch: 106432 - Loss: 0.101736 - Time:245.04006958007812\n",
      "Epoch: 59 - Batch: 110528 - Loss: 0.482733 - Time:254.45117020606995\n",
      "Epoch: 59 - Batch: 114624 - Loss: 0.234455 - Time:263.8626947402954\n",
      "Epoch: 59 - Batch: 118720 - Loss: 0.295416 - Time:273.29302620887756\n",
      "Epoch: 59 - Batch: 122816 - Loss: 0.199082 - Time:282.7053518295288\n",
      "Epoch: 59 - Batch: 126912 - Loss: 0.173786 - Time:292.11908197402954\n",
      "Epoch: 59 - Batch: 131008 - Loss: 0.193243 - Time:301.55159640312195\n",
      "Epoch: 59 - Batch: 135104 - Loss: 0.187152 - Time:310.9635384082794\n",
      "Epoch: 59 - Batch: 139200 - Loss: 0.080050 - Time:320.3781795501709\n",
      "Epoch: 59 - Batch: 143296 - Loss: 0.045795 - Time:329.79196333885193\n",
      "Epoch: 59 - Batch: 147392 - Loss: 0.297914 - Time:339.20471382141113\n",
      "Epoch: 59 - Batch: 151488 - Loss: 0.270668 - Time:348.6178388595581\n",
      "Epoch: 60 - Batch: 4032 - Loss: 0.183043 - Time:9.57749319076538\n",
      "Epoch: 60 - Batch: 8128 - Loss: 0.113194 - Time:18.993995904922485\n",
      "Epoch: 60 - Batch: 12224 - Loss: 0.168339 - Time:28.43870997428894\n",
      "Epoch: 60 - Batch: 16320 - Loss: 0.181834 - Time:37.85538196563721\n",
      "Epoch: 60 - Batch: 20416 - Loss: 0.061728 - Time:47.269288539886475\n",
      "Epoch: 60 - Batch: 24512 - Loss: 0.094903 - Time:56.68372702598572\n",
      "Epoch: 60 - Batch: 28608 - Loss: 0.040910 - Time:66.1158447265625\n",
      "Epoch: 60 - Batch: 32704 - Loss: 0.079290 - Time:75.5283350944519\n",
      "Epoch: 60 - Batch: 36800 - Loss: 0.037375 - Time:84.94316720962524\n",
      "Epoch: 60 - Batch: 40896 - Loss: 0.050458 - Time:94.37444281578064\n",
      "Epoch: 60 - Batch: 44992 - Loss: 0.160162 - Time:103.79040956497192\n",
      "Epoch: 60 - Batch: 49088 - Loss: 0.115255 - Time:113.2027645111084\n",
      "Epoch: 60 - Batch: 53184 - Loss: 0.183570 - Time:122.61578488349915\n",
      "Epoch: 60 - Batch: 57280 - Loss: 0.213649 - Time:132.0318169593811\n",
      "Epoch: 60 - Batch: 61376 - Loss: 0.247411 - Time:141.4460871219635\n",
      "Epoch: 60 - Batch: 65472 - Loss: 0.059741 - Time:150.86277318000793\n",
      "Epoch: 60 - Batch: 69568 - Loss: 0.126220 - Time:160.27699208259583\n",
      "Epoch: 60 - Batch: 73664 - Loss: 0.207239 - Time:169.70946311950684\n",
      "Epoch: 60 - Batch: 77760 - Loss: 0.632708 - Time:179.1226327419281\n",
      "Epoch: 60 - Batch: 81856 - Loss: 0.145012 - Time:188.53451776504517\n",
      "Epoch: 60 - Batch: 85952 - Loss: 0.057631 - Time:197.94434905052185\n",
      "Epoch: 60 - Batch: 90048 - Loss: 0.039769 - Time:207.3758270740509\n",
      "Epoch: 60 - Batch: 94144 - Loss: 0.340345 - Time:216.78860116004944\n",
      "Epoch: 60 - Batch: 98240 - Loss: 0.182153 - Time:226.20068669319153\n",
      "Epoch: 60 - Batch: 102336 - Loss: 0.110277 - Time:235.63158917427063\n",
      "Epoch: 60 - Batch: 106432 - Loss: 0.184161 - Time:245.04300999641418\n",
      "Epoch: 60 - Batch: 110528 - Loss: 0.037398 - Time:254.4552297592163\n",
      "Epoch: 60 - Batch: 114624 - Loss: 0.308375 - Time:263.8685374259949\n",
      "Epoch: 60 - Batch: 118720 - Loss: 0.147224 - Time:273.28101992607117\n",
      "Epoch: 60 - Batch: 122816 - Loss: 0.065251 - Time:282.69438433647156\n",
      "Epoch: 60 - Batch: 126912 - Loss: 0.326464 - Time:292.1076817512512\n",
      "Epoch: 60 - Batch: 131008 - Loss: 0.239453 - Time:301.521044254303\n",
      "Epoch: 60 - Batch: 135104 - Loss: 0.252027 - Time:310.95175313949585\n",
      "Epoch: 60 - Batch: 139200 - Loss: 0.283960 - Time:320.36472058296204\n",
      "Epoch: 60 - Batch: 143296 - Loss: 0.634525 - Time:329.781640291214\n",
      "Epoch: 60 - Batch: 147392 - Loss: 0.235069 - Time:339.1962387561798\n",
      "Epoch: 60 - Batch: 151488 - Loss: 0.114685 - Time:348.62811732292175\n",
      "Epoch: 61 - Batch: 4032 - Loss: 0.316511 - Time:9.582908868789673\n",
      "Epoch: 61 - Batch: 8128 - Loss: 0.110819 - Time:18.99904751777649\n",
      "Epoch: 61 - Batch: 12224 - Loss: 0.081850 - Time:28.413273572921753\n",
      "Epoch: 61 - Batch: 16320 - Loss: 0.060392 - Time:37.8542160987854\n",
      "Epoch: 61 - Batch: 20416 - Loss: 0.174072 - Time:47.26789951324463\n",
      "Epoch: 61 - Batch: 24512 - Loss: 0.203906 - Time:56.68473768234253\n",
      "Epoch: 61 - Batch: 28608 - Loss: 0.058554 - Time:66.11595439910889\n",
      "Epoch: 61 - Batch: 32704 - Loss: 0.114618 - Time:75.53103375434875\n",
      "Epoch: 61 - Batch: 36800 - Loss: 0.112428 - Time:84.94812560081482\n",
      "Epoch: 61 - Batch: 40896 - Loss: 0.106916 - Time:94.36312007904053\n",
      "Epoch: 61 - Batch: 44992 - Loss: 0.119245 - Time:103.77907419204712\n",
      "Epoch: 61 - Batch: 49088 - Loss: 0.039909 - Time:113.19400215148926\n",
      "Epoch: 61 - Batch: 53184 - Loss: 0.073807 - Time:122.60727572441101\n",
      "Epoch: 61 - Batch: 57280 - Loss: 0.539520 - Time:132.02117729187012\n",
      "Epoch: 61 - Batch: 61376 - Loss: 0.162780 - Time:141.44892239570618\n",
      "Epoch: 61 - Batch: 65472 - Loss: 0.248512 - Time:150.86020708084106\n",
      "Epoch: 61 - Batch: 69568 - Loss: 0.065163 - Time:160.27197694778442\n",
      "Epoch: 61 - Batch: 73664 - Loss: 0.034555 - Time:169.6835527420044\n",
      "Epoch: 61 - Batch: 77760 - Loss: 0.587436 - Time:179.11460375785828\n",
      "Epoch: 61 - Batch: 81856 - Loss: 0.341268 - Time:188.52683210372925\n",
      "Epoch: 61 - Batch: 85952 - Loss: 0.302363 - Time:197.93920516967773\n",
      "Epoch: 61 - Batch: 90048 - Loss: 0.291986 - Time:207.37050247192383\n",
      "Epoch: 61 - Batch: 94144 - Loss: 0.341646 - Time:216.78351402282715\n",
      "Epoch: 61 - Batch: 98240 - Loss: 0.224274 - Time:226.19539093971252\n",
      "Epoch: 61 - Batch: 102336 - Loss: 0.193633 - Time:235.60740637779236\n",
      "Epoch: 61 - Batch: 106432 - Loss: 0.200134 - Time:245.02083468437195\n",
      "Epoch: 61 - Batch: 110528 - Loss: 0.038950 - Time:254.43355464935303\n",
      "Epoch: 61 - Batch: 114624 - Loss: 0.137674 - Time:263.84521317481995\n",
      "Epoch: 61 - Batch: 118720 - Loss: 0.194984 - Time:273.25697469711304\n",
      "Epoch: 61 - Batch: 122816 - Loss: 0.130413 - Time:282.6868665218353\n",
      "Epoch: 61 - Batch: 126912 - Loss: 0.067026 - Time:292.09801387786865\n",
      "Epoch: 61 - Batch: 131008 - Loss: 0.080784 - Time:301.51266837120056\n",
      "Epoch: 61 - Batch: 135104 - Loss: 0.245702 - Time:310.92689180374146\n",
      "Epoch: 61 - Batch: 139200 - Loss: 0.066964 - Time:320.35662388801575\n",
      "Epoch: 61 - Batch: 143296 - Loss: 0.191656 - Time:329.7704954147339\n",
      "Epoch: 61 - Batch: 147392 - Loss: 0.048381 - Time:339.18152594566345\n",
      "Epoch: 61 - Batch: 151488 - Loss: 0.195655 - Time:348.6100072860718\n",
      "Epoch: 62 - Batch: 4032 - Loss: 0.099739 - Time:9.571918487548828\n",
      "Epoch: 62 - Batch: 8128 - Loss: 0.205591 - Time:18.98923110961914\n",
      "Epoch: 62 - Batch: 12224 - Loss: 0.183860 - Time:28.43505835533142\n",
      "Epoch: 62 - Batch: 16320 - Loss: 0.241221 - Time:37.848024129867554\n",
      "Epoch: 62 - Batch: 20416 - Loss: 0.075526 - Time:47.263413190841675\n",
      "Epoch: 62 - Batch: 24512 - Loss: 0.140412 - Time:56.67824935913086\n",
      "Epoch: 62 - Batch: 28608 - Loss: 0.351593 - Time:66.09099674224854\n",
      "Epoch: 62 - Batch: 32704 - Loss: 0.056085 - Time:75.50307941436768\n",
      "Epoch: 62 - Batch: 36800 - Loss: 0.030980 - Time:84.91511464118958\n",
      "Epoch: 62 - Batch: 40896 - Loss: 0.203448 - Time:94.32814431190491\n",
      "Epoch: 62 - Batch: 44992 - Loss: 0.335423 - Time:103.75874161720276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 - Batch: 49088 - Loss: 0.200130 - Time:113.17170643806458\n",
      "Epoch: 62 - Batch: 53184 - Loss: 0.077488 - Time:122.58346629142761\n",
      "Epoch: 62 - Batch: 57280 - Loss: 0.056835 - Time:131.99790930747986\n",
      "Epoch: 62 - Batch: 61376 - Loss: 0.137408 - Time:141.43053483963013\n",
      "Epoch: 62 - Batch: 65472 - Loss: 0.235373 - Time:150.84428024291992\n",
      "Epoch: 62 - Batch: 69568 - Loss: 0.207889 - Time:160.25696444511414\n",
      "Epoch: 62 - Batch: 73664 - Loss: 0.210459 - Time:169.68637132644653\n",
      "Epoch: 62 - Batch: 77760 - Loss: 0.075154 - Time:179.09796380996704\n",
      "Epoch: 62 - Batch: 81856 - Loss: 0.060339 - Time:188.50790405273438\n",
      "Epoch: 62 - Batch: 85952 - Loss: 0.108077 - Time:197.92037630081177\n",
      "Epoch: 62 - Batch: 90048 - Loss: 0.153984 - Time:207.33312273025513\n",
      "Epoch: 62 - Batch: 94144 - Loss: 0.232953 - Time:216.7472698688507\n",
      "Epoch: 62 - Batch: 98240 - Loss: 0.171180 - Time:226.16139888763428\n",
      "Epoch: 62 - Batch: 102336 - Loss: 0.399138 - Time:235.57329726219177\n",
      "Epoch: 62 - Batch: 106432 - Loss: 0.150526 - Time:245.00355863571167\n",
      "Epoch: 62 - Batch: 110528 - Loss: 0.049517 - Time:254.4158535003662\n",
      "Epoch: 62 - Batch: 114624 - Loss: 0.132243 - Time:263.82885909080505\n",
      "Epoch: 62 - Batch: 118720 - Loss: 0.299341 - Time:273.2421681880951\n",
      "Epoch: 62 - Batch: 122816 - Loss: 0.400993 - Time:282.6555655002594\n",
      "Epoch: 62 - Batch: 126912 - Loss: 0.321714 - Time:292.08464646339417\n",
      "Epoch: 62 - Batch: 131008 - Loss: 0.357880 - Time:301.4972448348999\n",
      "Epoch: 62 - Batch: 135104 - Loss: 0.273606 - Time:310.91180062294006\n",
      "Epoch: 62 - Batch: 139200 - Loss: 0.281742 - Time:320.34134006500244\n",
      "Epoch: 62 - Batch: 143296 - Loss: 0.105503 - Time:329.7545530796051\n",
      "Epoch: 62 - Batch: 147392 - Loss: 0.209674 - Time:339.16757345199585\n",
      "Epoch: 62 - Batch: 151488 - Loss: 0.078840 - Time:348.5809338092804\n",
      "Epoch: 63 - Batch: 4032 - Loss: 0.299320 - Time:9.572212934494019\n",
      "Epoch: 63 - Batch: 8128 - Loss: 0.085810 - Time:19.01588463783264\n",
      "Epoch: 63 - Batch: 12224 - Loss: 0.192587 - Time:28.430362462997437\n",
      "Epoch: 63 - Batch: 16320 - Loss: 0.115915 - Time:37.84571027755737\n",
      "Epoch: 63 - Batch: 20416 - Loss: 0.094556 - Time:47.25826287269592\n",
      "Epoch: 63 - Batch: 24512 - Loss: 0.229555 - Time:56.674949645996094\n",
      "Epoch: 63 - Batch: 28608 - Loss: 0.049456 - Time:66.08871245384216\n",
      "Epoch: 63 - Batch: 32704 - Loss: 0.084081 - Time:75.49985003471375\n",
      "Epoch: 63 - Batch: 36800 - Loss: 0.087392 - Time:84.91227626800537\n",
      "Epoch: 63 - Batch: 40896 - Loss: 0.312069 - Time:94.34021472930908\n",
      "Epoch: 63 - Batch: 44992 - Loss: 0.028050 - Time:103.75305986404419\n",
      "Epoch: 63 - Batch: 49088 - Loss: 0.041333 - Time:113.16445016860962\n",
      "Epoch: 63 - Batch: 53184 - Loss: 0.273288 - Time:122.57724952697754\n",
      "Epoch: 63 - Batch: 57280 - Loss: 0.330049 - Time:132.00803399085999\n",
      "Epoch: 63 - Batch: 61376 - Loss: 0.183373 - Time:141.42057299613953\n",
      "Epoch: 63 - Batch: 65472 - Loss: 0.161864 - Time:150.83285570144653\n",
      "Epoch: 63 - Batch: 69568 - Loss: 0.276406 - Time:160.26475834846497\n",
      "Epoch: 63 - Batch: 73664 - Loss: 0.066753 - Time:169.67785787582397\n",
      "Epoch: 63 - Batch: 77760 - Loss: 0.376602 - Time:179.08982348442078\n",
      "Epoch: 63 - Batch: 81856 - Loss: 0.070812 - Time:188.50120282173157\n",
      "Epoch: 63 - Batch: 85952 - Loss: 0.109575 - Time:197.914785861969\n",
      "Epoch: 63 - Batch: 90048 - Loss: 0.261322 - Time:207.328519821167\n",
      "Epoch: 63 - Batch: 94144 - Loss: 0.387367 - Time:216.74290084838867\n",
      "Epoch: 63 - Batch: 98240 - Loss: 0.397182 - Time:226.15977144241333\n",
      "Epoch: 63 - Batch: 102336 - Loss: 0.087453 - Time:235.59357619285583\n",
      "Epoch: 63 - Batch: 106432 - Loss: 0.193206 - Time:245.0079469680786\n",
      "Epoch: 63 - Batch: 110528 - Loss: 0.275797 - Time:254.42155766487122\n",
      "Epoch: 63 - Batch: 114624 - Loss: 0.208455 - Time:263.83533453941345\n",
      "Epoch: 63 - Batch: 118720 - Loss: 0.741054 - Time:273.26611852645874\n",
      "Epoch: 63 - Batch: 122816 - Loss: 0.100899 - Time:282.6802067756653\n",
      "Epoch: 63 - Batch: 126912 - Loss: 0.210980 - Time:292.09222197532654\n",
      "Epoch: 63 - Batch: 131008 - Loss: 0.615431 - Time:301.525671005249\n",
      "Epoch: 63 - Batch: 135104 - Loss: 0.097616 - Time:310.93816924095154\n",
      "Epoch: 63 - Batch: 139200 - Loss: 0.201829 - Time:320.3523097038269\n",
      "Epoch: 63 - Batch: 143296 - Loss: 0.122017 - Time:329.7647798061371\n",
      "Epoch: 63 - Batch: 147392 - Loss: 0.263831 - Time:339.17859983444214\n",
      "Epoch: 63 - Batch: 151488 - Loss: 0.041031 - Time:348.5902328491211\n",
      "Epoch: 64 - Batch: 4032 - Loss: 0.195648 - Time:9.575435400009155\n",
      "Epoch: 64 - Batch: 8128 - Loss: 0.066313 - Time:18.99286675453186\n",
      "Epoch: 64 - Batch: 12224 - Loss: 0.231847 - Time:28.43492341041565\n",
      "Epoch: 64 - Batch: 16320 - Loss: 0.257211 - Time:37.849663734436035\n",
      "Epoch: 64 - Batch: 20416 - Loss: 0.038769 - Time:47.26324009895325\n",
      "Epoch: 64 - Batch: 24512 - Loss: 0.055718 - Time:56.6757538318634\n",
      "Epoch: 64 - Batch: 28608 - Loss: 0.257471 - Time:66.10548853874207\n",
      "Epoch: 64 - Batch: 32704 - Loss: 0.159504 - Time:75.51881051063538\n",
      "Epoch: 64 - Batch: 36800 - Loss: 0.250145 - Time:84.93137216567993\n",
      "Epoch: 64 - Batch: 40896 - Loss: 0.140364 - Time:94.36250066757202\n",
      "Epoch: 64 - Batch: 44992 - Loss: 0.215231 - Time:103.7772421836853\n",
      "Epoch: 64 - Batch: 49088 - Loss: 0.031191 - Time:113.19186162948608\n",
      "Epoch: 64 - Batch: 53184 - Loss: 0.333589 - Time:122.6047420501709\n",
      "Epoch: 64 - Batch: 57280 - Loss: 0.257868 - Time:132.01968502998352\n",
      "Epoch: 64 - Batch: 61376 - Loss: 0.288857 - Time:141.43279480934143\n",
      "Epoch: 64 - Batch: 65472 - Loss: 0.100704 - Time:150.8484320640564\n",
      "Epoch: 64 - Batch: 69568 - Loss: 0.239739 - Time:160.26277565956116\n",
      "Epoch: 64 - Batch: 73664 - Loss: 0.322275 - Time:169.6927194595337\n",
      "Epoch: 64 - Batch: 77760 - Loss: 0.171644 - Time:179.10679078102112\n",
      "Epoch: 64 - Batch: 81856 - Loss: 0.114755 - Time:188.51929092407227\n",
      "Epoch: 64 - Batch: 85952 - Loss: 0.287787 - Time:197.9336359500885\n",
      "Epoch: 64 - Batch: 90048 - Loss: 0.223695 - Time:207.36272811889648\n",
      "Epoch: 64 - Batch: 94144 - Loss: 0.148506 - Time:216.775470495224\n",
      "Epoch: 64 - Batch: 98240 - Loss: 0.228980 - Time:226.18860960006714\n",
      "Epoch: 64 - Batch: 102336 - Loss: 0.250180 - Time:235.61953258514404\n",
      "Epoch: 64 - Batch: 106432 - Loss: 0.278591 - Time:245.033509016037\n",
      "Epoch: 64 - Batch: 110528 - Loss: 0.229425 - Time:254.44504857063293\n",
      "Epoch: 64 - Batch: 114624 - Loss: 0.123253 - Time:263.85829401016235\n",
      "Epoch: 64 - Batch: 118720 - Loss: 0.192485 - Time:273.27179169654846\n",
      "Epoch: 64 - Batch: 122816 - Loss: 0.067711 - Time:282.6839380264282\n",
      "Epoch: 64 - Batch: 126912 - Loss: 0.431337 - Time:292.0964925289154\n",
      "Epoch: 64 - Batch: 131008 - Loss: 0.457984 - Time:301.5084238052368\n",
      "Epoch: 64 - Batch: 135104 - Loss: 0.537532 - Time:310.93802094459534\n",
      "Epoch: 64 - Batch: 139200 - Loss: 0.194388 - Time:320.34974694252014\n",
      "Epoch: 64 - Batch: 143296 - Loss: 0.159387 - Time:329.76222372055054\n",
      "Epoch: 64 - Batch: 147392 - Loss: 0.192524 - Time:339.17492151260376\n",
      "Epoch: 64 - Batch: 151488 - Loss: 0.133162 - Time:348.6057813167572\n",
      "Epoch: 65 - Batch: 4032 - Loss: 0.047025 - Time:9.579836130142212\n",
      "Epoch: 65 - Batch: 8128 - Loss: 0.076960 - Time:18.99564480781555\n",
      "Epoch: 65 - Batch: 12224 - Loss: 0.243318 - Time:28.411080598831177\n",
      "Epoch: 65 - Batch: 16320 - Loss: 0.070609 - Time:37.85303544998169\n",
      "Epoch: 65 - Batch: 20416 - Loss: 0.187506 - Time:47.269426107406616\n",
      "Epoch: 65 - Batch: 24512 - Loss: 0.479185 - Time:56.68242692947388\n",
      "Epoch: 65 - Batch: 28608 - Loss: 0.131380 - Time:66.11332249641418\n",
      "Epoch: 65 - Batch: 32704 - Loss: 0.056987 - Time:75.52518963813782\n",
      "Epoch: 65 - Batch: 36800 - Loss: 0.086293 - Time:84.9388997554779\n",
      "Epoch: 65 - Batch: 40896 - Loss: 0.307209 - Time:94.34992599487305\n",
      "Epoch: 65 - Batch: 44992 - Loss: 0.077038 - Time:103.76042318344116\n",
      "Epoch: 65 - Batch: 49088 - Loss: 0.052723 - Time:113.17340159416199\n",
      "Epoch: 65 - Batch: 53184 - Loss: 0.046902 - Time:122.58704352378845\n",
      "Epoch: 65 - Batch: 57280 - Loss: 0.100391 - Time:132.0003490447998\n",
      "Epoch: 65 - Batch: 61376 - Loss: 0.287754 - Time:141.43002772331238\n",
      "Epoch: 65 - Batch: 65472 - Loss: 0.144507 - Time:150.84312796592712\n",
      "Epoch: 65 - Batch: 69568 - Loss: 0.099326 - Time:160.25729274749756\n",
      "Epoch: 65 - Batch: 73664 - Loss: 0.030347 - Time:169.6716969013214\n",
      "Epoch: 65 - Batch: 77760 - Loss: 0.214185 - Time:179.1047067642212\n",
      "Epoch: 65 - Batch: 81856 - Loss: 0.099301 - Time:188.51677179336548\n",
      "Epoch: 65 - Batch: 85952 - Loss: 0.153022 - Time:197.9295506477356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 - Batch: 90048 - Loss: 0.032454 - Time:207.3582456111908\n",
      "Epoch: 65 - Batch: 94144 - Loss: 0.081543 - Time:216.7713189125061\n",
      "Epoch: 65 - Batch: 98240 - Loss: 0.174741 - Time:226.18582344055176\n",
      "Epoch: 65 - Batch: 102336 - Loss: 0.122904 - Time:235.59842109680176\n",
      "Epoch: 65 - Batch: 106432 - Loss: 0.053800 - Time:245.01203799247742\n",
      "Epoch: 65 - Batch: 110528 - Loss: 0.390773 - Time:254.42625880241394\n",
      "Epoch: 65 - Batch: 114624 - Loss: 0.084740 - Time:263.8392798900604\n",
      "Epoch: 65 - Batch: 118720 - Loss: 0.073792 - Time:273.2503402233124\n",
      "Epoch: 65 - Batch: 122816 - Loss: 0.058154 - Time:282.67936754226685\n",
      "Epoch: 65 - Batch: 126912 - Loss: 0.026077 - Time:292.0930161476135\n",
      "Epoch: 65 - Batch: 131008 - Loss: 0.177774 - Time:301.5050039291382\n",
      "Epoch: 65 - Batch: 135104 - Loss: 0.332376 - Time:310.916428565979\n",
      "Epoch: 65 - Batch: 139200 - Loss: 0.272820 - Time:320.34531140327454\n",
      "Epoch: 65 - Batch: 143296 - Loss: 0.164306 - Time:329.7579827308655\n",
      "Epoch: 65 - Batch: 147392 - Loss: 0.178840 - Time:339.17068433761597\n",
      "Epoch: 65 - Batch: 151488 - Loss: 0.036336 - Time:348.6047773361206\n",
      "Epoch: 66 - Batch: 4032 - Loss: 0.114271 - Time:9.583005905151367\n",
      "Epoch: 66 - Batch: 8128 - Loss: 0.488332 - Time:19.00009274482727\n",
      "Epoch: 66 - Batch: 12224 - Loss: 0.463931 - Time:28.442782640457153\n",
      "Epoch: 66 - Batch: 16320 - Loss: 0.058315 - Time:37.85788178443909\n",
      "Epoch: 66 - Batch: 20416 - Loss: 0.264385 - Time:47.27170252799988\n",
      "Epoch: 66 - Batch: 24512 - Loss: 0.071752 - Time:56.68778944015503\n",
      "Epoch: 66 - Batch: 28608 - Loss: 0.061082 - Time:66.10303568840027\n",
      "Epoch: 66 - Batch: 32704 - Loss: 0.180726 - Time:75.51510572433472\n",
      "Epoch: 66 - Batch: 36800 - Loss: 0.417983 - Time:84.92767024040222\n",
      "Epoch: 66 - Batch: 40896 - Loss: 0.117752 - Time:94.34000730514526\n",
      "Epoch: 66 - Batch: 44992 - Loss: 0.061140 - Time:103.76802587509155\n",
      "Epoch: 66 - Batch: 49088 - Loss: 0.075592 - Time:113.1789402961731\n",
      "Epoch: 66 - Batch: 53184 - Loss: 0.040360 - Time:122.59088730812073\n",
      "Epoch: 66 - Batch: 57280 - Loss: 0.172007 - Time:132.0048463344574\n",
      "Epoch: 66 - Batch: 61376 - Loss: 0.135701 - Time:141.43418407440186\n",
      "Epoch: 66 - Batch: 65472 - Loss: 0.459181 - Time:150.84457206726074\n",
      "Epoch: 66 - Batch: 69568 - Loss: 0.052045 - Time:160.25618076324463\n",
      "Epoch: 66 - Batch: 73664 - Loss: 0.028797 - Time:169.68495798110962\n",
      "Epoch: 66 - Batch: 77760 - Loss: 0.252402 - Time:179.0986168384552\n",
      "Epoch: 66 - Batch: 81856 - Loss: 0.146501 - Time:188.51280665397644\n",
      "Epoch: 66 - Batch: 85952 - Loss: 0.099758 - Time:197.9297435283661\n",
      "Epoch: 66 - Batch: 90048 - Loss: 0.111429 - Time:207.3422658443451\n",
      "Epoch: 66 - Batch: 94144 - Loss: 0.096746 - Time:216.7566180229187\n",
      "Epoch: 66 - Batch: 98240 - Loss: 0.100906 - Time:226.17092418670654\n",
      "Epoch: 66 - Batch: 102336 - Loss: 0.098549 - Time:235.58478593826294\n",
      "Epoch: 66 - Batch: 106432 - Loss: 0.184082 - Time:245.01252675056458\n",
      "Epoch: 66 - Batch: 110528 - Loss: 0.035077 - Time:254.42404413223267\n",
      "Epoch: 66 - Batch: 114624 - Loss: 0.084791 - Time:263.8361141681671\n",
      "Epoch: 66 - Batch: 118720 - Loss: 0.276008 - Time:273.246541261673\n",
      "Epoch: 66 - Batch: 122816 - Loss: 0.310701 - Time:282.65810656547546\n",
      "Epoch: 66 - Batch: 126912 - Loss: 0.154135 - Time:292.0865807533264\n",
      "Epoch: 66 - Batch: 131008 - Loss: 0.034192 - Time:301.4971284866333\n",
      "Epoch: 66 - Batch: 135104 - Loss: 0.340655 - Time:310.9278063774109\n",
      "Epoch: 66 - Batch: 139200 - Loss: 0.044453 - Time:320.3409173488617\n",
      "Epoch: 66 - Batch: 143296 - Loss: 0.034963 - Time:329.7545471191406\n",
      "Epoch: 66 - Batch: 147392 - Loss: 0.110408 - Time:339.1662154197693\n",
      "Epoch: 66 - Batch: 151488 - Loss: 0.042081 - Time:348.5777292251587\n",
      "Epoch: 67 - Batch: 4032 - Loss: 0.123793 - Time:9.577430248260498\n",
      "Epoch: 67 - Batch: 8128 - Loss: 0.033873 - Time:19.021855354309082\n",
      "Epoch: 67 - Batch: 12224 - Loss: 0.031513 - Time:28.435840606689453\n",
      "Epoch: 67 - Batch: 16320 - Loss: 0.048036 - Time:37.848965644836426\n",
      "Epoch: 67 - Batch: 20416 - Loss: 0.573911 - Time:47.276310205459595\n",
      "Epoch: 67 - Batch: 24512 - Loss: 0.129396 - Time:56.68911790847778\n",
      "Epoch: 67 - Batch: 28608 - Loss: 0.051344 - Time:66.1018934249878\n",
      "Epoch: 67 - Batch: 32704 - Loss: 0.247517 - Time:75.51708126068115\n",
      "Epoch: 67 - Batch: 36800 - Loss: 0.125443 - Time:84.9314239025116\n",
      "Epoch: 67 - Batch: 40896 - Loss: 0.197601 - Time:94.36014437675476\n",
      "Epoch: 67 - Batch: 44992 - Loss: 0.077748 - Time:103.7701485157013\n",
      "Epoch: 67 - Batch: 49088 - Loss: 0.114450 - Time:113.18444871902466\n",
      "Epoch: 67 - Batch: 53184 - Loss: 0.097392 - Time:122.59813714027405\n",
      "Epoch: 67 - Batch: 57280 - Loss: 0.072569 - Time:132.0297713279724\n",
      "Epoch: 67 - Batch: 61376 - Loss: 0.470059 - Time:141.4403805732727\n",
      "Epoch: 67 - Batch: 65472 - Loss: 0.041948 - Time:150.85215950012207\n",
      "Epoch: 67 - Batch: 69568 - Loss: 0.088750 - Time:160.28025460243225\n",
      "Epoch: 67 - Batch: 73664 - Loss: 0.103846 - Time:169.69363951683044\n",
      "Epoch: 67 - Batch: 77760 - Loss: 0.041531 - Time:179.10887169837952\n",
      "Epoch: 67 - Batch: 81856 - Loss: 0.234675 - Time:188.52370309829712\n",
      "Epoch: 67 - Batch: 85952 - Loss: 0.366082 - Time:197.93798828125\n",
      "Epoch: 67 - Batch: 90048 - Loss: 0.106169 - Time:207.3547203540802\n",
      "Epoch: 67 - Batch: 94144 - Loss: 0.092600 - Time:216.77134919166565\n",
      "Epoch: 67 - Batch: 98240 - Loss: 0.056107 - Time:226.18499445915222\n",
      "Epoch: 67 - Batch: 102336 - Loss: 0.036339 - Time:235.61835193634033\n",
      "Epoch: 67 - Batch: 106432 - Loss: 0.203260 - Time:245.03531622886658\n",
      "Epoch: 67 - Batch: 110528 - Loss: 0.045651 - Time:254.4496157169342\n",
      "Epoch: 67 - Batch: 114624 - Loss: 0.186639 - Time:263.8637340068817\n",
      "Epoch: 67 - Batch: 118720 - Loss: 0.177351 - Time:273.2931342124939\n",
      "Epoch: 67 - Batch: 122816 - Loss: 0.052077 - Time:282.704962015152\n",
      "Epoch: 67 - Batch: 126912 - Loss: 0.172006 - Time:292.1184928417206\n",
      "Epoch: 67 - Batch: 131008 - Loss: 0.451576 - Time:301.54779839515686\n",
      "Epoch: 67 - Batch: 135104 - Loss: 0.127010 - Time:310.96020793914795\n",
      "Epoch: 67 - Batch: 139200 - Loss: 0.222687 - Time:320.3719253540039\n",
      "Epoch: 67 - Batch: 143296 - Loss: 0.393017 - Time:329.78490924835205\n",
      "Epoch: 67 - Batch: 147392 - Loss: 0.184386 - Time:339.1952271461487\n",
      "Epoch: 67 - Batch: 151488 - Loss: 0.093631 - Time:348.6051766872406\n",
      "Epoch: 68 - Batch: 4032 - Loss: 0.112388 - Time:9.586806774139404\n",
      "Epoch: 68 - Batch: 8128 - Loss: 0.073304 - Time:19.00473165512085\n",
      "Epoch: 68 - Batch: 12224 - Loss: 0.134942 - Time:28.44747304916382\n",
      "Epoch: 68 - Batch: 16320 - Loss: 0.165763 - Time:37.86360955238342\n",
      "Epoch: 68 - Batch: 20416 - Loss: 0.040720 - Time:47.280922174453735\n",
      "Epoch: 68 - Batch: 24512 - Loss: 0.245733 - Time:56.695279598236084\n",
      "Epoch: 68 - Batch: 28608 - Loss: 0.725048 - Time:66.12638235092163\n",
      "Epoch: 68 - Batch: 32704 - Loss: 0.227512 - Time:75.54260468482971\n",
      "Epoch: 68 - Batch: 36800 - Loss: 0.116052 - Time:84.9595193862915\n",
      "Epoch: 68 - Batch: 40896 - Loss: 0.053648 - Time:94.38978791236877\n",
      "Epoch: 68 - Batch: 44992 - Loss: 0.116856 - Time:103.80494117736816\n",
      "Epoch: 68 - Batch: 49088 - Loss: 0.071322 - Time:113.21837949752808\n",
      "Epoch: 68 - Batch: 53184 - Loss: 0.093316 - Time:122.63160443305969\n",
      "Epoch: 68 - Batch: 57280 - Loss: 0.282006 - Time:132.04771971702576\n",
      "Epoch: 68 - Batch: 61376 - Loss: 0.202707 - Time:141.46038460731506\n",
      "Epoch: 68 - Batch: 65472 - Loss: 0.072582 - Time:150.8744683265686\n",
      "Epoch: 68 - Batch: 69568 - Loss: 0.208590 - Time:160.28955340385437\n",
      "Epoch: 68 - Batch: 73664 - Loss: 0.201597 - Time:169.719500541687\n",
      "Epoch: 68 - Batch: 77760 - Loss: 0.079284 - Time:179.1326994895935\n",
      "Epoch: 68 - Batch: 81856 - Loss: 0.045452 - Time:188.5456895828247\n",
      "Epoch: 68 - Batch: 85952 - Loss: 0.106581 - Time:197.9591941833496\n",
      "Epoch: 68 - Batch: 90048 - Loss: 0.186415 - Time:207.39220809936523\n",
      "Epoch: 68 - Batch: 94144 - Loss: 0.032483 - Time:216.8075988292694\n",
      "Epoch: 68 - Batch: 98240 - Loss: 0.258387 - Time:226.22407817840576\n",
      "Epoch: 68 - Batch: 102336 - Loss: 0.059873 - Time:235.65541410446167\n",
      "Epoch: 68 - Batch: 106432 - Loss: 0.199735 - Time:245.06929111480713\n",
      "Epoch: 68 - Batch: 110528 - Loss: 0.192545 - Time:254.48536443710327\n",
      "Epoch: 68 - Batch: 114624 - Loss: 0.164431 - Time:263.89838767051697\n",
      "Epoch: 68 - Batch: 118720 - Loss: 0.278163 - Time:273.3123199939728\n",
      "Epoch: 68 - Batch: 122816 - Loss: 0.065587 - Time:282.7279577255249\n",
      "Epoch: 68 - Batch: 126912 - Loss: 0.056236 - Time:292.1464788913727\n",
      "Epoch: 68 - Batch: 131008 - Loss: 0.233930 - Time:301.56036138534546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 - Batch: 135104 - Loss: 0.049040 - Time:310.99302077293396\n",
      "Epoch: 68 - Batch: 139200 - Loss: 0.421266 - Time:320.4069781303406\n",
      "Epoch: 68 - Batch: 143296 - Loss: 0.134729 - Time:329.8202950954437\n",
      "Epoch: 68 - Batch: 147392 - Loss: 0.180494 - Time:339.2346205711365\n",
      "Epoch: 68 - Batch: 151488 - Loss: 0.062045 - Time:348.66521525382996\n",
      "Epoch: 69 - Batch: 4032 - Loss: 0.085820 - Time:9.591550588607788\n",
      "Epoch: 69 - Batch: 8128 - Loss: 0.060301 - Time:19.010629653930664\n",
      "Epoch: 69 - Batch: 12224 - Loss: 0.146770 - Time:28.42972207069397\n",
      "Epoch: 69 - Batch: 16320 - Loss: 0.295580 - Time:37.87585663795471\n",
      "Epoch: 69 - Batch: 20416 - Loss: 0.048624 - Time:47.29030656814575\n",
      "Epoch: 69 - Batch: 24512 - Loss: 0.026524 - Time:56.70494508743286\n",
      "Epoch: 69 - Batch: 28608 - Loss: 0.042993 - Time:66.1413004398346\n",
      "Epoch: 69 - Batch: 32704 - Loss: 0.037429 - Time:75.55679416656494\n",
      "Epoch: 69 - Batch: 36800 - Loss: 0.359087 - Time:84.9721302986145\n",
      "Epoch: 69 - Batch: 40896 - Loss: 0.101032 - Time:94.38929200172424\n",
      "Epoch: 69 - Batch: 44992 - Loss: 0.063221 - Time:103.80496215820312\n",
      "Epoch: 69 - Batch: 49088 - Loss: 0.217352 - Time:113.21953129768372\n",
      "Epoch: 69 - Batch: 53184 - Loss: 0.408741 - Time:122.63179779052734\n",
      "Epoch: 69 - Batch: 57280 - Loss: 0.197726 - Time:132.04766368865967\n",
      "Epoch: 69 - Batch: 61376 - Loss: 0.048539 - Time:141.48026156425476\n",
      "Epoch: 69 - Batch: 65472 - Loss: 0.352877 - Time:150.89477229118347\n",
      "Epoch: 69 - Batch: 69568 - Loss: 0.046350 - Time:160.31019496917725\n",
      "Epoch: 69 - Batch: 73664 - Loss: 0.070757 - Time:169.72625875473022\n",
      "Epoch: 69 - Batch: 77760 - Loss: 0.668705 - Time:179.1605567932129\n",
      "Epoch: 69 - Batch: 81856 - Loss: 0.083014 - Time:188.57688927650452\n",
      "Epoch: 69 - Batch: 85952 - Loss: 0.042784 - Time:197.99372696876526\n",
      "Epoch: 69 - Batch: 90048 - Loss: 0.039039 - Time:207.42567133903503\n",
      "Epoch: 69 - Batch: 94144 - Loss: 0.184442 - Time:216.84378480911255\n",
      "Epoch: 69 - Batch: 98240 - Loss: 0.030189 - Time:226.2601478099823\n",
      "Epoch: 69 - Batch: 102336 - Loss: 0.114984 - Time:235.67450761795044\n",
      "Epoch: 69 - Batch: 106432 - Loss: 0.052167 - Time:245.08919429779053\n",
      "Epoch: 69 - Batch: 110528 - Loss: 0.071470 - Time:254.50636911392212\n",
      "Epoch: 69 - Batch: 114624 - Loss: 0.290385 - Time:263.9238405227661\n",
      "Epoch: 69 - Batch: 118720 - Loss: 0.097963 - Time:273.34046626091003\n",
      "Epoch: 69 - Batch: 122816 - Loss: 0.444711 - Time:282.7725145816803\n",
      "Epoch: 69 - Batch: 126912 - Loss: 0.160727 - Time:292.1886057853699\n",
      "Epoch: 69 - Batch: 131008 - Loss: 0.055514 - Time:301.6024672985077\n",
      "Epoch: 69 - Batch: 135104 - Loss: 0.040354 - Time:311.01572275161743\n",
      "Epoch: 69 - Batch: 139200 - Loss: 0.179020 - Time:320.44801449775696\n",
      "Epoch: 69 - Batch: 143296 - Loss: 0.201425 - Time:329.86386585235596\n",
      "Epoch: 69 - Batch: 147392 - Loss: 0.257670 - Time:339.2778654098511\n",
      "Epoch: 69 - Batch: 151488 - Loss: 0.394209 - Time:348.71188282966614\n",
      "Epoch: 70 - Batch: 4032 - Loss: 0.062573 - Time:9.583913326263428\n",
      "Epoch: 70 - Batch: 8128 - Loss: 0.093025 - Time:18.999889135360718\n",
      "Epoch: 70 - Batch: 12224 - Loss: 0.154882 - Time:28.443753719329834\n",
      "Epoch: 70 - Batch: 16320 - Loss: 0.317722 - Time:37.8563551902771\n",
      "Epoch: 70 - Batch: 20416 - Loss: 0.107323 - Time:47.26973819732666\n",
      "Epoch: 70 - Batch: 24512 - Loss: 0.087682 - Time:56.683539152145386\n",
      "Epoch: 70 - Batch: 28608 - Loss: 0.068441 - Time:66.09639859199524\n",
      "Epoch: 70 - Batch: 32704 - Loss: 0.064177 - Time:75.50985312461853\n",
      "Epoch: 70 - Batch: 36800 - Loss: 0.265472 - Time:84.92224669456482\n",
      "Epoch: 70 - Batch: 40896 - Loss: 0.199418 - Time:94.33529376983643\n",
      "Epoch: 70 - Batch: 44992 - Loss: 0.071647 - Time:103.76609992980957\n",
      "Epoch: 70 - Batch: 49088 - Loss: 0.051690 - Time:113.17966794967651\n",
      "Epoch: 70 - Batch: 53184 - Loss: 0.155322 - Time:122.59246826171875\n",
      "Epoch: 70 - Batch: 57280 - Loss: 0.154094 - Time:132.00433325767517\n",
      "Epoch: 70 - Batch: 61376 - Loss: 0.074047 - Time:141.43441557884216\n",
      "Epoch: 70 - Batch: 65472 - Loss: 0.029793 - Time:150.84470748901367\n",
      "Epoch: 70 - Batch: 69568 - Loss: 0.156475 - Time:160.2559585571289\n",
      "Epoch: 70 - Batch: 73664 - Loss: 0.190746 - Time:169.68451023101807\n",
      "Epoch: 70 - Batch: 77760 - Loss: 0.070608 - Time:179.09583735466003\n",
      "Epoch: 70 - Batch: 81856 - Loss: 0.386151 - Time:188.50867700576782\n",
      "Epoch: 70 - Batch: 85952 - Loss: 0.028557 - Time:197.92028760910034\n",
      "Epoch: 70 - Batch: 90048 - Loss: 0.270326 - Time:207.33420300483704\n",
      "Epoch: 70 - Batch: 94144 - Loss: 0.088151 - Time:216.74885845184326\n",
      "Epoch: 70 - Batch: 98240 - Loss: 0.061021 - Time:226.16026091575623\n",
      "Epoch: 70 - Batch: 102336 - Loss: 0.109708 - Time:235.57138562202454\n",
      "Epoch: 70 - Batch: 106432 - Loss: 0.067128 - Time:245.00441241264343\n",
      "Epoch: 70 - Batch: 110528 - Loss: 0.109453 - Time:254.41708040237427\n",
      "Epoch: 70 - Batch: 114624 - Loss: 0.200176 - Time:263.8293035030365\n",
      "Epoch: 70 - Batch: 118720 - Loss: 0.252855 - Time:273.2429714202881\n",
      "Epoch: 70 - Batch: 122816 - Loss: 0.044212 - Time:282.6552565097809\n",
      "Epoch: 70 - Batch: 126912 - Loss: 0.282226 - Time:292.08664727211\n",
      "Epoch: 70 - Batch: 131008 - Loss: 0.392429 - Time:301.49975419044495\n",
      "Epoch: 70 - Batch: 135104 - Loss: 0.260353 - Time:310.91299319267273\n",
      "Epoch: 70 - Batch: 139200 - Loss: 0.242837 - Time:320.3431363105774\n",
      "Epoch: 70 - Batch: 143296 - Loss: 0.201737 - Time:329.75431513786316\n",
      "Epoch: 70 - Batch: 147392 - Loss: 0.034423 - Time:339.16825008392334\n",
      "Epoch: 70 - Batch: 151488 - Loss: 0.188114 - Time:348.58421063423157\n",
      "Epoch: 71 - Batch: 4032 - Loss: 0.130103 - Time:9.574243307113647\n",
      "Epoch: 71 - Batch: 8128 - Loss: 0.021879 - Time:19.02081274986267\n",
      "Epoch: 71 - Batch: 12224 - Loss: 0.080405 - Time:28.433318376541138\n",
      "Epoch: 71 - Batch: 16320 - Loss: 0.039828 - Time:37.84663367271423\n",
      "Epoch: 71 - Batch: 20416 - Loss: 0.032736 - Time:47.2597451210022\n",
      "Epoch: 71 - Batch: 24512 - Loss: 0.105029 - Time:56.67131328582764\n",
      "Epoch: 71 - Batch: 28608 - Loss: 0.277239 - Time:66.08435201644897\n",
      "Epoch: 71 - Batch: 32704 - Loss: 0.114208 - Time:75.49844884872437\n",
      "Epoch: 71 - Batch: 36800 - Loss: 0.200158 - Time:84.91229033470154\n",
      "Epoch: 71 - Batch: 40896 - Loss: 0.190381 - Time:94.3425841331482\n",
      "Epoch: 71 - Batch: 44992 - Loss: 0.047375 - Time:103.75619506835938\n",
      "Epoch: 71 - Batch: 49088 - Loss: 0.031997 - Time:113.16943168640137\n",
      "Epoch: 71 - Batch: 53184 - Loss: 0.041461 - Time:122.58125233650208\n",
      "Epoch: 71 - Batch: 57280 - Loss: 0.038375 - Time:132.01084327697754\n",
      "Epoch: 71 - Batch: 61376 - Loss: 0.245329 - Time:141.4255313873291\n",
      "Epoch: 71 - Batch: 65472 - Loss: 0.046353 - Time:150.8393177986145\n",
      "Epoch: 71 - Batch: 69568 - Loss: 0.148270 - Time:160.26987051963806\n",
      "Epoch: 71 - Batch: 73664 - Loss: 0.237802 - Time:169.68516421318054\n",
      "Epoch: 71 - Batch: 77760 - Loss: 0.089336 - Time:179.09858965873718\n",
      "Epoch: 71 - Batch: 81856 - Loss: 0.212050 - Time:188.51179599761963\n",
      "Epoch: 71 - Batch: 85952 - Loss: 0.338831 - Time:197.92442655563354\n",
      "Epoch: 71 - Batch: 90048 - Loss: 0.025829 - Time:207.33589267730713\n",
      "Epoch: 71 - Batch: 94144 - Loss: 0.062930 - Time:216.7489128112793\n",
      "Epoch: 71 - Batch: 98240 - Loss: 0.081421 - Time:226.1626100540161\n",
      "Epoch: 71 - Batch: 102336 - Loss: 0.484947 - Time:235.59168982505798\n",
      "Epoch: 71 - Batch: 106432 - Loss: 0.062953 - Time:245.0018653869629\n",
      "Epoch: 71 - Batch: 110528 - Loss: 0.129635 - Time:254.41305947303772\n",
      "Epoch: 71 - Batch: 114624 - Loss: 0.045151 - Time:263.8264937400818\n",
      "Epoch: 71 - Batch: 118720 - Loss: 0.355150 - Time:273.2554888725281\n",
      "Epoch: 71 - Batch: 122816 - Loss: 0.302052 - Time:282.66830372810364\n",
      "Epoch: 71 - Batch: 126912 - Loss: 0.222817 - Time:292.081622838974\n",
      "Epoch: 71 - Batch: 131008 - Loss: 0.238268 - Time:301.51231241226196\n",
      "Epoch: 71 - Batch: 135104 - Loss: 0.170357 - Time:310.9245116710663\n",
      "Epoch: 71 - Batch: 139200 - Loss: 0.160875 - Time:320.3365857601166\n",
      "Epoch: 71 - Batch: 143296 - Loss: 0.103931 - Time:329.7510676383972\n",
      "Epoch: 71 - Batch: 147392 - Loss: 0.376968 - Time:339.1646800041199\n",
      "Epoch: 71 - Batch: 151488 - Loss: 0.186647 - Time:348.57929968833923\n",
      "Epoch: 72 - Batch: 4032 - Loss: 0.036086 - Time:9.584760189056396\n",
      "Epoch: 72 - Batch: 8128 - Loss: 0.171540 - Time:19.00234627723694\n",
      "Epoch: 72 - Batch: 12224 - Loss: 0.140961 - Time:28.446337938308716\n",
      "Epoch: 72 - Batch: 16320 - Loss: 0.406585 - Time:37.860793113708496\n",
      "Epoch: 72 - Batch: 20416 - Loss: 0.080569 - Time:47.273239850997925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 - Batch: 24512 - Loss: 0.064927 - Time:56.68658423423767\n",
      "Epoch: 72 - Batch: 28608 - Loss: 0.084064 - Time:66.1188542842865\n",
      "Epoch: 72 - Batch: 32704 - Loss: 0.036856 - Time:75.53292083740234\n",
      "Epoch: 72 - Batch: 36800 - Loss: 0.175811 - Time:84.94680404663086\n",
      "Epoch: 72 - Batch: 40896 - Loss: 0.114806 - Time:94.37868022918701\n",
      "Epoch: 72 - Batch: 44992 - Loss: 0.024342 - Time:103.79214072227478\n",
      "Epoch: 72 - Batch: 49088 - Loss: 0.100106 - Time:113.20773410797119\n",
      "Epoch: 72 - Batch: 53184 - Loss: 0.060124 - Time:122.62151646614075\n",
      "Epoch: 72 - Batch: 57280 - Loss: 0.661484 - Time:132.03625202178955\n",
      "Epoch: 72 - Batch: 61376 - Loss: 0.096126 - Time:141.45174193382263\n",
      "Epoch: 72 - Batch: 65472 - Loss: 0.039949 - Time:150.86902117729187\n",
      "Epoch: 72 - Batch: 69568 - Loss: 0.281498 - Time:160.2827787399292\n",
      "Epoch: 72 - Batch: 73664 - Loss: 0.108103 - Time:169.7127275466919\n",
      "Epoch: 72 - Batch: 77760 - Loss: 0.313924 - Time:179.12532877922058\n",
      "Epoch: 72 - Batch: 81856 - Loss: 0.109807 - Time:188.54033184051514\n",
      "Epoch: 72 - Batch: 85952 - Loss: 0.062799 - Time:197.9557375907898\n",
      "Epoch: 72 - Batch: 90048 - Loss: 0.125251 - Time:207.38920760154724\n",
      "Epoch: 72 - Batch: 94144 - Loss: 0.083152 - Time:216.80225682258606\n",
      "Epoch: 72 - Batch: 98240 - Loss: 0.119101 - Time:226.2152066230774\n",
      "Epoch: 72 - Batch: 102336 - Loss: 0.138771 - Time:235.64746236801147\n",
      "Epoch: 72 - Batch: 106432 - Loss: 0.222712 - Time:245.05980277061462\n",
      "Epoch: 72 - Batch: 110528 - Loss: 0.057172 - Time:254.47367978096008\n",
      "Epoch: 72 - Batch: 114624 - Loss: 0.242596 - Time:263.88793420791626\n",
      "Epoch: 72 - Batch: 118720 - Loss: 0.063542 - Time:273.30331325531006\n",
      "Epoch: 72 - Batch: 122816 - Loss: 0.117117 - Time:282.7201716899872\n",
      "Epoch: 72 - Batch: 126912 - Loss: 0.084727 - Time:292.13498997688293\n",
      "Epoch: 72 - Batch: 131008 - Loss: 0.155929 - Time:301.5500383377075\n",
      "Epoch: 72 - Batch: 135104 - Loss: 0.238149 - Time:310.9825415611267\n",
      "Epoch: 72 - Batch: 139200 - Loss: 0.156202 - Time:320.4000463485718\n",
      "Epoch: 72 - Batch: 143296 - Loss: 0.316482 - Time:329.81677651405334\n",
      "Epoch: 72 - Batch: 147392 - Loss: 0.143856 - Time:339.23376417160034\n",
      "Epoch: 72 - Batch: 151488 - Loss: 0.073787 - Time:348.6664276123047\n",
      "Epoch: 73 - Batch: 4032 - Loss: 0.182630 - Time:9.579753160476685\n",
      "Epoch: 73 - Batch: 8128 - Loss: 0.040807 - Time:18.999577283859253\n",
      "Epoch: 73 - Batch: 12224 - Loss: 0.073684 - Time:28.417206525802612\n",
      "Epoch: 73 - Batch: 16320 - Loss: 0.096661 - Time:37.862730264663696\n",
      "Epoch: 73 - Batch: 20416 - Loss: 0.167492 - Time:47.28100848197937\n",
      "Epoch: 73 - Batch: 24512 - Loss: 0.130694 - Time:56.69707894325256\n",
      "Epoch: 73 - Batch: 28608 - Loss: 0.037556 - Time:66.13091588020325\n",
      "Epoch: 73 - Batch: 32704 - Loss: 0.086484 - Time:75.54580593109131\n",
      "Epoch: 73 - Batch: 36800 - Loss: 0.148303 - Time:84.96221399307251\n",
      "Epoch: 73 - Batch: 40896 - Loss: 0.088267 - Time:94.37979459762573\n",
      "Epoch: 73 - Batch: 44992 - Loss: 0.183315 - Time:103.79610276222229\n",
      "Epoch: 73 - Batch: 49088 - Loss: 0.037655 - Time:113.21201062202454\n",
      "Epoch: 73 - Batch: 53184 - Loss: 0.493612 - Time:122.62835955619812\n",
      "Epoch: 73 - Batch: 57280 - Loss: 0.080427 - Time:132.0434250831604\n",
      "Epoch: 73 - Batch: 61376 - Loss: 0.037549 - Time:141.4771330356598\n",
      "Epoch: 73 - Batch: 65472 - Loss: 0.080758 - Time:150.8917396068573\n",
      "Epoch: 73 - Batch: 69568 - Loss: 0.138569 - Time:160.3058271408081\n",
      "Epoch: 73 - Batch: 73664 - Loss: 0.045309 - Time:169.7210705280304\n",
      "Epoch: 73 - Batch: 77760 - Loss: 0.066944 - Time:179.15156745910645\n",
      "Epoch: 73 - Batch: 81856 - Loss: 0.144243 - Time:188.56656408309937\n",
      "Epoch: 73 - Batch: 85952 - Loss: 0.039937 - Time:197.9795801639557\n",
      "Epoch: 73 - Batch: 90048 - Loss: 0.054616 - Time:207.40858483314514\n",
      "Epoch: 73 - Batch: 94144 - Loss: 0.044129 - Time:216.82223773002625\n",
      "Epoch: 73 - Batch: 98240 - Loss: 0.204042 - Time:226.23649096488953\n",
      "Epoch: 73 - Batch: 102336 - Loss: 0.096828 - Time:235.6497311592102\n",
      "Epoch: 73 - Batch: 106432 - Loss: 0.205437 - Time:245.0647749900818\n",
      "Epoch: 73 - Batch: 110528 - Loss: 0.209371 - Time:254.47756123542786\n",
      "Epoch: 73 - Batch: 114624 - Loss: 0.734211 - Time:263.8911929130554\n",
      "Epoch: 73 - Batch: 118720 - Loss: 0.123058 - Time:273.306263923645\n",
      "Epoch: 73 - Batch: 122816 - Loss: 0.193310 - Time:282.73517417907715\n",
      "Epoch: 73 - Batch: 126912 - Loss: 0.398836 - Time:292.1466257572174\n",
      "Epoch: 73 - Batch: 131008 - Loss: 0.168098 - Time:301.5589029788971\n",
      "Epoch: 73 - Batch: 135104 - Loss: 0.316160 - Time:310.97144842147827\n",
      "Epoch: 73 - Batch: 139200 - Loss: 0.123822 - Time:320.40132117271423\n",
      "Epoch: 73 - Batch: 143296 - Loss: 0.064070 - Time:329.8155117034912\n",
      "Epoch: 73 - Batch: 147392 - Loss: 0.177161 - Time:339.22754645347595\n",
      "Epoch: 73 - Batch: 151488 - Loss: 0.107176 - Time:348.6571900844574\n",
      "Epoch: 74 - Batch: 4032 - Loss: 0.093229 - Time:9.583386659622192\n",
      "Epoch: 74 - Batch: 8128 - Loss: 0.459890 - Time:18.997172355651855\n",
      "Epoch: 74 - Batch: 12224 - Loss: 0.702273 - Time:28.43795609474182\n",
      "Epoch: 74 - Batch: 16320 - Loss: 0.112788 - Time:37.85368347167969\n",
      "Epoch: 74 - Batch: 20416 - Loss: 0.043547 - Time:47.26905345916748\n",
      "Epoch: 74 - Batch: 24512 - Loss: 0.068491 - Time:56.684061765670776\n",
      "Epoch: 74 - Batch: 28608 - Loss: 0.680787 - Time:66.09827375411987\n",
      "Epoch: 74 - Batch: 32704 - Loss: 0.179511 - Time:75.51356720924377\n",
      "Epoch: 74 - Batch: 36800 - Loss: 0.024446 - Time:84.92584705352783\n",
      "Epoch: 74 - Batch: 40896 - Loss: 0.072446 - Time:94.3385636806488\n",
      "Epoch: 74 - Batch: 44992 - Loss: 0.170903 - Time:103.76938486099243\n",
      "Epoch: 74 - Batch: 49088 - Loss: 0.125435 - Time:113.18279528617859\n",
      "Epoch: 74 - Batch: 53184 - Loss: 0.046045 - Time:122.5970892906189\n",
      "Epoch: 74 - Batch: 57280 - Loss: 0.120830 - Time:132.0116834640503\n",
      "Epoch: 74 - Batch: 61376 - Loss: 0.062900 - Time:141.44202542304993\n",
      "Epoch: 74 - Batch: 65472 - Loss: 0.048819 - Time:150.85499572753906\n",
      "Epoch: 74 - Batch: 69568 - Loss: 0.130498 - Time:160.26930141448975\n",
      "Epoch: 74 - Batch: 73664 - Loss: 0.403316 - Time:169.6984350681305\n",
      "Epoch: 74 - Batch: 77760 - Loss: 0.023554 - Time:179.10998439788818\n",
      "Epoch: 74 - Batch: 81856 - Loss: 0.247651 - Time:188.52213549613953\n",
      "Epoch: 74 - Batch: 85952 - Loss: 0.106537 - Time:197.93567299842834\n",
      "Epoch: 74 - Batch: 90048 - Loss: 0.062713 - Time:207.3506977558136\n",
      "Epoch: 74 - Batch: 94144 - Loss: 0.116545 - Time:216.76496410369873\n",
      "Epoch: 74 - Batch: 98240 - Loss: 0.125285 - Time:226.17769265174866\n",
      "Epoch: 74 - Batch: 102336 - Loss: 0.165853 - Time:235.58989453315735\n",
      "Epoch: 74 - Batch: 106432 - Loss: 0.032884 - Time:245.01941347122192\n",
      "Epoch: 74 - Batch: 110528 - Loss: 0.218343 - Time:254.43349480628967\n",
      "Epoch: 74 - Batch: 114624 - Loss: 0.200725 - Time:263.84787940979004\n",
      "Epoch: 74 - Batch: 118720 - Loss: 0.398332 - Time:273.2623014450073\n",
      "Epoch: 74 - Batch: 122816 - Loss: 0.021228 - Time:282.6763725280762\n",
      "Epoch: 74 - Batch: 126912 - Loss: 0.036115 - Time:292.1090624332428\n",
      "Epoch: 74 - Batch: 131008 - Loss: 0.074421 - Time:301.5235381126404\n",
      "Epoch: 74 - Batch: 135104 - Loss: 0.226426 - Time:310.9373757839203\n",
      "Epoch: 74 - Batch: 139200 - Loss: 0.146394 - Time:320.3684296607971\n",
      "Epoch: 74 - Batch: 143296 - Loss: 0.037270 - Time:329.78077507019043\n",
      "Epoch: 74 - Batch: 147392 - Loss: 0.084040 - Time:339.1939158439636\n",
      "Epoch: 74 - Batch: 151488 - Loss: 0.182666 - Time:348.60667300224304\n",
      "Epoch: 75 - Batch: 4032 - Loss: 0.148722 - Time:9.581196069717407\n",
      "Epoch: 75 - Batch: 8128 - Loss: 0.088377 - Time:19.02762484550476\n",
      "Epoch: 75 - Batch: 12224 - Loss: 0.140148 - Time:28.443448781967163\n",
      "Epoch: 75 - Batch: 16320 - Loss: 0.042312 - Time:37.85810923576355\n",
      "Epoch: 75 - Batch: 20416 - Loss: 0.039027 - Time:47.27543830871582\n",
      "Epoch: 75 - Batch: 24512 - Loss: 0.101605 - Time:56.68891644477844\n",
      "Epoch: 75 - Batch: 28608 - Loss: 0.070600 - Time:66.10462641716003\n",
      "Epoch: 75 - Batch: 32704 - Loss: 0.155879 - Time:75.5210030078888\n",
      "Epoch: 75 - Batch: 36800 - Loss: 0.029007 - Time:84.93951940536499\n",
      "Epoch: 75 - Batch: 40896 - Loss: 0.073059 - Time:94.36994624137878\n",
      "Epoch: 75 - Batch: 44992 - Loss: 0.052452 - Time:103.78535509109497\n",
      "Epoch: 75 - Batch: 49088 - Loss: 0.169352 - Time:113.20143628120422\n",
      "Epoch: 75 - Batch: 53184 - Loss: 0.022340 - Time:122.61703324317932\n",
      "Epoch: 75 - Batch: 57280 - Loss: 0.038601 - Time:132.05026030540466\n",
      "Epoch: 75 - Batch: 61376 - Loss: 0.526927 - Time:141.46465396881104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 - Batch: 65472 - Loss: 0.082035 - Time:150.87844586372375\n",
      "Epoch: 75 - Batch: 69568 - Loss: 0.037202 - Time:160.3129358291626\n",
      "Epoch: 75 - Batch: 73664 - Loss: 0.041573 - Time:169.7268042564392\n",
      "Epoch: 75 - Batch: 77760 - Loss: 0.023173 - Time:179.14001417160034\n",
      "Epoch: 75 - Batch: 81856 - Loss: 0.043204 - Time:188.55514073371887\n",
      "Epoch: 75 - Batch: 85952 - Loss: 0.055940 - Time:197.96933794021606\n",
      "Epoch: 75 - Batch: 90048 - Loss: 0.137055 - Time:207.38194012641907\n",
      "Epoch: 75 - Batch: 94144 - Loss: 0.067644 - Time:216.79725456237793\n",
      "Epoch: 75 - Batch: 98240 - Loss: 0.055455 - Time:226.21269178390503\n",
      "Epoch: 75 - Batch: 102336 - Loss: 0.134661 - Time:235.6446077823639\n",
      "Epoch: 75 - Batch: 106432 - Loss: 0.056827 - Time:245.05880165100098\n",
      "Epoch: 75 - Batch: 110528 - Loss: 0.102307 - Time:254.47355484962463\n",
      "Epoch: 75 - Batch: 114624 - Loss: 0.090310 - Time:263.8879246711731\n",
      "Epoch: 75 - Batch: 118720 - Loss: 0.231174 - Time:273.3209161758423\n",
      "Epoch: 75 - Batch: 122816 - Loss: 0.373753 - Time:282.73654985427856\n",
      "Epoch: 75 - Batch: 126912 - Loss: 0.082019 - Time:292.15345764160156\n",
      "Epoch: 75 - Batch: 131008 - Loss: 0.113049 - Time:301.5867838859558\n",
      "Epoch: 75 - Batch: 135104 - Loss: 0.043325 - Time:311.0031650066376\n",
      "Epoch: 75 - Batch: 139200 - Loss: 0.327163 - Time:320.41877484321594\n",
      "Epoch: 75 - Batch: 143296 - Loss: 0.227495 - Time:329.83413672447205\n",
      "Epoch: 75 - Batch: 147392 - Loss: 0.051065 - Time:339.24988532066345\n",
      "Epoch: 75 - Batch: 151488 - Loss: 0.342131 - Time:348.66358947753906\n",
      "Epoch: 76 - Batch: 4032 - Loss: 0.043375 - Time:9.582748889923096\n",
      "Epoch: 76 - Batch: 8128 - Loss: 0.141411 - Time:19.001920223236084\n",
      "Epoch: 76 - Batch: 12224 - Loss: 0.048655 - Time:28.44682502746582\n",
      "Epoch: 76 - Batch: 16320 - Loss: 0.212937 - Time:37.86234951019287\n",
      "Epoch: 76 - Batch: 20416 - Loss: 0.077734 - Time:47.278942584991455\n",
      "Epoch: 76 - Batch: 24512 - Loss: 0.465331 - Time:56.69461536407471\n",
      "Epoch: 76 - Batch: 28608 - Loss: 0.254835 - Time:66.1280825138092\n",
      "Epoch: 76 - Batch: 32704 - Loss: 0.627147 - Time:75.54445934295654\n",
      "Epoch: 76 - Batch: 36800 - Loss: 0.118868 - Time:84.96106147766113\n",
      "Epoch: 76 - Batch: 40896 - Loss: 0.114104 - Time:94.392409324646\n",
      "Epoch: 76 - Batch: 44992 - Loss: 0.067482 - Time:103.80675053596497\n",
      "Epoch: 76 - Batch: 49088 - Loss: 0.188783 - Time:113.22087335586548\n",
      "Epoch: 76 - Batch: 53184 - Loss: 0.152967 - Time:122.6349446773529\n",
      "Epoch: 76 - Batch: 57280 - Loss: 0.029851 - Time:132.0511908531189\n",
      "Epoch: 76 - Batch: 61376 - Loss: 0.059069 - Time:141.46515941619873\n",
      "Epoch: 76 - Batch: 65472 - Loss: 0.065768 - Time:150.8799753189087\n",
      "Epoch: 76 - Batch: 69568 - Loss: 0.090262 - Time:160.29692721366882\n",
      "Epoch: 76 - Batch: 73664 - Loss: 0.035001 - Time:169.7281506061554\n",
      "Epoch: 76 - Batch: 77760 - Loss: 0.218769 - Time:179.142986536026\n",
      "Epoch: 76 - Batch: 81856 - Loss: 0.063905 - Time:188.5568151473999\n",
      "Epoch: 76 - Batch: 85952 - Loss: 0.064183 - Time:197.97251844406128\n",
      "Epoch: 76 - Batch: 90048 - Loss: 0.081576 - Time:207.40310335159302\n",
      "Epoch: 76 - Batch: 94144 - Loss: 0.110219 - Time:216.81895422935486\n",
      "Epoch: 76 - Batch: 98240 - Loss: 0.073732 - Time:226.23655581474304\n",
      "Epoch: 76 - Batch: 102336 - Loss: 0.031035 - Time:235.66717886924744\n",
      "Epoch: 76 - Batch: 106432 - Loss: 0.017489 - Time:245.08242297172546\n",
      "Epoch: 76 - Batch: 110528 - Loss: 0.339946 - Time:254.4983651638031\n",
      "Epoch: 76 - Batch: 114624 - Loss: 0.153070 - Time:263.91312527656555\n",
      "Epoch: 76 - Batch: 118720 - Loss: 0.477493 - Time:273.3282389640808\n",
      "Epoch: 76 - Batch: 122816 - Loss: 0.060401 - Time:282.7435476779938\n",
      "Epoch: 76 - Batch: 126912 - Loss: 0.057091 - Time:292.16048979759216\n",
      "Epoch: 76 - Batch: 131008 - Loss: 0.027591 - Time:301.57396388053894\n",
      "Epoch: 76 - Batch: 135104 - Loss: 0.238702 - Time:311.00417590141296\n",
      "Epoch: 76 - Batch: 139200 - Loss: 0.169145 - Time:320.4180908203125\n",
      "Epoch: 76 - Batch: 143296 - Loss: 0.046676 - Time:329.83071184158325\n",
      "Epoch: 76 - Batch: 147392 - Loss: 0.050641 - Time:339.2432703971863\n",
      "Epoch: 76 - Batch: 151488 - Loss: 0.028905 - Time:348.67591762542725\n",
      "Epoch: 77 - Batch: 4032 - Loss: 0.128127 - Time:9.577946424484253\n",
      "Epoch: 77 - Batch: 8128 - Loss: 0.037128 - Time:18.993502855300903\n",
      "Epoch: 77 - Batch: 12224 - Loss: 0.074338 - Time:28.437740802764893\n",
      "Epoch: 77 - Batch: 16320 - Loss: 0.101231 - Time:37.85205960273743\n",
      "Epoch: 77 - Batch: 20416 - Loss: 0.034829 - Time:47.266205072402954\n",
      "Epoch: 77 - Batch: 24512 - Loss: 0.173212 - Time:56.68252873420715\n",
      "Epoch: 77 - Batch: 28608 - Loss: 0.038134 - Time:66.09670329093933\n",
      "Epoch: 77 - Batch: 32704 - Loss: 0.033518 - Time:75.51032996177673\n",
      "Epoch: 77 - Batch: 36800 - Loss: 0.044332 - Time:84.92471623420715\n",
      "Epoch: 77 - Batch: 40896 - Loss: 0.165078 - Time:94.33927035331726\n",
      "Epoch: 77 - Batch: 44992 - Loss: 0.061510 - Time:103.77097773551941\n",
      "Epoch: 77 - Batch: 49088 - Loss: 0.040904 - Time:113.18481826782227\n",
      "Epoch: 77 - Batch: 53184 - Loss: 0.488271 - Time:122.60037016868591\n",
      "Epoch: 77 - Batch: 57280 - Loss: 0.064094 - Time:132.01279211044312\n",
      "Epoch: 77 - Batch: 61376 - Loss: 0.260014 - Time:141.44400715827942\n",
      "Epoch: 77 - Batch: 65472 - Loss: 0.045891 - Time:150.85629630088806\n",
      "Epoch: 77 - Batch: 69568 - Loss: 0.389558 - Time:160.26840376853943\n",
      "Epoch: 77 - Batch: 73664 - Loss: 0.173180 - Time:169.70086908340454\n",
      "Epoch: 77 - Batch: 77760 - Loss: 0.026284 - Time:179.11556267738342\n",
      "Epoch: 77 - Batch: 81856 - Loss: 0.043605 - Time:188.53143739700317\n",
      "Epoch: 77 - Batch: 85952 - Loss: 0.102890 - Time:197.94786429405212\n",
      "Epoch: 77 - Batch: 90048 - Loss: 0.131486 - Time:207.36359238624573\n",
      "Epoch: 77 - Batch: 94144 - Loss: 0.047205 - Time:216.78087091445923\n",
      "Epoch: 77 - Batch: 98240 - Loss: 0.130541 - Time:226.19678449630737\n",
      "Epoch: 77 - Batch: 102336 - Loss: 0.101232 - Time:235.60992789268494\n",
      "Epoch: 77 - Batch: 106432 - Loss: 0.124784 - Time:245.03965711593628\n",
      "Epoch: 77 - Batch: 110528 - Loss: 0.460141 - Time:254.45443296432495\n",
      "Epoch: 77 - Batch: 114624 - Loss: 0.240137 - Time:263.8671841621399\n",
      "Epoch: 77 - Batch: 118720 - Loss: 0.073173 - Time:273.2794277667999\n",
      "Epoch: 77 - Batch: 122816 - Loss: 0.189710 - Time:282.71144914627075\n",
      "Epoch: 77 - Batch: 126912 - Loss: 0.258852 - Time:292.1261074542999\n",
      "Epoch: 77 - Batch: 131008 - Loss: 0.250375 - Time:301.538476228714\n",
      "Epoch: 77 - Batch: 135104 - Loss: 0.427290 - Time:310.9700736999512\n",
      "Epoch: 77 - Batch: 139200 - Loss: 0.090080 - Time:320.3843820095062\n",
      "Epoch: 77 - Batch: 143296 - Loss: 0.264011 - Time:329.7984929084778\n",
      "Epoch: 77 - Batch: 147392 - Loss: 0.087768 - Time:339.2130997180939\n",
      "Epoch: 77 - Batch: 151488 - Loss: 0.085497 - Time:348.6294467449188\n",
      "Epoch: 78 - Batch: 4032 - Loss: 0.055644 - Time:9.578480243682861\n",
      "Epoch: 78 - Batch: 8128 - Loss: 0.042643 - Time:19.02246880531311\n",
      "Epoch: 78 - Batch: 12224 - Loss: 0.062959 - Time:28.43822193145752\n",
      "Epoch: 78 - Batch: 16320 - Loss: 0.084930 - Time:37.8516948223114\n",
      "Epoch: 78 - Batch: 20416 - Loss: 0.357772 - Time:47.26575970649719\n",
      "Epoch: 78 - Batch: 24512 - Loss: 0.085054 - Time:56.68320655822754\n",
      "Epoch: 78 - Batch: 28608 - Loss: 0.164283 - Time:66.09707999229431\n",
      "Epoch: 78 - Batch: 32704 - Loss: 0.023650 - Time:75.51124596595764\n",
      "Epoch: 78 - Batch: 36800 - Loss: 0.038098 - Time:84.9262421131134\n",
      "Epoch: 78 - Batch: 40896 - Loss: 0.178231 - Time:94.35833358764648\n",
      "Epoch: 78 - Batch: 44992 - Loss: 0.200779 - Time:103.77315902709961\n",
      "Epoch: 78 - Batch: 49088 - Loss: 0.150442 - Time:113.18657493591309\n",
      "Epoch: 78 - Batch: 53184 - Loss: 0.550153 - Time:122.59956622123718\n",
      "Epoch: 78 - Batch: 57280 - Loss: 0.074561 - Time:132.0275092124939\n",
      "Epoch: 78 - Batch: 61376 - Loss: 0.043228 - Time:141.44021821022034\n",
      "Epoch: 78 - Batch: 65472 - Loss: 0.060479 - Time:150.85219979286194\n",
      "Epoch: 78 - Batch: 69568 - Loss: 0.036745 - Time:160.28196096420288\n",
      "Epoch: 78 - Batch: 73664 - Loss: 0.240027 - Time:169.69455528259277\n",
      "Epoch: 78 - Batch: 77760 - Loss: 0.555032 - Time:179.10656714439392\n",
      "Epoch: 78 - Batch: 81856 - Loss: 0.179843 - Time:188.51783061027527\n",
      "Epoch: 78 - Batch: 85952 - Loss: 0.243927 - Time:197.9307074546814\n",
      "Epoch: 78 - Batch: 90048 - Loss: 0.027488 - Time:207.34544706344604\n",
      "Epoch: 78 - Batch: 94144 - Loss: 0.067153 - Time:216.76091384887695\n",
      "Epoch: 78 - Batch: 98240 - Loss: 0.206058 - Time:226.17434811592102\n",
      "Epoch: 78 - Batch: 102336 - Loss: 0.102373 - Time:235.6075541973114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 - Batch: 106432 - Loss: 0.103119 - Time:245.02355337142944\n",
      "Epoch: 78 - Batch: 110528 - Loss: 0.032878 - Time:254.4369089603424\n",
      "Epoch: 78 - Batch: 114624 - Loss: 0.068081 - Time:263.8488233089447\n",
      "Epoch: 78 - Batch: 118720 - Loss: 0.125241 - Time:273.2771213054657\n",
      "Epoch: 78 - Batch: 122816 - Loss: 0.216811 - Time:282.6905345916748\n",
      "Epoch: 78 - Batch: 126912 - Loss: 0.122756 - Time:292.1041204929352\n",
      "Epoch: 78 - Batch: 131008 - Loss: 0.087070 - Time:301.5353329181671\n",
      "Epoch: 78 - Batch: 135104 - Loss: 0.067123 - Time:310.94676065444946\n",
      "Epoch: 78 - Batch: 139200 - Loss: 0.159104 - Time:320.36020064353943\n",
      "Epoch: 78 - Batch: 143296 - Loss: 0.064731 - Time:329.77834820747375\n",
      "Epoch: 78 - Batch: 147392 - Loss: 0.132735 - Time:339.188925743103\n",
      "Epoch: 78 - Batch: 151488 - Loss: 0.061233 - Time:348.5993638038635\n",
      "Epoch: 79 - Batch: 4032 - Loss: 0.236258 - Time:9.607343435287476\n",
      "Epoch: 79 - Batch: 8128 - Loss: 0.346711 - Time:19.024350881576538\n",
      "Epoch: 79 - Batch: 12224 - Loss: 0.065608 - Time:28.441759824752808\n",
      "Epoch: 79 - Batch: 16320 - Loss: 0.081236 - Time:37.8596727848053\n",
      "Epoch: 79 - Batch: 20416 - Loss: 0.041313 - Time:47.27673959732056\n",
      "Epoch: 79 - Batch: 24512 - Loss: 0.105448 - Time:56.69114327430725\n",
      "Epoch: 79 - Batch: 28608 - Loss: 0.069643 - Time:66.107177734375\n",
      "Epoch: 79 - Batch: 32704 - Loss: 0.114953 - Time:75.51970338821411\n",
      "Epoch: 79 - Batch: 36800 - Loss: 0.212923 - Time:84.9510657787323\n",
      "Epoch: 79 - Batch: 40896 - Loss: 0.055983 - Time:94.36531376838684\n",
      "Epoch: 79 - Batch: 44992 - Loss: 0.172820 - Time:103.77874445915222\n",
      "Epoch: 79 - Batch: 49088 - Loss: 0.065823 - Time:113.1912100315094\n",
      "Epoch: 79 - Batch: 53184 - Loss: 0.044310 - Time:122.60174012184143\n",
      "Epoch: 79 - Batch: 57280 - Loss: 0.215916 - Time:132.03347039222717\n",
      "Epoch: 79 - Batch: 61376 - Loss: 0.200534 - Time:141.44570183753967\n",
      "Epoch: 79 - Batch: 65472 - Loss: 0.044351 - Time:150.85980701446533\n",
      "Epoch: 79 - Batch: 69568 - Loss: 0.056152 - Time:160.29428720474243\n",
      "Epoch: 79 - Batch: 73664 - Loss: 0.123395 - Time:169.70945405960083\n",
      "Epoch: 79 - Batch: 77760 - Loss: 0.208331 - Time:179.1243760585785\n",
      "Epoch: 79 - Batch: 81856 - Loss: 0.081363 - Time:188.5388298034668\n",
      "Epoch: 79 - Batch: 85952 - Loss: 0.061704 - Time:197.9508194923401\n",
      "Epoch: 79 - Batch: 90048 - Loss: 0.087950 - Time:207.36329793930054\n",
      "Epoch: 79 - Batch: 94144 - Loss: 0.166034 - Time:216.7747552394867\n",
      "Epoch: 79 - Batch: 98240 - Loss: 0.329345 - Time:226.18827319145203\n",
      "Epoch: 79 - Batch: 102336 - Loss: 0.193699 - Time:235.6207914352417\n",
      "Epoch: 79 - Batch: 106432 - Loss: 0.175915 - Time:245.03560829162598\n",
      "Epoch: 79 - Batch: 110528 - Loss: 0.028328 - Time:254.4496831893921\n",
      "Epoch: 79 - Batch: 114624 - Loss: 0.097558 - Time:263.8618497848511\n",
      "Epoch: 79 - Batch: 118720 - Loss: 0.045058 - Time:273.2926275730133\n",
      "Epoch: 79 - Batch: 122816 - Loss: 0.032166 - Time:282.705628156662\n",
      "Epoch: 79 - Batch: 126912 - Loss: 0.266530 - Time:292.1185736656189\n",
      "Epoch: 79 - Batch: 131008 - Loss: 0.040729 - Time:301.54993748664856\n",
      "Epoch: 79 - Batch: 135104 - Loss: 0.031892 - Time:310.9600086212158\n",
      "Epoch: 79 - Batch: 139200 - Loss: 0.134575 - Time:320.3741433620453\n",
      "Epoch: 79 - Batch: 143296 - Loss: 0.065913 - Time:329.78919982910156\n",
      "Epoch: 79 - Batch: 147392 - Loss: 0.222070 - Time:339.2025969028473\n",
      "Epoch: 79 - Batch: 151488 - Loss: 0.397403 - Time:348.6182987689972\n",
      "Epoch: 80 - Batch: 4032 - Loss: 0.070824 - Time:9.587797403335571\n",
      "Epoch: 80 - Batch: 8128 - Loss: 0.027717 - Time:19.034069299697876\n",
      "Epoch: 80 - Batch: 12224 - Loss: 0.031637 - Time:28.449825286865234\n",
      "Epoch: 80 - Batch: 16320 - Loss: 0.080247 - Time:37.86539649963379\n",
      "Epoch: 80 - Batch: 20416 - Loss: 0.031787 - Time:47.280293464660645\n",
      "Epoch: 80 - Batch: 24512 - Loss: 0.082207 - Time:56.7110538482666\n",
      "Epoch: 80 - Batch: 28608 - Loss: 0.201422 - Time:66.12597560882568\n",
      "Epoch: 80 - Batch: 32704 - Loss: 0.083704 - Time:75.53762626647949\n",
      "Epoch: 80 - Batch: 36800 - Loss: 0.043912 - Time:84.965824842453\n",
      "Epoch: 80 - Batch: 40896 - Loss: 0.063201 - Time:94.3766417503357\n",
      "Epoch: 80 - Batch: 44992 - Loss: 0.105830 - Time:103.79100632667542\n",
      "Epoch: 80 - Batch: 49088 - Loss: 0.095716 - Time:113.20559763908386\n",
      "Epoch: 80 - Batch: 53184 - Loss: 0.113937 - Time:122.61904835700989\n",
      "Epoch: 80 - Batch: 57280 - Loss: 0.219735 - Time:132.03223323822021\n",
      "Epoch: 80 - Batch: 61376 - Loss: 0.066538 - Time:141.4440746307373\n",
      "Epoch: 80 - Batch: 65472 - Loss: 0.203994 - Time:150.85706496238708\n",
      "Epoch: 80 - Batch: 69568 - Loss: 0.175690 - Time:160.2887511253357\n",
      "Epoch: 80 - Batch: 73664 - Loss: 0.041774 - Time:169.70259881019592\n",
      "Epoch: 80 - Batch: 77760 - Loss: 0.134404 - Time:179.11682796478271\n",
      "Epoch: 80 - Batch: 81856 - Loss: 0.231049 - Time:188.53057193756104\n",
      "Epoch: 80 - Batch: 85952 - Loss: 0.059419 - Time:197.94554257392883\n",
      "Epoch: 80 - Batch: 90048 - Loss: 0.092759 - Time:207.3770513534546\n",
      "Epoch: 80 - Batch: 94144 - Loss: 0.120588 - Time:216.7929711341858\n",
      "Epoch: 80 - Batch: 98240 - Loss: 0.180475 - Time:226.2084231376648\n",
      "Epoch: 80 - Batch: 102336 - Loss: 0.067391 - Time:235.64038157463074\n",
      "Epoch: 80 - Batch: 106432 - Loss: 0.212864 - Time:245.0532190799713\n",
      "Epoch: 80 - Batch: 110528 - Loss: 0.070892 - Time:254.4671037197113\n",
      "Epoch: 80 - Batch: 114624 - Loss: 0.129093 - Time:263.8816454410553\n",
      "Epoch: 80 - Batch: 118720 - Loss: 0.071947 - Time:273.297061920166\n",
      "Epoch: 80 - Batch: 122816 - Loss: 0.572537 - Time:282.7101511955261\n",
      "Epoch: 80 - Batch: 126912 - Loss: 0.386713 - Time:292.1256823539734\n",
      "Epoch: 80 - Batch: 131008 - Loss: 0.031755 - Time:301.54074788093567\n",
      "Epoch: 80 - Batch: 135104 - Loss: 0.130272 - Time:310.9743447303772\n",
      "Epoch: 80 - Batch: 139200 - Loss: 0.193457 - Time:320.3890700340271\n",
      "Epoch: 80 - Batch: 143296 - Loss: 0.410499 - Time:329.8015422821045\n",
      "Epoch: 80 - Batch: 147392 - Loss: 0.060528 - Time:339.2141263484955\n",
      "Epoch: 80 - Batch: 151488 - Loss: 0.219029 - Time:348.64294362068176\n",
      "Epoch: 81 - Batch: 4032 - Loss: 0.385208 - Time:9.572616815567017\n",
      "Epoch: 81 - Batch: 8128 - Loss: 0.212703 - Time:18.9885516166687\n",
      "Epoch: 81 - Batch: 12224 - Loss: 0.098582 - Time:28.429561138153076\n",
      "Epoch: 81 - Batch: 16320 - Loss: 0.144064 - Time:37.84125828742981\n",
      "Epoch: 81 - Batch: 20416 - Loss: 0.034980 - Time:47.254170179367065\n",
      "Epoch: 81 - Batch: 24512 - Loss: 0.029599 - Time:56.68286108970642\n",
      "Epoch: 81 - Batch: 28608 - Loss: 0.182795 - Time:66.09402823448181\n",
      "Epoch: 81 - Batch: 32704 - Loss: 0.218527 - Time:75.50839829444885\n",
      "Epoch: 81 - Batch: 36800 - Loss: 0.156299 - Time:84.92040753364563\n",
      "Epoch: 81 - Batch: 40896 - Loss: 0.053422 - Time:94.3305914402008\n",
      "Epoch: 81 - Batch: 44992 - Loss: 0.064748 - Time:103.74174952507019\n",
      "Epoch: 81 - Batch: 49088 - Loss: 0.212893 - Time:113.15260815620422\n",
      "Epoch: 81 - Batch: 53184 - Loss: 0.061583 - Time:122.56555676460266\n",
      "Epoch: 81 - Batch: 57280 - Loss: 0.030913 - Time:131.9959135055542\n",
      "Epoch: 81 - Batch: 61376 - Loss: 0.091556 - Time:141.40749335289001\n",
      "Epoch: 81 - Batch: 65472 - Loss: 0.073884 - Time:150.8190953731537\n",
      "Epoch: 81 - Batch: 69568 - Loss: 0.146608 - Time:160.23049092292786\n",
      "Epoch: 81 - Batch: 73664 - Loss: 0.134531 - Time:169.6609661579132\n",
      "Epoch: 81 - Batch: 77760 - Loss: 0.129178 - Time:179.0721938610077\n",
      "Epoch: 81 - Batch: 81856 - Loss: 0.295953 - Time:188.4840202331543\n",
      "Epoch: 81 - Batch: 85952 - Loss: 0.078735 - Time:197.91381669044495\n",
      "Epoch: 81 - Batch: 90048 - Loss: 0.053445 - Time:207.32961225509644\n",
      "Epoch: 81 - Batch: 94144 - Loss: 0.036433 - Time:216.74287486076355\n",
      "Epoch: 81 - Batch: 98240 - Loss: 0.049130 - Time:226.1557698249817\n",
      "Epoch: 81 - Batch: 102336 - Loss: 0.053734 - Time:235.5676646232605\n",
      "Epoch: 81 - Batch: 106432 - Loss: 0.075076 - Time:244.98146653175354\n",
      "Epoch: 81 - Batch: 110528 - Loss: 0.058624 - Time:254.39436721801758\n",
      "Epoch: 81 - Batch: 114624 - Loss: 0.095583 - Time:263.8068196773529\n",
      "Epoch: 81 - Batch: 118720 - Loss: 0.150603 - Time:273.237074136734\n",
      "Epoch: 81 - Batch: 122816 - Loss: 0.175553 - Time:282.648309469223\n",
      "Epoch: 81 - Batch: 126912 - Loss: 0.102621 - Time:292.0592505931854\n",
      "Epoch: 81 - Batch: 131008 - Loss: 0.254184 - Time:301.47037649154663\n",
      "Epoch: 81 - Batch: 135104 - Loss: 0.200678 - Time:310.8989291191101\n",
      "Epoch: 81 - Batch: 139200 - Loss: 0.093718 - Time:320.3117961883545\n",
      "Epoch: 81 - Batch: 143296 - Loss: 0.200995 - Time:329.72236227989197\n",
      "Epoch: 81 - Batch: 147392 - Loss: 0.307570 - Time:339.15359473228455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 - Batch: 151488 - Loss: 0.063036 - Time:348.56673312187195\n",
      "Epoch: 82 - Batch: 4032 - Loss: 0.120261 - Time:9.577919006347656\n",
      "Epoch: 82 - Batch: 8128 - Loss: 0.077744 - Time:18.992846250534058\n",
      "Epoch: 82 - Batch: 12224 - Loss: 0.134265 - Time:28.406773567199707\n",
      "Epoch: 82 - Batch: 16320 - Loss: 0.525736 - Time:37.82208466529846\n",
      "Epoch: 82 - Batch: 20416 - Loss: 0.166378 - Time:47.23821401596069\n",
      "Epoch: 82 - Batch: 24512 - Loss: 0.357012 - Time:56.65681028366089\n",
      "Epoch: 82 - Batch: 28608 - Loss: 0.017650 - Time:66.09929990768433\n",
      "Epoch: 82 - Batch: 32704 - Loss: 0.016651 - Time:75.51423001289368\n",
      "Epoch: 82 - Batch: 36800 - Loss: 0.030384 - Time:84.93015432357788\n",
      "Epoch: 82 - Batch: 40896 - Loss: 0.033808 - Time:94.34430265426636\n",
      "Epoch: 82 - Batch: 44992 - Loss: 0.157772 - Time:103.77511763572693\n",
      "Epoch: 82 - Batch: 49088 - Loss: 0.108768 - Time:113.18795990943909\n",
      "Epoch: 82 - Batch: 53184 - Loss: 0.075219 - Time:122.60043811798096\n",
      "Epoch: 82 - Batch: 57280 - Loss: 0.255828 - Time:132.0313606262207\n",
      "Epoch: 82 - Batch: 61376 - Loss: 0.136325 - Time:141.44472765922546\n",
      "Epoch: 82 - Batch: 65472 - Loss: 0.149124 - Time:150.85927176475525\n",
      "Epoch: 82 - Batch: 69568 - Loss: 0.100312 - Time:160.27337646484375\n",
      "Epoch: 82 - Batch: 73664 - Loss: 0.045524 - Time:169.68751883506775\n",
      "Epoch: 82 - Batch: 77760 - Loss: 0.076706 - Time:179.10131788253784\n",
      "Epoch: 82 - Batch: 81856 - Loss: 0.063245 - Time:188.51496148109436\n",
      "Epoch: 82 - Batch: 85952 - Loss: 0.051772 - Time:197.9300217628479\n",
      "Epoch: 82 - Batch: 90048 - Loss: 0.092900 - Time:207.36096739768982\n",
      "Epoch: 82 - Batch: 94144 - Loss: 0.083713 - Time:216.77369952201843\n",
      "Epoch: 82 - Batch: 98240 - Loss: 0.019712 - Time:226.1858103275299\n",
      "Epoch: 82 - Batch: 102336 - Loss: 0.172879 - Time:235.60045289993286\n",
      "Epoch: 82 - Batch: 106432 - Loss: 0.034372 - Time:245.03015065193176\n",
      "Epoch: 82 - Batch: 110528 - Loss: 0.301047 - Time:254.44389939308167\n",
      "Epoch: 82 - Batch: 114624 - Loss: 0.140335 - Time:263.85598826408386\n",
      "Epoch: 82 - Batch: 118720 - Loss: 0.369900 - Time:273.2863519191742\n",
      "Epoch: 82 - Batch: 122816 - Loss: 0.202657 - Time:282.7000341415405\n",
      "Epoch: 82 - Batch: 126912 - Loss: 0.068664 - Time:292.1114721298218\n",
      "Epoch: 82 - Batch: 131008 - Loss: 0.100367 - Time:301.5258836746216\n",
      "Epoch: 82 - Batch: 135104 - Loss: 0.032703 - Time:310.94081711769104\n",
      "Epoch: 82 - Batch: 139200 - Loss: 0.081204 - Time:320.3532192707062\n",
      "Epoch: 82 - Batch: 143296 - Loss: 0.047217 - Time:329.7653512954712\n",
      "Epoch: 82 - Batch: 147392 - Loss: 0.143056 - Time:339.1793165206909\n",
      "Epoch: 82 - Batch: 151488 - Loss: 0.386260 - Time:348.6101043224335\n",
      "Epoch: 83 - Batch: 4032 - Loss: 0.040611 - Time:9.585876703262329\n",
      "Epoch: 83 - Batch: 8128 - Loss: 0.029098 - Time:19.00562310218811\n",
      "Epoch: 83 - Batch: 12224 - Loss: 0.180762 - Time:28.420786380767822\n",
      "Epoch: 83 - Batch: 16320 - Loss: 0.301615 - Time:37.83488726615906\n",
      "Epoch: 83 - Batch: 20416 - Loss: 0.400697 - Time:47.25112342834473\n",
      "Epoch: 83 - Batch: 24512 - Loss: 0.120527 - Time:56.664525508880615\n",
      "Epoch: 83 - Batch: 28608 - Loss: 0.035494 - Time:66.10638976097107\n",
      "Epoch: 83 - Batch: 32704 - Loss: 0.048811 - Time:75.52170777320862\n",
      "Epoch: 83 - Batch: 36800 - Loss: 0.057047 - Time:84.93578577041626\n",
      "Epoch: 83 - Batch: 40896 - Loss: 0.335612 - Time:94.34999227523804\n",
      "Epoch: 83 - Batch: 44992 - Loss: 0.096730 - Time:103.78046464920044\n",
      "Epoch: 83 - Batch: 49088 - Loss: 0.148057 - Time:113.19286322593689\n",
      "Epoch: 83 - Batch: 53184 - Loss: 0.092675 - Time:122.60761904716492\n",
      "Epoch: 83 - Batch: 57280 - Loss: 0.070333 - Time:132.03787446022034\n",
      "Epoch: 83 - Batch: 61376 - Loss: 0.041345 - Time:141.4518358707428\n",
      "Epoch: 83 - Batch: 65472 - Loss: 0.080568 - Time:150.86571335792542\n",
      "Epoch: 83 - Batch: 69568 - Loss: 0.153049 - Time:160.27890992164612\n",
      "Epoch: 83 - Batch: 73664 - Loss: 0.036077 - Time:169.69081020355225\n",
      "Epoch: 83 - Batch: 77760 - Loss: 0.129657 - Time:179.10434222221375\n",
      "Epoch: 83 - Batch: 81856 - Loss: 0.127388 - Time:188.51717400550842\n",
      "Epoch: 83 - Batch: 85952 - Loss: 0.077795 - Time:197.9286756515503\n",
      "Epoch: 83 - Batch: 90048 - Loss: 0.111908 - Time:207.36058807373047\n",
      "Epoch: 83 - Batch: 94144 - Loss: 0.154834 - Time:216.77447152137756\n",
      "Epoch: 83 - Batch: 98240 - Loss: 0.327060 - Time:226.19126272201538\n",
      "Epoch: 83 - Batch: 102336 - Loss: 0.147635 - Time:235.60776901245117\n",
      "Epoch: 83 - Batch: 106432 - Loss: 0.165721 - Time:245.01980638504028\n",
      "Epoch: 83 - Batch: 110528 - Loss: 0.050904 - Time:254.4484567642212\n",
      "Epoch: 83 - Batch: 114624 - Loss: 0.110752 - Time:263.8591434955597\n",
      "Epoch: 83 - Batch: 118720 - Loss: 0.235979 - Time:273.2898483276367\n",
      "Epoch: 83 - Batch: 122816 - Loss: 0.193885 - Time:282.70351696014404\n",
      "Epoch: 83 - Batch: 126912 - Loss: 0.220737 - Time:292.11776852607727\n",
      "Epoch: 83 - Batch: 131008 - Loss: 0.042227 - Time:301.5335068702698\n",
      "Epoch: 83 - Batch: 135104 - Loss: 0.072050 - Time:310.9485981464386\n",
      "Epoch: 83 - Batch: 139200 - Loss: 0.023425 - Time:320.36467719078064\n",
      "Epoch: 83 - Batch: 143296 - Loss: 0.098245 - Time:329.77716064453125\n",
      "Epoch: 83 - Batch: 147392 - Loss: 0.038173 - Time:339.1906678676605\n",
      "Epoch: 83 - Batch: 151488 - Loss: 0.375304 - Time:348.6026861667633\n",
      "Epoch: 84 - Batch: 4032 - Loss: 0.502739 - Time:9.577157974243164\n",
      "Epoch: 84 - Batch: 8128 - Loss: 0.023710 - Time:18.995038747787476\n",
      "Epoch: 84 - Batch: 12224 - Loss: 0.211206 - Time:28.412896156311035\n",
      "Epoch: 84 - Batch: 16320 - Loss: 0.382316 - Time:37.85657501220703\n",
      "Epoch: 84 - Batch: 20416 - Loss: 0.112152 - Time:47.271770000457764\n",
      "Epoch: 84 - Batch: 24512 - Loss: 0.031233 - Time:56.685978174209595\n",
      "Epoch: 84 - Batch: 28608 - Loss: 0.144477 - Time:66.11695766448975\n",
      "Epoch: 84 - Batch: 32704 - Loss: 0.046276 - Time:75.53156447410583\n",
      "Epoch: 84 - Batch: 36800 - Loss: 0.033074 - Time:84.94619297981262\n",
      "Epoch: 84 - Batch: 40896 - Loss: 0.093327 - Time:94.3593897819519\n",
      "Epoch: 84 - Batch: 44992 - Loss: 0.060053 - Time:103.77328610420227\n",
      "Epoch: 84 - Batch: 49088 - Loss: 0.179161 - Time:113.18765354156494\n",
      "Epoch: 84 - Batch: 53184 - Loss: 0.063870 - Time:122.60051155090332\n",
      "Epoch: 84 - Batch: 57280 - Loss: 0.027190 - Time:132.01431441307068\n",
      "Epoch: 84 - Batch: 61376 - Loss: 0.060867 - Time:141.44438886642456\n",
      "Epoch: 84 - Batch: 65472 - Loss: 0.027588 - Time:150.85753417015076\n",
      "Epoch: 84 - Batch: 69568 - Loss: 0.126266 - Time:160.27045035362244\n",
      "Epoch: 84 - Batch: 73664 - Loss: 0.090871 - Time:169.68396711349487\n",
      "Epoch: 84 - Batch: 77760 - Loss: 0.033711 - Time:179.1155252456665\n",
      "Epoch: 84 - Batch: 81856 - Loss: 0.237657 - Time:188.52885246276855\n",
      "Epoch: 84 - Batch: 85952 - Loss: 0.093006 - Time:197.944397687912\n",
      "Epoch: 84 - Batch: 90048 - Loss: 0.156288 - Time:207.3776617050171\n",
      "Epoch: 84 - Batch: 94144 - Loss: 0.247292 - Time:216.79002213478088\n",
      "Epoch: 84 - Batch: 98240 - Loss: 0.124403 - Time:226.20468616485596\n",
      "Epoch: 84 - Batch: 102336 - Loss: 0.078989 - Time:235.6176815032959\n",
      "Epoch: 84 - Batch: 106432 - Loss: 0.228962 - Time:245.0305414199829\n",
      "Epoch: 84 - Batch: 110528 - Loss: 0.110066 - Time:254.4445207118988\n",
      "Epoch: 84 - Batch: 114624 - Loss: 0.063129 - Time:263.8584158420563\n",
      "Epoch: 84 - Batch: 118720 - Loss: 0.112372 - Time:273.2733542919159\n",
      "Epoch: 84 - Batch: 122816 - Loss: 0.515392 - Time:282.7055335044861\n",
      "Epoch: 84 - Batch: 126912 - Loss: 0.143004 - Time:292.1210114955902\n",
      "Epoch: 84 - Batch: 131008 - Loss: 0.051204 - Time:301.5377175807953\n",
      "Epoch: 84 - Batch: 135104 - Loss: 0.190285 - Time:310.9532399177551\n",
      "Epoch: 84 - Batch: 139200 - Loss: 0.068966 - Time:320.3696036338806\n",
      "Epoch: 84 - Batch: 143296 - Loss: 0.279013 - Time:329.7998380661011\n",
      "Epoch: 84 - Batch: 147392 - Loss: 0.080156 - Time:339.2136423587799\n",
      "Epoch: 84 - Batch: 151488 - Loss: 0.202321 - Time:348.6294894218445\n",
      "Epoch: 85 - Batch: 4032 - Loss: 0.278624 - Time:9.571078777313232\n",
      "Epoch: 85 - Batch: 8128 - Loss: 0.144675 - Time:18.986263275146484\n",
      "Epoch: 85 - Batch: 12224 - Loss: 0.176804 - Time:28.428709745407104\n",
      "Epoch: 85 - Batch: 16320 - Loss: 0.124703 - Time:37.84150290489197\n",
      "Epoch: 85 - Batch: 20416 - Loss: 0.063905 - Time:47.2548394203186\n",
      "Epoch: 85 - Batch: 24512 - Loss: 0.039922 - Time:56.66842722892761\n",
      "Epoch: 85 - Batch: 28608 - Loss: 0.381793 - Time:66.0803062915802\n",
      "Epoch: 85 - Batch: 32704 - Loss: 0.358306 - Time:75.49461841583252\n",
      "Epoch: 85 - Batch: 36800 - Loss: 0.107446 - Time:84.90884137153625\n",
      "Epoch: 85 - Batch: 40896 - Loss: 0.027545 - Time:94.32005834579468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 - Batch: 44992 - Loss: 0.069632 - Time:103.75098085403442\n",
      "Epoch: 85 - Batch: 49088 - Loss: 0.121636 - Time:113.16314244270325\n",
      "Epoch: 85 - Batch: 53184 - Loss: 0.088122 - Time:122.57370281219482\n",
      "Epoch: 85 - Batch: 57280 - Loss: 0.050035 - Time:131.98789405822754\n",
      "Epoch: 85 - Batch: 61376 - Loss: 0.170449 - Time:141.40217542648315\n",
      "Epoch: 85 - Batch: 65472 - Loss: 0.051108 - Time:150.8343164920807\n",
      "Epoch: 85 - Batch: 69568 - Loss: 0.084532 - Time:160.25080585479736\n",
      "Epoch: 85 - Batch: 73664 - Loss: 0.244406 - Time:169.68177485466003\n",
      "Epoch: 85 - Batch: 77760 - Loss: 0.067865 - Time:179.09636545181274\n",
      "Epoch: 85 - Batch: 81856 - Loss: 0.238015 - Time:188.51282405853271\n",
      "Epoch: 85 - Batch: 85952 - Loss: 0.027053 - Time:197.92763090133667\n",
      "Epoch: 85 - Batch: 90048 - Loss: 0.065620 - Time:207.3419852256775\n",
      "Epoch: 85 - Batch: 94144 - Loss: 0.355328 - Time:216.7568907737732\n",
      "Epoch: 85 - Batch: 98240 - Loss: 0.342298 - Time:226.17180681228638\n",
      "Epoch: 85 - Batch: 102336 - Loss: 0.222909 - Time:235.58838057518005\n",
      "Epoch: 85 - Batch: 106432 - Loss: 0.050689 - Time:245.003835439682\n",
      "Epoch: 85 - Batch: 110528 - Loss: 0.165711 - Time:254.43686175346375\n",
      "Epoch: 85 - Batch: 114624 - Loss: 0.042893 - Time:263.8509454727173\n",
      "Epoch: 85 - Batch: 118720 - Loss: 0.026554 - Time:273.26398944854736\n",
      "Epoch: 85 - Batch: 122816 - Loss: 0.102435 - Time:282.67683839797974\n",
      "Epoch: 85 - Batch: 126912 - Loss: 0.206594 - Time:292.10854387283325\n",
      "Epoch: 85 - Batch: 131008 - Loss: 0.106643 - Time:301.52469420433044\n",
      "Epoch: 85 - Batch: 135104 - Loss: 0.101140 - Time:310.9399642944336\n",
      "Epoch: 85 - Batch: 139200 - Loss: 0.185283 - Time:320.37453746795654\n",
      "Epoch: 85 - Batch: 143296 - Loss: 0.102669 - Time:329.79142236709595\n",
      "Epoch: 85 - Batch: 147392 - Loss: 0.042892 - Time:339.20795035362244\n",
      "Epoch: 85 - Batch: 151488 - Loss: 0.396711 - Time:348.62310338020325\n",
      "Epoch: 86 - Batch: 4032 - Loss: 0.022675 - Time:9.576940059661865\n",
      "Epoch: 86 - Batch: 8128 - Loss: 0.026385 - Time:19.022624492645264\n",
      "Epoch: 86 - Batch: 12224 - Loss: 0.290847 - Time:28.440383672714233\n",
      "Epoch: 86 - Batch: 16320 - Loss: 0.071513 - Time:37.85499095916748\n",
      "Epoch: 86 - Batch: 20416 - Loss: 0.184652 - Time:47.268439292907715\n",
      "Epoch: 86 - Batch: 24512 - Loss: 0.035977 - Time:56.68314456939697\n",
      "Epoch: 86 - Batch: 28608 - Loss: 0.279237 - Time:66.09722399711609\n",
      "Epoch: 86 - Batch: 32704 - Loss: 0.125066 - Time:75.5117175579071\n",
      "Epoch: 86 - Batch: 36800 - Loss: 0.025298 - Time:84.92462587356567\n",
      "Epoch: 86 - Batch: 40896 - Loss: 0.143058 - Time:94.3570728302002\n",
      "Epoch: 86 - Batch: 44992 - Loss: 0.271776 - Time:103.76974320411682\n",
      "Epoch: 86 - Batch: 49088 - Loss: 0.173899 - Time:113.18307495117188\n",
      "Epoch: 86 - Batch: 53184 - Loss: 0.093738 - Time:122.59627199172974\n",
      "Epoch: 86 - Batch: 57280 - Loss: 0.055948 - Time:132.02639842033386\n",
      "Epoch: 86 - Batch: 61376 - Loss: 0.063950 - Time:141.4392855167389\n",
      "Epoch: 86 - Batch: 65472 - Loss: 0.185249 - Time:150.8541543483734\n",
      "Epoch: 86 - Batch: 69568 - Loss: 0.100709 - Time:160.28777146339417\n",
      "Epoch: 86 - Batch: 73664 - Loss: 0.123131 - Time:169.70136523246765\n",
      "Epoch: 86 - Batch: 77760 - Loss: 0.618212 - Time:179.11319398880005\n",
      "Epoch: 86 - Batch: 81856 - Loss: 0.169378 - Time:188.52626633644104\n",
      "Epoch: 86 - Batch: 85952 - Loss: 0.418989 - Time:197.93825101852417\n",
      "Epoch: 86 - Batch: 90048 - Loss: 0.030866 - Time:207.3507559299469\n",
      "Epoch: 86 - Batch: 94144 - Loss: 0.106587 - Time:216.765704870224\n",
      "Epoch: 86 - Batch: 98240 - Loss: 0.219483 - Time:226.17849254608154\n",
      "Epoch: 86 - Batch: 102336 - Loss: 0.236549 - Time:235.61308789253235\n",
      "Epoch: 86 - Batch: 106432 - Loss: 0.085380 - Time:245.02858352661133\n",
      "Epoch: 86 - Batch: 110528 - Loss: 0.095516 - Time:254.4421305656433\n",
      "Epoch: 86 - Batch: 114624 - Loss: 0.024113 - Time:263.85379099845886\n",
      "Epoch: 86 - Batch: 118720 - Loss: 0.091014 - Time:273.283983707428\n",
      "Epoch: 86 - Batch: 122816 - Loss: 0.049125 - Time:282.6962766647339\n",
      "Epoch: 86 - Batch: 126912 - Loss: 0.058273 - Time:292.10998821258545\n",
      "Epoch: 86 - Batch: 131008 - Loss: 0.031946 - Time:301.5407636165619\n",
      "Epoch: 86 - Batch: 135104 - Loss: 0.074640 - Time:310.9542541503906\n",
      "Epoch: 86 - Batch: 139200 - Loss: 0.591308 - Time:320.36968302726746\n",
      "Epoch: 86 - Batch: 143296 - Loss: 0.100736 - Time:329.78213357925415\n",
      "Epoch: 86 - Batch: 147392 - Loss: 0.302866 - Time:339.196799993515\n",
      "Epoch: 86 - Batch: 151488 - Loss: 0.276643 - Time:348.6105704307556\n",
      "Epoch: 87 - Batch: 4032 - Loss: 0.025779 - Time:9.610647678375244\n",
      "Epoch: 87 - Batch: 8128 - Loss: 0.029953 - Time:19.026760578155518\n",
      "Epoch: 87 - Batch: 12224 - Loss: 0.322518 - Time:28.44073796272278\n",
      "Epoch: 87 - Batch: 16320 - Loss: 0.045138 - Time:37.853148221969604\n",
      "Epoch: 87 - Batch: 20416 - Loss: 0.095900 - Time:47.26588582992554\n",
      "Epoch: 87 - Batch: 24512 - Loss: 0.035908 - Time:56.68010878562927\n",
      "Epoch: 87 - Batch: 28608 - Loss: 0.053942 - Time:66.09466409683228\n",
      "Epoch: 87 - Batch: 32704 - Loss: 0.202237 - Time:75.50742673873901\n",
      "Epoch: 87 - Batch: 36800 - Loss: 0.405976 - Time:84.93776679039001\n",
      "Epoch: 87 - Batch: 40896 - Loss: 0.160395 - Time:94.34835147857666\n",
      "Epoch: 87 - Batch: 44992 - Loss: 0.399241 - Time:103.75912714004517\n",
      "Epoch: 87 - Batch: 49088 - Loss: 0.367962 - Time:113.17289018630981\n",
      "Epoch: 87 - Batch: 53184 - Loss: 0.090113 - Time:122.58450365066528\n",
      "Epoch: 87 - Batch: 57280 - Loss: 0.224562 - Time:132.01409530639648\n",
      "Epoch: 87 - Batch: 61376 - Loss: 0.051855 - Time:141.42534494400024\n",
      "Epoch: 87 - Batch: 65472 - Loss: 0.130850 - Time:150.84013104438782\n",
      "Epoch: 87 - Batch: 69568 - Loss: 0.232701 - Time:160.2782552242279\n",
      "Epoch: 87 - Batch: 73664 - Loss: 0.052564 - Time:169.6939034461975\n",
      "Epoch: 87 - Batch: 77760 - Loss: 0.103408 - Time:179.1077196598053\n",
      "Epoch: 87 - Batch: 81856 - Loss: 0.125688 - Time:188.52053880691528\n",
      "Epoch: 87 - Batch: 85952 - Loss: 0.425807 - Time:197.93499660491943\n",
      "Epoch: 87 - Batch: 90048 - Loss: 0.116789 - Time:207.3468792438507\n",
      "Epoch: 87 - Batch: 94144 - Loss: 0.070719 - Time:216.7595956325531\n",
      "Epoch: 87 - Batch: 98240 - Loss: 0.331540 - Time:226.1734721660614\n",
      "Epoch: 87 - Batch: 102336 - Loss: 0.033219 - Time:235.6036388874054\n",
      "Epoch: 87 - Batch: 106432 - Loss: 0.024501 - Time:245.01555275917053\n",
      "Epoch: 87 - Batch: 110528 - Loss: 0.041072 - Time:254.4289472103119\n",
      "Epoch: 87 - Batch: 114624 - Loss: 0.119217 - Time:263.8404724597931\n",
      "Epoch: 87 - Batch: 118720 - Loss: 0.032291 - Time:273.27215695381165\n",
      "Epoch: 87 - Batch: 122816 - Loss: 0.058314 - Time:282.68580198287964\n",
      "Epoch: 87 - Batch: 126912 - Loss: 0.379403 - Time:292.0983192920685\n",
      "Epoch: 87 - Batch: 131008 - Loss: 0.151088 - Time:301.5290319919586\n",
      "Epoch: 87 - Batch: 135104 - Loss: 0.063801 - Time:310.9427170753479\n",
      "Epoch: 87 - Batch: 139200 - Loss: 0.043576 - Time:320.35597467422485\n",
      "Epoch: 87 - Batch: 143296 - Loss: 0.317677 - Time:329.7711477279663\n",
      "Epoch: 87 - Batch: 147392 - Loss: 0.090737 - Time:339.18512058258057\n",
      "Epoch: 87 - Batch: 151488 - Loss: 0.061150 - Time:348.5993323326111\n",
      "Epoch: 88 - Batch: 4032 - Loss: 0.016127 - Time:9.575803518295288\n",
      "Epoch: 88 - Batch: 8128 - Loss: 0.120692 - Time:19.0224130153656\n",
      "Epoch: 88 - Batch: 12224 - Loss: 0.018796 - Time:28.438674926757812\n",
      "Epoch: 88 - Batch: 16320 - Loss: 0.076861 - Time:37.85383915901184\n",
      "Epoch: 88 - Batch: 20416 - Loss: 0.022141 - Time:47.26782751083374\n",
      "Epoch: 88 - Batch: 24512 - Loss: 0.457015 - Time:56.69968271255493\n",
      "Epoch: 88 - Batch: 28608 - Loss: 0.033607 - Time:66.1149423122406\n",
      "Epoch: 88 - Batch: 32704 - Loss: 0.191753 - Time:75.52637839317322\n",
      "Epoch: 88 - Batch: 36800 - Loss: 0.087258 - Time:84.9552538394928\n",
      "Epoch: 88 - Batch: 40896 - Loss: 0.219528 - Time:94.36736106872559\n",
      "Epoch: 88 - Batch: 44992 - Loss: 0.028807 - Time:103.78072166442871\n",
      "Epoch: 88 - Batch: 49088 - Loss: 0.208015 - Time:113.1953022480011\n",
      "Epoch: 88 - Batch: 53184 - Loss: 0.037923 - Time:122.6088137626648\n",
      "Epoch: 88 - Batch: 57280 - Loss: 0.075286 - Time:132.02137756347656\n",
      "Epoch: 88 - Batch: 61376 - Loss: 0.156862 - Time:141.43364548683167\n",
      "Epoch: 88 - Batch: 65472 - Loss: 0.036261 - Time:150.84637260437012\n",
      "Epoch: 88 - Batch: 69568 - Loss: 0.041886 - Time:160.27707409858704\n",
      "Epoch: 88 - Batch: 73664 - Loss: 0.023066 - Time:169.68923544883728\n",
      "Epoch: 88 - Batch: 77760 - Loss: 0.166495 - Time:179.1030716896057\n",
      "Epoch: 88 - Batch: 81856 - Loss: 0.174651 - Time:188.51596689224243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 - Batch: 85952 - Loss: 0.140047 - Time:197.9274034500122\n",
      "Epoch: 88 - Batch: 90048 - Loss: 0.157891 - Time:207.35666918754578\n",
      "Epoch: 88 - Batch: 94144 - Loss: 0.133871 - Time:216.771639585495\n",
      "Epoch: 88 - Batch: 98240 - Loss: 0.028359 - Time:226.18631148338318\n",
      "Epoch: 88 - Batch: 102336 - Loss: 0.158983 - Time:235.61828112602234\n",
      "Epoch: 88 - Batch: 106432 - Loss: 0.269634 - Time:245.03595566749573\n",
      "Epoch: 88 - Batch: 110528 - Loss: 0.153457 - Time:254.45086240768433\n",
      "Epoch: 88 - Batch: 114624 - Loss: 0.177849 - Time:263.864999294281\n",
      "Epoch: 88 - Batch: 118720 - Loss: 0.150990 - Time:273.27823519706726\n",
      "Epoch: 88 - Batch: 122816 - Loss: 0.122434 - Time:282.6911692619324\n",
      "Epoch: 88 - Batch: 126912 - Loss: 0.087408 - Time:292.10709404945374\n",
      "Epoch: 88 - Batch: 131008 - Loss: 0.125564 - Time:301.5213973522186\n",
      "Epoch: 88 - Batch: 135104 - Loss: 0.094853 - Time:310.95368552207947\n",
      "Epoch: 88 - Batch: 139200 - Loss: 0.038266 - Time:320.36978244781494\n",
      "Epoch: 88 - Batch: 143296 - Loss: 0.169200 - Time:329.7853307723999\n",
      "Epoch: 88 - Batch: 147392 - Loss: 0.074463 - Time:339.20148229599\n",
      "Epoch: 88 - Batch: 151488 - Loss: 0.019003 - Time:348.6352639198303\n",
      "Epoch: 89 - Batch: 4032 - Loss: 0.174122 - Time:9.585056066513062\n",
      "Epoch: 89 - Batch: 8128 - Loss: 0.045641 - Time:18.999561309814453\n",
      "Epoch: 89 - Batch: 12224 - Loss: 0.020604 - Time:28.442823886871338\n",
      "Epoch: 89 - Batch: 16320 - Loss: 0.115487 - Time:37.85652995109558\n",
      "Epoch: 89 - Batch: 20416 - Loss: 0.027348 - Time:47.273250102996826\n",
      "Epoch: 89 - Batch: 24512 - Loss: 0.049599 - Time:56.70511603355408\n",
      "Epoch: 89 - Batch: 28608 - Loss: 0.289933 - Time:66.11730694770813\n",
      "Epoch: 89 - Batch: 32704 - Loss: 0.037772 - Time:75.53007459640503\n",
      "Epoch: 89 - Batch: 36800 - Loss: 0.069457 - Time:84.94488954544067\n",
      "Epoch: 89 - Batch: 40896 - Loss: 0.040929 - Time:94.35989689826965\n",
      "Epoch: 89 - Batch: 44992 - Loss: 0.448143 - Time:103.77380084991455\n",
      "Epoch: 89 - Batch: 49088 - Loss: 0.074284 - Time:113.18804025650024\n",
      "Epoch: 89 - Batch: 53184 - Loss: 0.089669 - Time:122.60491919517517\n",
      "Epoch: 89 - Batch: 57280 - Loss: 0.023301 - Time:132.0375771522522\n",
      "Epoch: 89 - Batch: 61376 - Loss: 0.026506 - Time:141.4493556022644\n",
      "Epoch: 89 - Batch: 65472 - Loss: 0.034122 - Time:150.86296796798706\n",
      "Epoch: 89 - Batch: 69568 - Loss: 0.104327 - Time:160.27723789215088\n",
      "Epoch: 89 - Batch: 73664 - Loss: 0.130431 - Time:169.71107053756714\n",
      "Epoch: 89 - Batch: 77760 - Loss: 0.026123 - Time:179.1263086795807\n",
      "Epoch: 89 - Batch: 81856 - Loss: 0.182257 - Time:188.5401074886322\n",
      "Epoch: 89 - Batch: 85952 - Loss: 0.067635 - Time:197.97301650047302\n",
      "Epoch: 89 - Batch: 90048 - Loss: 0.046963 - Time:207.39023447036743\n",
      "Epoch: 89 - Batch: 94144 - Loss: 0.046387 - Time:216.80326771736145\n",
      "Epoch: 89 - Batch: 98240 - Loss: 0.081070 - Time:226.21682572364807\n",
      "Epoch: 89 - Batch: 102336 - Loss: 0.260865 - Time:235.63481855392456\n",
      "Epoch: 89 - Batch: 106432 - Loss: 0.122588 - Time:245.05036163330078\n",
      "Epoch: 89 - Batch: 110528 - Loss: 0.023931 - Time:254.4638910293579\n",
      "Epoch: 89 - Batch: 114624 - Loss: 0.117124 - Time:263.87725853919983\n",
      "Epoch: 89 - Batch: 118720 - Loss: 0.058410 - Time:273.3081614971161\n",
      "Epoch: 89 - Batch: 122816 - Loss: 0.056675 - Time:282.72240233421326\n",
      "Epoch: 89 - Batch: 126912 - Loss: 0.142604 - Time:292.1373519897461\n",
      "Epoch: 89 - Batch: 131008 - Loss: 0.360642 - Time:301.55017256736755\n",
      "Epoch: 89 - Batch: 135104 - Loss: 0.160445 - Time:310.9626851081848\n",
      "Epoch: 89 - Batch: 139200 - Loss: 0.162505 - Time:320.3939218521118\n",
      "Epoch: 89 - Batch: 143296 - Loss: 0.080296 - Time:329.80455446243286\n",
      "Epoch: 89 - Batch: 147392 - Loss: 0.062465 - Time:339.2359387874603\n",
      "Epoch: 89 - Batch: 151488 - Loss: 0.032580 - Time:348.65228748321533\n",
      "Epoch: 90 - Batch: 4032 - Loss: 0.198043 - Time:9.586323738098145\n",
      "Epoch: 90 - Batch: 8128 - Loss: 0.285190 - Time:19.001883029937744\n",
      "Epoch: 90 - Batch: 12224 - Loss: 0.147513 - Time:28.44490623474121\n",
      "Epoch: 90 - Batch: 16320 - Loss: 0.073272 - Time:37.860960721969604\n",
      "Epoch: 90 - Batch: 20416 - Loss: 0.027317 - Time:47.27684164047241\n",
      "Epoch: 90 - Batch: 24512 - Loss: 0.071598 - Time:56.69237542152405\n",
      "Epoch: 90 - Batch: 28608 - Loss: 0.022474 - Time:66.10758757591248\n",
      "Epoch: 90 - Batch: 32704 - Loss: 0.071503 - Time:75.52177572250366\n",
      "Epoch: 90 - Batch: 36800 - Loss: 0.134354 - Time:84.93690752983093\n",
      "Epoch: 90 - Batch: 40896 - Loss: 0.189613 - Time:94.3558075428009\n",
      "Epoch: 90 - Batch: 44992 - Loss: 0.044717 - Time:103.78857946395874\n",
      "Epoch: 90 - Batch: 49088 - Loss: 0.095182 - Time:113.20159864425659\n",
      "Epoch: 90 - Batch: 53184 - Loss: 0.019809 - Time:122.61517310142517\n",
      "Epoch: 90 - Batch: 57280 - Loss: 0.019334 - Time:132.0291395187378\n",
      "Epoch: 90 - Batch: 61376 - Loss: 0.181996 - Time:141.46057438850403\n",
      "Epoch: 90 - Batch: 65472 - Loss: 0.066129 - Time:150.87344694137573\n",
      "Epoch: 90 - Batch: 69568 - Loss: 0.039730 - Time:160.28719305992126\n",
      "Epoch: 90 - Batch: 73664 - Loss: 0.131373 - Time:169.72056221961975\n",
      "Epoch: 90 - Batch: 77760 - Loss: 0.189510 - Time:179.13468289375305\n",
      "Epoch: 90 - Batch: 81856 - Loss: 0.278712 - Time:188.5467643737793\n",
      "Epoch: 90 - Batch: 85952 - Loss: 0.047077 - Time:197.96260023117065\n",
      "Epoch: 90 - Batch: 90048 - Loss: 0.032855 - Time:207.37620210647583\n",
      "Epoch: 90 - Batch: 94144 - Loss: 0.460933 - Time:216.79196333885193\n",
      "Epoch: 90 - Batch: 98240 - Loss: 0.078826 - Time:226.20603775978088\n",
      "Epoch: 90 - Batch: 102336 - Loss: 0.027815 - Time:235.61955571174622\n",
      "Epoch: 90 - Batch: 106432 - Loss: 0.024232 - Time:245.05139136314392\n",
      "Epoch: 90 - Batch: 110528 - Loss: 0.158262 - Time:254.46529388427734\n",
      "Epoch: 90 - Batch: 114624 - Loss: 0.064408 - Time:263.8800058364868\n",
      "Epoch: 90 - Batch: 118720 - Loss: 0.184877 - Time:273.2973201274872\n",
      "Epoch: 90 - Batch: 122816 - Loss: 0.125222 - Time:282.73019003868103\n",
      "Epoch: 90 - Batch: 126912 - Loss: 0.138553 - Time:292.1456732749939\n",
      "Epoch: 90 - Batch: 131008 - Loss: 0.395749 - Time:301.561475276947\n",
      "Epoch: 90 - Batch: 135104 - Loss: 0.133188 - Time:310.99496936798096\n",
      "Epoch: 90 - Batch: 139200 - Loss: 0.048163 - Time:320.41249442100525\n",
      "Epoch: 90 - Batch: 143296 - Loss: 0.113724 - Time:329.8278455734253\n",
      "Epoch: 90 - Batch: 147392 - Loss: 0.025199 - Time:339.240464925766\n",
      "Epoch: 90 - Batch: 151488 - Loss: 0.025182 - Time:348.65277194976807\n",
      "Epoch: 91 - Batch: 4032 - Loss: 0.045688 - Time:9.602272748947144\n",
      "Epoch: 91 - Batch: 8128 - Loss: 0.059216 - Time:19.05000352859497\n",
      "Epoch: 91 - Batch: 12224 - Loss: 0.139404 - Time:28.465633630752563\n",
      "Epoch: 91 - Batch: 16320 - Loss: 0.034891 - Time:37.879745960235596\n",
      "Epoch: 91 - Batch: 20416 - Loss: 0.169871 - Time:47.29222393035889\n",
      "Epoch: 91 - Batch: 24512 - Loss: 0.284739 - Time:56.70691657066345\n",
      "Epoch: 91 - Batch: 28608 - Loss: 0.121395 - Time:66.12311100959778\n",
      "Epoch: 91 - Batch: 32704 - Loss: 0.121946 - Time:75.53720831871033\n",
      "Epoch: 91 - Batch: 36800 - Loss: 0.018831 - Time:84.95343852043152\n",
      "Epoch: 91 - Batch: 40896 - Loss: 0.023301 - Time:94.38399362564087\n",
      "Epoch: 91 - Batch: 44992 - Loss: 0.063553 - Time:103.79905891418457\n",
      "Epoch: 91 - Batch: 49088 - Loss: 0.049179 - Time:113.21286487579346\n",
      "Epoch: 91 - Batch: 53184 - Loss: 0.063248 - Time:122.62882113456726\n",
      "Epoch: 91 - Batch: 57280 - Loss: 0.150655 - Time:132.0600507259369\n",
      "Epoch: 91 - Batch: 61376 - Loss: 0.020253 - Time:141.4730260372162\n",
      "Epoch: 91 - Batch: 65472 - Loss: 0.103710 - Time:150.88850474357605\n",
      "Epoch: 91 - Batch: 69568 - Loss: 0.087157 - Time:160.32191348075867\n",
      "Epoch: 91 - Batch: 73664 - Loss: 0.192647 - Time:169.7348291873932\n",
      "Epoch: 91 - Batch: 77760 - Loss: 0.087228 - Time:179.14861726760864\n",
      "Epoch: 91 - Batch: 81856 - Loss: 0.043998 - Time:188.56192779541016\n",
      "Epoch: 91 - Batch: 85952 - Loss: 0.028805 - Time:197.97299242019653\n",
      "Epoch: 91 - Batch: 90048 - Loss: 0.059672 - Time:207.38876152038574\n",
      "Epoch: 91 - Batch: 94144 - Loss: 0.043923 - Time:216.80255031585693\n",
      "Epoch: 91 - Batch: 98240 - Loss: 0.221129 - Time:226.21619486808777\n",
      "Epoch: 91 - Batch: 102336 - Loss: 0.134879 - Time:235.64741945266724\n",
      "Epoch: 91 - Batch: 106432 - Loss: 0.086433 - Time:245.06161952018738\n",
      "Epoch: 91 - Batch: 110528 - Loss: 0.114613 - Time:254.475031375885\n",
      "Epoch: 91 - Batch: 114624 - Loss: 0.029674 - Time:263.890456199646\n",
      "Epoch: 91 - Batch: 118720 - Loss: 0.020561 - Time:273.3211739063263\n",
      "Epoch: 91 - Batch: 122816 - Loss: 0.050384 - Time:282.73499274253845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 - Batch: 126912 - Loss: 0.045398 - Time:292.14849519729614\n",
      "Epoch: 91 - Batch: 131008 - Loss: 0.136917 - Time:301.5810775756836\n",
      "Epoch: 91 - Batch: 135104 - Loss: 0.329729 - Time:310.9952304363251\n",
      "Epoch: 91 - Batch: 139200 - Loss: 0.027150 - Time:320.4084973335266\n",
      "Epoch: 91 - Batch: 143296 - Loss: 0.095013 - Time:329.82299733161926\n",
      "Epoch: 91 - Batch: 147392 - Loss: 0.199599 - Time:339.23641443252563\n",
      "Epoch: 91 - Batch: 151488 - Loss: 0.026404 - Time:348.65218138694763\n",
      "Epoch: 92 - Batch: 4032 - Loss: 0.030563 - Time:9.57466173171997\n",
      "Epoch: 92 - Batch: 8128 - Loss: 0.102818 - Time:18.99150037765503\n",
      "Epoch: 92 - Batch: 12224 - Loss: 0.042228 - Time:28.43429732322693\n",
      "Epoch: 92 - Batch: 16320 - Loss: 0.058534 - Time:37.84827399253845\n",
      "Epoch: 92 - Batch: 20416 - Loss: 0.213977 - Time:47.26348829269409\n",
      "Epoch: 92 - Batch: 24512 - Loss: 0.094274 - Time:56.67705249786377\n",
      "Epoch: 92 - Batch: 28608 - Loss: 0.034899 - Time:66.1086573600769\n",
      "Epoch: 92 - Batch: 32704 - Loss: 0.022472 - Time:75.52413940429688\n",
      "Epoch: 92 - Batch: 36800 - Loss: 0.106757 - Time:84.94015312194824\n",
      "Epoch: 92 - Batch: 40896 - Loss: 0.129034 - Time:94.37260723114014\n",
      "Epoch: 92 - Batch: 44992 - Loss: 0.027183 - Time:103.7873911857605\n",
      "Epoch: 92 - Batch: 49088 - Loss: 0.054376 - Time:113.20291447639465\n",
      "Epoch: 92 - Batch: 53184 - Loss: 0.034072 - Time:122.61797738075256\n",
      "Epoch: 92 - Batch: 57280 - Loss: 0.070528 - Time:132.03441715240479\n",
      "Epoch: 92 - Batch: 61376 - Loss: 0.082550 - Time:141.4485468864441\n",
      "Epoch: 92 - Batch: 65472 - Loss: 0.015045 - Time:150.8616840839386\n",
      "Epoch: 92 - Batch: 69568 - Loss: 0.217626 - Time:160.27604174613953\n",
      "Epoch: 92 - Batch: 73664 - Loss: 0.162558 - Time:169.7049117088318\n",
      "Epoch: 92 - Batch: 77760 - Loss: 0.045616 - Time:179.11692261695862\n",
      "Epoch: 92 - Batch: 81856 - Loss: 0.026432 - Time:188.53053879737854\n",
      "Epoch: 92 - Batch: 85952 - Loss: 0.233668 - Time:197.94345664978027\n",
      "Epoch: 92 - Batch: 90048 - Loss: 0.186832 - Time:207.37356305122375\n",
      "Epoch: 92 - Batch: 94144 - Loss: 0.022284 - Time:216.78853702545166\n",
      "Epoch: 92 - Batch: 98240 - Loss: 0.057255 - Time:226.20216155052185\n",
      "Epoch: 92 - Batch: 102336 - Loss: 0.018766 - Time:235.63317131996155\n",
      "Epoch: 92 - Batch: 106432 - Loss: 0.021182 - Time:245.04833388328552\n",
      "Epoch: 92 - Batch: 110528 - Loss: 0.147261 - Time:254.4641993045807\n",
      "Epoch: 92 - Batch: 114624 - Loss: 0.025273 - Time:263.8788766860962\n",
      "Epoch: 92 - Batch: 118720 - Loss: 0.121534 - Time:273.2958860397339\n",
      "Epoch: 92 - Batch: 122816 - Loss: 0.032210 - Time:282.7087824344635\n",
      "Epoch: 92 - Batch: 126912 - Loss: 0.106091 - Time:292.1230585575104\n",
      "Epoch: 92 - Batch: 131008 - Loss: 0.023504 - Time:301.5390248298645\n",
      "Epoch: 92 - Batch: 135104 - Loss: 0.183600 - Time:310.97232365608215\n",
      "Epoch: 92 - Batch: 139200 - Loss: 0.020043 - Time:320.38616394996643\n",
      "Epoch: 92 - Batch: 143296 - Loss: 0.019397 - Time:329.800350189209\n",
      "Epoch: 92 - Batch: 147392 - Loss: 0.278969 - Time:339.2165312767029\n",
      "Epoch: 92 - Batch: 151488 - Loss: 0.045375 - Time:348.6470193862915\n",
      "Epoch: 93 - Batch: 4032 - Loss: 0.070294 - Time:9.58227252960205\n",
      "Epoch: 93 - Batch: 8128 - Loss: 0.034976 - Time:19.001984119415283\n",
      "Epoch: 93 - Batch: 12224 - Loss: 0.056550 - Time:28.44520902633667\n",
      "Epoch: 93 - Batch: 16320 - Loss: 0.088116 - Time:37.86057376861572\n",
      "Epoch: 93 - Batch: 20416 - Loss: 0.024518 - Time:47.27561593055725\n",
      "Epoch: 93 - Batch: 24512 - Loss: 0.018547 - Time:56.68818497657776\n",
      "Epoch: 93 - Batch: 28608 - Loss: 0.026612 - Time:66.10295033454895\n",
      "Epoch: 93 - Batch: 32704 - Loss: 0.015216 - Time:75.5157117843628\n",
      "Epoch: 93 - Batch: 36800 - Loss: 0.057989 - Time:84.92777633666992\n",
      "Epoch: 93 - Batch: 40896 - Loss: 0.040172 - Time:94.34030413627625\n",
      "Epoch: 93 - Batch: 44992 - Loss: 0.100656 - Time:103.77017307281494\n",
      "Epoch: 93 - Batch: 49088 - Loss: 0.020803 - Time:113.18394732475281\n",
      "Epoch: 93 - Batch: 53184 - Loss: 0.050709 - Time:122.59880352020264\n",
      "Epoch: 93 - Batch: 57280 - Loss: 0.078614 - Time:132.0129668712616\n",
      "Epoch: 93 - Batch: 61376 - Loss: 0.022227 - Time:141.4448685646057\n",
      "Epoch: 93 - Batch: 65472 - Loss: 0.040981 - Time:150.86133670806885\n",
      "Epoch: 93 - Batch: 69568 - Loss: 0.063879 - Time:160.27602910995483\n",
      "Epoch: 93 - Batch: 73664 - Loss: 0.187969 - Time:169.7064688205719\n",
      "Epoch: 93 - Batch: 77760 - Loss: 0.026697 - Time:179.1208062171936\n",
      "Epoch: 93 - Batch: 81856 - Loss: 0.160530 - Time:188.5330309867859\n",
      "Epoch: 93 - Batch: 85952 - Loss: 0.047277 - Time:197.9458930492401\n",
      "Epoch: 93 - Batch: 90048 - Loss: 0.184991 - Time:207.35840487480164\n",
      "Epoch: 93 - Batch: 94144 - Loss: 0.063716 - Time:216.77097916603088\n",
      "Epoch: 93 - Batch: 98240 - Loss: 0.046133 - Time:226.1839644908905\n",
      "Epoch: 93 - Batch: 102336 - Loss: 0.062733 - Time:235.5962872505188\n",
      "Epoch: 93 - Batch: 106432 - Loss: 0.033061 - Time:245.02542996406555\n",
      "Epoch: 93 - Batch: 110528 - Loss: 0.118834 - Time:254.4380328655243\n",
      "Epoch: 93 - Batch: 114624 - Loss: 0.144542 - Time:263.8515980243683\n",
      "Epoch: 93 - Batch: 118720 - Loss: 0.025331 - Time:273.26442193984985\n",
      "Epoch: 93 - Batch: 122816 - Loss: 0.136106 - Time:282.69352889060974\n",
      "Epoch: 93 - Batch: 126912 - Loss: 0.132430 - Time:292.1063461303711\n",
      "Epoch: 93 - Batch: 131008 - Loss: 0.073272 - Time:301.5195598602295\n",
      "Epoch: 93 - Batch: 135104 - Loss: 0.372275 - Time:310.94856572151184\n",
      "Epoch: 93 - Batch: 139200 - Loss: 0.053476 - Time:320.3623161315918\n",
      "Epoch: 93 - Batch: 143296 - Loss: 0.170804 - Time:329.77495527267456\n",
      "Epoch: 93 - Batch: 147392 - Loss: 0.035697 - Time:339.189825296402\n",
      "Epoch: 93 - Batch: 151488 - Loss: 0.311589 - Time:348.60388922691345\n",
      "Epoch: 94 - Batch: 4032 - Loss: 0.031752 - Time:9.57547926902771\n",
      "Epoch: 94 - Batch: 8128 - Loss: 0.073862 - Time:19.018762588500977\n",
      "Epoch: 94 - Batch: 12224 - Loss: 0.032360 - Time:28.433329820632935\n",
      "Epoch: 94 - Batch: 16320 - Loss: 0.044274 - Time:37.84564137458801\n",
      "Epoch: 94 - Batch: 20416 - Loss: 0.033185 - Time:47.26054072380066\n",
      "Epoch: 94 - Batch: 24512 - Loss: 0.117337 - Time:56.67503118515015\n",
      "Epoch: 94 - Batch: 28608 - Loss: 0.163693 - Time:66.08821725845337\n",
      "Epoch: 94 - Batch: 32704 - Loss: 0.043237 - Time:75.50151681900024\n",
      "Epoch: 94 - Batch: 36800 - Loss: 0.046137 - Time:84.91295647621155\n",
      "Epoch: 94 - Batch: 40896 - Loss: 0.021852 - Time:94.34340071678162\n",
      "Epoch: 94 - Batch: 44992 - Loss: 0.063754 - Time:103.75415372848511\n",
      "Epoch: 94 - Batch: 49088 - Loss: 0.043630 - Time:113.16416215896606\n",
      "Epoch: 94 - Batch: 53184 - Loss: 0.028539 - Time:122.57633662223816\n",
      "Epoch: 94 - Batch: 57280 - Loss: 0.038181 - Time:132.0051486492157\n",
      "Epoch: 94 - Batch: 61376 - Loss: 0.113109 - Time:141.41961479187012\n",
      "Epoch: 94 - Batch: 65472 - Loss: 0.281088 - Time:150.8333592414856\n",
      "Epoch: 94 - Batch: 69568 - Loss: 0.068077 - Time:160.2627818584442\n",
      "Epoch: 94 - Batch: 73664 - Loss: 0.038005 - Time:169.67531728744507\n",
      "Epoch: 94 - Batch: 77760 - Loss: 0.199539 - Time:179.0894114971161\n",
      "Epoch: 94 - Batch: 81856 - Loss: 0.023286 - Time:188.5026137828827\n",
      "Epoch: 94 - Batch: 85952 - Loss: 0.366942 - Time:197.91628432273865\n",
      "Epoch: 94 - Batch: 90048 - Loss: 0.049766 - Time:207.3298578262329\n",
      "Epoch: 94 - Batch: 94144 - Loss: 0.231368 - Time:216.74450492858887\n",
      "Epoch: 94 - Batch: 98240 - Loss: 0.174757 - Time:226.15822052955627\n",
      "Epoch: 94 - Batch: 102336 - Loss: 0.036917 - Time:235.58766508102417\n",
      "Epoch: 94 - Batch: 106432 - Loss: 0.045288 - Time:244.99895358085632\n",
      "Epoch: 94 - Batch: 110528 - Loss: 0.101627 - Time:254.41014742851257\n",
      "Epoch: 94 - Batch: 114624 - Loss: 0.020335 - Time:263.82402300834656\n",
      "Epoch: 94 - Batch: 118720 - Loss: 0.040198 - Time:273.2557728290558\n",
      "Epoch: 94 - Batch: 122816 - Loss: 0.063495 - Time:282.6705460548401\n",
      "Epoch: 94 - Batch: 126912 - Loss: 0.079155 - Time:292.0842053890228\n",
      "Epoch: 94 - Batch: 131008 - Loss: 0.405323 - Time:301.51639223098755\n",
      "Epoch: 94 - Batch: 135104 - Loss: 0.028479 - Time:310.9309432506561\n",
      "Epoch: 94 - Batch: 139200 - Loss: 0.068772 - Time:320.34342408180237\n",
      "Epoch: 94 - Batch: 143296 - Loss: 0.077041 - Time:329.7564902305603\n",
      "Epoch: 94 - Batch: 147392 - Loss: 0.352644 - Time:339.16847348213196\n",
      "Epoch: 94 - Batch: 151488 - Loss: 0.228605 - Time:348.5800223350525\n",
      "Epoch: 95 - Batch: 4032 - Loss: 0.251518 - Time:9.609961032867432\n",
      "Epoch: 95 - Batch: 8128 - Loss: 0.080866 - Time:19.027608633041382\n",
      "Epoch: 95 - Batch: 12224 - Loss: 0.377169 - Time:28.447180032730103\n",
      "Epoch: 95 - Batch: 16320 - Loss: 0.069898 - Time:37.86185312271118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 - Batch: 20416 - Loss: 0.045294 - Time:47.27451682090759\n",
      "Epoch: 95 - Batch: 24512 - Loss: 0.279192 - Time:56.690237045288086\n",
      "Epoch: 95 - Batch: 28608 - Loss: 0.077461 - Time:66.10448789596558\n",
      "Epoch: 95 - Batch: 32704 - Loss: 0.079831 - Time:75.52002048492432\n",
      "Epoch: 95 - Batch: 36800 - Loss: 0.049170 - Time:84.95032405853271\n",
      "Epoch: 95 - Batch: 40896 - Loss: 0.035838 - Time:94.36286067962646\n",
      "Epoch: 95 - Batch: 44992 - Loss: 0.037414 - Time:103.77632284164429\n",
      "Epoch: 95 - Batch: 49088 - Loss: 0.050429 - Time:113.18846726417542\n",
      "Epoch: 95 - Batch: 53184 - Loss: 0.246711 - Time:122.60201644897461\n",
      "Epoch: 95 - Batch: 57280 - Loss: 0.053213 - Time:132.03136658668518\n",
      "Epoch: 95 - Batch: 61376 - Loss: 0.061005 - Time:141.44597744941711\n",
      "Epoch: 95 - Batch: 65472 - Loss: 0.027268 - Time:150.85914182662964\n",
      "Epoch: 95 - Batch: 69568 - Loss: 0.146969 - Time:160.29057908058167\n",
      "Epoch: 95 - Batch: 73664 - Loss: 0.086414 - Time:169.7052915096283\n",
      "Epoch: 95 - Batch: 77760 - Loss: 0.032095 - Time:179.1196858882904\n",
      "Epoch: 95 - Batch: 81856 - Loss: 0.101682 - Time:188.53661274909973\n",
      "Epoch: 95 - Batch: 85952 - Loss: 0.157648 - Time:197.95179510116577\n",
      "Epoch: 95 - Batch: 90048 - Loss: 0.023975 - Time:207.3648645877838\n",
      "Epoch: 95 - Batch: 94144 - Loss: 0.045895 - Time:216.77973341941833\n",
      "Epoch: 95 - Batch: 98240 - Loss: 0.022605 - Time:226.19226717948914\n",
      "Epoch: 95 - Batch: 102336 - Loss: 0.039963 - Time:235.62228679656982\n",
      "Epoch: 95 - Batch: 106432 - Loss: 0.087448 - Time:245.03529119491577\n",
      "Epoch: 95 - Batch: 110528 - Loss: 0.497004 - Time:254.45177102088928\n",
      "Epoch: 95 - Batch: 114624 - Loss: 0.126477 - Time:263.8657248020172\n",
      "Epoch: 95 - Batch: 118720 - Loss: 0.202770 - Time:273.2972991466522\n",
      "Epoch: 95 - Batch: 122816 - Loss: 0.143872 - Time:282.71015882492065\n",
      "Epoch: 95 - Batch: 126912 - Loss: 0.033915 - Time:292.1236426830292\n",
      "Epoch: 95 - Batch: 131008 - Loss: 0.028179 - Time:301.5539507865906\n",
      "Epoch: 95 - Batch: 135104 - Loss: 0.082290 - Time:310.9663507938385\n",
      "Epoch: 95 - Batch: 139200 - Loss: 0.187307 - Time:320.37921357154846\n",
      "Epoch: 95 - Batch: 143296 - Loss: 0.177681 - Time:329.792941570282\n",
      "Epoch: 95 - Batch: 147392 - Loss: 0.055001 - Time:339.2065899372101\n",
      "Epoch: 95 - Batch: 151488 - Loss: 0.200909 - Time:348.6202530860901\n",
      "Epoch: 96 - Batch: 4032 - Loss: 0.019096 - Time:9.580005645751953\n",
      "Epoch: 96 - Batch: 8128 - Loss: 0.136628 - Time:19.024968147277832\n",
      "Epoch: 96 - Batch: 12224 - Loss: 0.031368 - Time:28.443548440933228\n",
      "Epoch: 96 - Batch: 16320 - Loss: 0.039852 - Time:37.86110520362854\n",
      "Epoch: 96 - Batch: 20416 - Loss: 0.123701 - Time:47.27667713165283\n",
      "Epoch: 96 - Batch: 24512 - Loss: 0.048132 - Time:56.708274364471436\n",
      "Epoch: 96 - Batch: 28608 - Loss: 0.015449 - Time:66.12204241752625\n",
      "Epoch: 96 - Batch: 32704 - Loss: 0.031164 - Time:75.53723073005676\n",
      "Epoch: 96 - Batch: 36800 - Loss: 0.055018 - Time:84.96966576576233\n",
      "Epoch: 96 - Batch: 40896 - Loss: 0.017443 - Time:94.38252568244934\n",
      "Epoch: 96 - Batch: 44992 - Loss: 0.142219 - Time:103.79701352119446\n",
      "Epoch: 96 - Batch: 49088 - Loss: 0.156539 - Time:113.21148991584778\n",
      "Epoch: 96 - Batch: 53184 - Loss: 0.175983 - Time:122.62611174583435\n",
      "Epoch: 96 - Batch: 57280 - Loss: 0.035917 - Time:132.04285645484924\n",
      "Epoch: 96 - Batch: 61376 - Loss: 0.046675 - Time:141.45942163467407\n",
      "Epoch: 96 - Batch: 65472 - Loss: 0.065792 - Time:150.8772189617157\n",
      "Epoch: 96 - Batch: 69568 - Loss: 0.028589 - Time:160.31028199195862\n",
      "Epoch: 96 - Batch: 73664 - Loss: 0.016645 - Time:169.72473573684692\n",
      "Epoch: 96 - Batch: 77760 - Loss: 0.115200 - Time:179.13981103897095\n",
      "Epoch: 96 - Batch: 81856 - Loss: 0.048129 - Time:188.55262303352356\n",
      "Epoch: 96 - Batch: 85952 - Loss: 0.073311 - Time:197.9668309688568\n",
      "Epoch: 96 - Batch: 90048 - Loss: 0.026485 - Time:207.39756608009338\n",
      "Epoch: 96 - Batch: 94144 - Loss: 0.110097 - Time:216.8106174468994\n",
      "Epoch: 96 - Batch: 98240 - Loss: 0.049072 - Time:226.22505450248718\n",
      "Epoch: 96 - Batch: 102336 - Loss: 0.087497 - Time:235.65509748458862\n",
      "Epoch: 96 - Batch: 106432 - Loss: 0.063804 - Time:245.0704107284546\n",
      "Epoch: 96 - Batch: 110528 - Loss: 0.329353 - Time:254.48244261741638\n",
      "Epoch: 96 - Batch: 114624 - Loss: 0.110703 - Time:263.89823365211487\n",
      "Epoch: 96 - Batch: 118720 - Loss: 0.095659 - Time:273.3126826286316\n",
      "Epoch: 96 - Batch: 122816 - Loss: 0.071433 - Time:282.7284891605377\n",
      "Epoch: 96 - Batch: 126912 - Loss: 0.024760 - Time:292.14256477355957\n",
      "Epoch: 96 - Batch: 131008 - Loss: 0.033758 - Time:301.55640625953674\n",
      "Epoch: 96 - Batch: 135104 - Loss: 0.150310 - Time:310.9878988265991\n",
      "Epoch: 96 - Batch: 139200 - Loss: 0.179628 - Time:320.4015552997589\n",
      "Epoch: 96 - Batch: 143296 - Loss: 0.108258 - Time:329.8138818740845\n",
      "Epoch: 96 - Batch: 147392 - Loss: 0.026948 - Time:339.2280185222626\n",
      "Epoch: 96 - Batch: 151488 - Loss: 0.106939 - Time:348.65876150131226\n",
      "Epoch: 97 - Batch: 4032 - Loss: 0.025354 - Time:9.575338363647461\n",
      "Epoch: 97 - Batch: 8128 - Loss: 0.114704 - Time:18.990880489349365\n",
      "Epoch: 97 - Batch: 12224 - Loss: 0.039930 - Time:28.432812452316284\n",
      "Epoch: 97 - Batch: 16320 - Loss: 0.044600 - Time:37.84732937812805\n",
      "Epoch: 97 - Batch: 20416 - Loss: 0.146191 - Time:47.261231660842896\n",
      "Epoch: 97 - Batch: 24512 - Loss: 0.101648 - Time:56.692482233047485\n",
      "Epoch: 97 - Batch: 28608 - Loss: 0.032741 - Time:66.10595941543579\n",
      "Epoch: 97 - Batch: 32704 - Loss: 0.225254 - Time:75.51928329467773\n",
      "Epoch: 97 - Batch: 36800 - Loss: 0.200128 - Time:84.93146681785583\n",
      "Epoch: 97 - Batch: 40896 - Loss: 0.391380 - Time:94.34331011772156\n",
      "Epoch: 97 - Batch: 44992 - Loss: 0.336396 - Time:103.75676393508911\n",
      "Epoch: 97 - Batch: 49088 - Loss: 0.155487 - Time:113.1685631275177\n",
      "Epoch: 97 - Batch: 53184 - Loss: 0.017344 - Time:122.57997274398804\n",
      "Epoch: 97 - Batch: 57280 - Loss: 0.087969 - Time:132.01159715652466\n",
      "Epoch: 97 - Batch: 61376 - Loss: 0.059950 - Time:141.4233341217041\n",
      "Epoch: 97 - Batch: 65472 - Loss: 0.036380 - Time:150.83695530891418\n",
      "Epoch: 97 - Batch: 69568 - Loss: 0.078639 - Time:160.24939823150635\n",
      "Epoch: 97 - Batch: 73664 - Loss: 0.022134 - Time:169.67988777160645\n",
      "Epoch: 97 - Batch: 77760 - Loss: 0.056783 - Time:179.09428238868713\n",
      "Epoch: 97 - Batch: 81856 - Loss: 0.037370 - Time:188.50718545913696\n",
      "Epoch: 97 - Batch: 85952 - Loss: 0.042125 - Time:197.93973112106323\n",
      "Epoch: 97 - Batch: 90048 - Loss: 0.095827 - Time:207.35454654693604\n",
      "Epoch: 97 - Batch: 94144 - Loss: 0.151747 - Time:216.77040076255798\n",
      "Epoch: 97 - Batch: 98240 - Loss: 0.075547 - Time:226.182377576828\n",
      "Epoch: 97 - Batch: 102336 - Loss: 0.123287 - Time:235.59894490242004\n",
      "Epoch: 97 - Batch: 106432 - Loss: 0.081832 - Time:245.01243376731873\n",
      "Epoch: 97 - Batch: 110528 - Loss: 0.182182 - Time:254.42723035812378\n",
      "Epoch: 97 - Batch: 114624 - Loss: 0.112950 - Time:263.84001207351685\n",
      "Epoch: 97 - Batch: 118720 - Loss: 0.148633 - Time:273.27131724357605\n",
      "Epoch: 97 - Batch: 122816 - Loss: 0.046236 - Time:282.6845920085907\n",
      "Epoch: 97 - Batch: 126912 - Loss: 0.078077 - Time:292.1006028652191\n",
      "Epoch: 97 - Batch: 131008 - Loss: 0.051854 - Time:301.5153331756592\n",
      "Epoch: 97 - Batch: 135104 - Loss: 0.107331 - Time:310.9329779148102\n",
      "Epoch: 97 - Batch: 139200 - Loss: 0.029987 - Time:320.3653974533081\n",
      "Epoch: 97 - Batch: 143296 - Loss: 0.037880 - Time:329.7825586795807\n",
      "Epoch: 97 - Batch: 147392 - Loss: 0.099710 - Time:339.2134094238281\n",
      "Epoch: 97 - Batch: 151488 - Loss: 0.558208 - Time:348.6263270378113\n",
      "Epoch: 98 - Batch: 4032 - Loss: 0.216668 - Time:9.572453022003174\n",
      "Epoch: 98 - Batch: 8128 - Loss: 0.105061 - Time:18.989072799682617\n",
      "Epoch: 98 - Batch: 12224 - Loss: 0.095670 - Time:28.430374145507812\n",
      "Epoch: 98 - Batch: 16320 - Loss: 0.129101 - Time:37.845683574676514\n",
      "Epoch: 98 - Batch: 20416 - Loss: 0.108707 - Time:47.26024031639099\n",
      "Epoch: 98 - Batch: 24512 - Loss: 0.075865 - Time:56.673938035964966\n",
      "Epoch: 98 - Batch: 28608 - Loss: 0.150983 - Time:66.08893966674805\n",
      "Epoch: 98 - Batch: 32704 - Loss: 0.028222 - Time:75.50411081314087\n",
      "Epoch: 98 - Batch: 36800 - Loss: 0.067735 - Time:84.9185848236084\n",
      "Epoch: 98 - Batch: 40896 - Loss: 0.019505 - Time:94.3322479724884\n",
      "Epoch: 98 - Batch: 44992 - Loss: 0.079452 - Time:103.76386857032776\n",
      "Epoch: 98 - Batch: 49088 - Loss: 0.031699 - Time:113.17718863487244\n",
      "Epoch: 98 - Batch: 53184 - Loss: 0.154327 - Time:122.58970046043396\n",
      "Epoch: 98 - Batch: 57280 - Loss: 0.064725 - Time:132.00284838676453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 - Batch: 61376 - Loss: 0.106359 - Time:141.43098378181458\n",
      "Epoch: 98 - Batch: 65472 - Loss: 0.093846 - Time:150.84435391426086\n",
      "Epoch: 98 - Batch: 69568 - Loss: 0.029850 - Time:160.2571566104889\n",
      "Epoch: 98 - Batch: 73664 - Loss: 0.095764 - Time:169.68763613700867\n",
      "Epoch: 98 - Batch: 77760 - Loss: 0.044316 - Time:179.09974813461304\n",
      "Epoch: 98 - Batch: 81856 - Loss: 0.178024 - Time:188.51147270202637\n",
      "Epoch: 98 - Batch: 85952 - Loss: 0.037120 - Time:197.92380690574646\n",
      "Epoch: 98 - Batch: 90048 - Loss: 0.059585 - Time:207.336275100708\n",
      "Epoch: 98 - Batch: 94144 - Loss: 0.037634 - Time:216.74654579162598\n",
      "Epoch: 98 - Batch: 98240 - Loss: 0.095407 - Time:226.15892505645752\n",
      "Epoch: 98 - Batch: 102336 - Loss: 0.074807 - Time:235.57213878631592\n",
      "Epoch: 98 - Batch: 106432 - Loss: 0.025861 - Time:245.0008761882782\n",
      "Epoch: 98 - Batch: 110528 - Loss: 0.136065 - Time:254.41507983207703\n",
      "Epoch: 98 - Batch: 114624 - Loss: 0.082256 - Time:263.8270146846771\n",
      "Epoch: 98 - Batch: 118720 - Loss: 0.170119 - Time:273.24127888679504\n",
      "Epoch: 98 - Batch: 122816 - Loss: 0.070807 - Time:282.6731035709381\n",
      "Epoch: 98 - Batch: 126912 - Loss: 0.046672 - Time:292.08549308776855\n",
      "Epoch: 98 - Batch: 131008 - Loss: 0.128215 - Time:301.4998154640198\n",
      "Epoch: 98 - Batch: 135104 - Loss: 0.028333 - Time:310.9315764904022\n",
      "Epoch: 98 - Batch: 139200 - Loss: 0.195758 - Time:320.34573459625244\n",
      "Epoch: 98 - Batch: 143296 - Loss: 0.139707 - Time:329.75889015197754\n",
      "Epoch: 98 - Batch: 147392 - Loss: 0.053834 - Time:339.17338275909424\n",
      "Epoch: 98 - Batch: 151488 - Loss: 0.202735 - Time:348.58573484420776\n",
      "Epoch: 99 - Batch: 4032 - Loss: 0.044431 - Time:9.578793048858643\n",
      "Epoch: 99 - Batch: 8128 - Loss: 0.255442 - Time:19.03038239479065\n",
      "Epoch: 99 - Batch: 12224 - Loss: 0.042872 - Time:28.447453498840332\n",
      "Epoch: 99 - Batch: 16320 - Loss: 0.076478 - Time:37.86203360557556\n",
      "Epoch: 99 - Batch: 20416 - Loss: 0.049060 - Time:47.27520132064819\n",
      "Epoch: 99 - Batch: 24512 - Loss: 0.016612 - Time:56.69261288642883\n",
      "Epoch: 99 - Batch: 28608 - Loss: 0.213625 - Time:66.10787391662598\n",
      "Epoch: 99 - Batch: 32704 - Loss: 0.027080 - Time:75.52260565757751\n",
      "Epoch: 99 - Batch: 36800 - Loss: 0.235882 - Time:84.93738675117493\n",
      "Epoch: 99 - Batch: 40896 - Loss: 0.021103 - Time:94.36899423599243\n",
      "Epoch: 99 - Batch: 44992 - Loss: 0.475887 - Time:103.78365278244019\n",
      "Epoch: 99 - Batch: 49088 - Loss: 0.089575 - Time:113.20097231864929\n",
      "Epoch: 99 - Batch: 53184 - Loss: 0.071089 - Time:122.61637830734253\n",
      "Epoch: 99 - Batch: 57280 - Loss: 0.125130 - Time:132.04969143867493\n",
      "Epoch: 99 - Batch: 61376 - Loss: 0.126083 - Time:141.46439790725708\n",
      "Epoch: 99 - Batch: 65472 - Loss: 0.028323 - Time:150.87776374816895\n",
      "Epoch: 99 - Batch: 69568 - Loss: 0.065355 - Time:160.3089406490326\n",
      "Epoch: 99 - Batch: 73664 - Loss: 0.113788 - Time:169.72371435165405\n",
      "Epoch: 99 - Batch: 77760 - Loss: 0.032117 - Time:179.13998746871948\n",
      "Epoch: 99 - Batch: 81856 - Loss: 0.159207 - Time:188.55659341812134\n",
      "Epoch: 99 - Batch: 85952 - Loss: 0.038863 - Time:197.9765419960022\n",
      "Epoch: 99 - Batch: 90048 - Loss: 0.018473 - Time:207.39140605926514\n",
      "Epoch: 99 - Batch: 94144 - Loss: 0.058820 - Time:216.8072545528412\n",
      "Epoch: 99 - Batch: 98240 - Loss: 0.157044 - Time:226.222589969635\n",
      "Epoch: 99 - Batch: 102336 - Loss: 0.090857 - Time:235.652161359787\n",
      "Epoch: 99 - Batch: 106432 - Loss: 0.144692 - Time:245.06889033317566\n",
      "Epoch: 99 - Batch: 110528 - Loss: 0.226516 - Time:254.48372220993042\n",
      "Epoch: 99 - Batch: 114624 - Loss: 0.026278 - Time:263.90012645721436\n",
      "Epoch: 99 - Batch: 118720 - Loss: 0.102630 - Time:273.3343858718872\n",
      "Epoch: 99 - Batch: 122816 - Loss: 0.023836 - Time:282.75081753730774\n",
      "Epoch: 99 - Batch: 126912 - Loss: 0.024852 - Time:292.16722679138184\n",
      "Epoch: 99 - Batch: 131008 - Loss: 0.068827 - Time:301.6011173725128\n",
      "Epoch: 99 - Batch: 135104 - Loss: 0.075146 - Time:311.0163679122925\n",
      "Epoch: 99 - Batch: 139200 - Loss: 0.048722 - Time:320.4325087070465\n",
      "Epoch: 99 - Batch: 143296 - Loss: 0.017699 - Time:329.849750995636\n",
      "Epoch: 99 - Batch: 147392 - Loss: 0.022856 - Time:339.266450881958\n",
      "Epoch: 99 - Batch: 151488 - Loss: 0.142476 - Time:348.6813316345215\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 512\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "embedder = Embedder(embedding_size=embedding_size)\n",
    "arcface = ArcFaceLoss(num_classes=num_classe, embedding_size=embedding_size,margin=0.3, scale=30.0)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    embedder = nn.DataParallel(embedder)\n",
    "    arcface = nn.DataParallel(arcface)\n",
    "embedder = embedder.to(device)\n",
    "arcface = arcface.to(device)\n",
    "\n",
    "optimizer = optim.Adam(embedder.parameters(), lr=1e-3 ) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.9)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    e_time = time.time()\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        embedder.train()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embeddings = embedder(images)\n",
    "        \n",
    "        logits = arcface(embeddings, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "        if (i+1) % 64 == 0:\n",
    "            print(f'Epoch: {epoch} - Batch: {i*batch_size} - Loss: {loss:.6f} - Time:{time.time() - e_time}')\n",
    "            with open('/home/ielab/project/samples/masked_loss.txt', 'a') as file:\n",
    "                file.write(f'{loss:.6f}\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d909c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d6fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebdd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ff673d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): Embedder(\n",
      "    (model): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (classifier): Linear(in_features=1000, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedder = torch.load(\"masked_model.pth\", map_location=device)\n",
    "print(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad3e48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  정확도 측정\n",
    "train_results = []\n",
    "train_labels = []\n",
    "test_results = []\n",
    "test_labels = []\n",
    "\n",
    "embedder.eval()\n",
    "with torch.no_grad():\n",
    "  for img, label in trainloader:\n",
    "    img = img.cuda()\n",
    "    train_results.append(embedder(img).cpu().detach().numpy())\n",
    "    train_labels.append(label)\n",
    "\n",
    "train_results = np.concatenate(train_results)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "embedder.eval()\n",
    "with torch.no_grad():\n",
    "  for img, label in testloader:\n",
    "    img = img.cuda()\n",
    "    test_results.append(embedder(img).cpu().detach().numpy())\n",
    "    test_labels.append(label)\n",
    "\n",
    "test_results = np.concatenate(test_results)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a022c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.96643107249384\n",
      "test acc : 0.6540348327147195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# kNN 모델 선언\n",
    "k = 50\n",
    "model = KNeighborsClassifier(n_neighbors = k)\n",
    "# 모델 학습\n",
    "model.fit(train_results, train_labels)\n",
    "#knn 검증\n",
    "train_pred = model.predict(train_results)\n",
    "train_acc = (train_pred == train_labels).mean()\n",
    "with open('/home/ielab/project/samples/gan_train_acc.txt', 'a') as file:\n",
    "    file.write(f'{train_acc:.6f}\\n')\n",
    "print(f'train acc : {train_acc}')\n",
    "test_pred = model.predict(test_results)\n",
    "test_acc = (test_pred == test_labels).mean()\n",
    "with open('/home/ielab/project/samples/gan_test_acc.txt', 'a') as file:\n",
    "    file.write(f'{test_acc:.6f}\\n')\n",
    "print(f'test acc : {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bd6f217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ielab/anaconda3/envs/arcface/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# F1-Score\n",
    "f1 = metrics.classification_report(test_labels, test_pred)\n",
    "with open('/home/ielab/project/samples/gan_test_f1.txt', 'a') as file:\n",
    "    file.write(f'{f1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35f49dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        13\n",
      "           1       1.00      0.58      0.73        19\n",
      "           2       0.90      0.86      0.88        21\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.36      0.67      0.47         6\n",
      "           5       0.93      0.93      0.93        15\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       1.00      0.68      0.81        19\n",
      "           8       1.00      0.67      0.80        24\n",
      "           9       0.13      0.67      0.22        52\n",
      "          10       0.88      0.75      0.81        20\n",
      "          11       1.00      0.71      0.83        17\n",
      "          12       1.00      0.67      0.80         9\n",
      "          13       0.58      0.50      0.54        14\n",
      "          14       0.67      0.20      0.31        10\n",
      "          15       1.00      0.53      0.69        17\n",
      "          16       0.12      0.20      0.15         5\n",
      "          17       1.00      0.86      0.92         7\n",
      "          18       0.18      0.33      0.24         6\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.93      0.93      0.93        14\n",
      "          21       0.93      0.70      0.80        20\n",
      "          22       0.86      0.75      0.80        16\n",
      "          23       0.67      0.67      0.67        18\n",
      "          24       0.89      0.77      0.83        22\n",
      "          25       0.62      0.57      0.59        14\n",
      "          26       1.00      0.79      0.88        14\n",
      "          27       1.00      0.74      0.85        23\n",
      "          28       0.60      0.60      0.60        10\n",
      "          29       1.00      0.60      0.75        10\n",
      "          30       1.00      0.91      0.95        23\n",
      "          31       0.94      0.88      0.91        17\n",
      "          32       0.94      0.89      0.92        19\n",
      "          33       0.84      0.80      0.82        20\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       0.27      0.43      0.33         7\n",
      "          36       0.95      0.83      0.88        23\n",
      "          37       1.00      0.80      0.89        10\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       1.00      0.44      0.62         9\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.15      0.71      0.25        21\n",
      "          44       1.00      0.86      0.92         7\n",
      "          45       1.00      0.79      0.88        14\n",
      "          46       1.00      0.83      0.91         6\n",
      "          47       1.00      0.59      0.74        17\n",
      "          48       0.67      0.50      0.57        20\n",
      "          49       1.00      0.58      0.74        12\n",
      "          50       1.00      0.57      0.73        14\n",
      "          51       0.82      0.56      0.67        16\n",
      "          52       1.00      0.78      0.88        18\n",
      "          53       1.00      0.91      0.95        23\n",
      "          54       1.00      0.73      0.84        22\n",
      "          55       0.75      0.88      0.81        17\n",
      "          56       0.53      0.67      0.59        15\n",
      "          57       1.00      0.55      0.71        22\n",
      "          58       0.88      0.50      0.64        14\n",
      "          59       0.50      0.50      0.50         2\n",
      "          60       0.93      0.65      0.76        20\n",
      "          61       0.86      0.90      0.88        20\n",
      "          62       1.00      0.89      0.94         9\n",
      "          63       1.00      0.86      0.93        22\n",
      "          64       0.00      0.00      0.00         2\n",
      "          65       0.89      0.84      0.86        19\n",
      "          66       0.62      0.56      0.59         9\n",
      "          67       0.92      1.00      0.96        12\n",
      "          68       1.00      0.88      0.93        16\n",
      "          69       1.00      0.50      0.67         4\n",
      "          70       1.00      0.67      0.80        15\n",
      "          71       1.00      0.31      0.48        16\n",
      "          72       1.00      0.80      0.89        10\n",
      "          73       1.00      0.65      0.79        20\n",
      "          74       0.43      0.30      0.35        10\n",
      "          75       0.42      0.71      0.53        14\n",
      "          76       0.75      0.30      0.43        10\n",
      "          77       1.00      0.65      0.79        17\n",
      "          78       0.65      0.75      0.70        20\n",
      "          79       1.00      0.25      0.40         8\n",
      "          80       1.00      0.55      0.71        11\n",
      "          81       0.83      0.71      0.77         7\n",
      "          82       1.00      0.90      0.95        20\n",
      "          83       0.91      0.71      0.80        14\n",
      "          84       1.00      0.50      0.67        22\n",
      "          85       1.00      0.57      0.73         7\n",
      "          86       1.00      0.67      0.80        15\n",
      "          87       1.00      0.53      0.69        17\n",
      "          88       1.00      0.43      0.60         7\n",
      "          89       0.83      0.50      0.62        10\n",
      "          90       0.00      0.00      0.00         2\n",
      "          91       0.43      0.75      0.55         8\n",
      "          92       0.73      0.69      0.71        16\n",
      "          93       0.33      0.25      0.29         8\n",
      "          94       0.72      0.91      0.81        23\n",
      "          95       0.89      0.80      0.84        10\n",
      "          96       0.00      0.00      0.00         3\n",
      "          97       1.00      0.55      0.71        22\n",
      "          98       0.20      0.20      0.20         5\n",
      "          99       1.00      0.70      0.82        23\n",
      "         100       0.82      0.74      0.78        19\n",
      "         101       0.93      0.62      0.74        21\n",
      "         102       0.86      0.71      0.77        17\n",
      "         103       0.73      0.73      0.73        11\n",
      "         104       1.00      0.64      0.78        14\n",
      "         105       1.00      0.73      0.85        15\n",
      "         106       1.00      0.88      0.94        17\n",
      "         107       1.00      0.58      0.73        19\n",
      "         108       1.00      0.17      0.29         6\n",
      "         109       1.00      0.46      0.63        13\n",
      "         110       1.00      0.20      0.33         5\n",
      "         111       1.00      0.50      0.67         6\n",
      "         112       1.00      0.43      0.60         7\n",
      "         113       0.82      0.93      0.87        15\n",
      "         114       1.00      0.12      0.22         8\n",
      "         115       1.00      1.00      1.00         9\n",
      "         116       1.00      0.25      0.40         4\n",
      "         117       1.00      0.64      0.78        22\n",
      "         118       1.00      0.75      0.86        12\n",
      "         119       1.00      0.71      0.83        17\n",
      "         120       0.62      0.80      0.70        20\n",
      "         121       1.00      0.55      0.71        22\n",
      "         122       1.00      0.75      0.86         8\n",
      "         123       1.00      0.73      0.84        22\n",
      "         124       0.94      0.83      0.88        18\n",
      "         125       0.00      0.00      0.00         1\n",
      "         126       1.00      0.25      0.40         4\n",
      "         127       0.79      0.73      0.76        15\n",
      "         128       0.00      0.00      0.00         6\n",
      "         129       1.00      0.50      0.67        10\n",
      "         130       1.00      0.78      0.88         9\n",
      "         131       0.92      0.50      0.65        22\n",
      "         132       1.00      0.65      0.79        17\n",
      "         133       0.23      0.56      0.32         9\n",
      "         134       0.75      0.86      0.80         7\n",
      "         135       0.82      0.64      0.72        14\n",
      "         136       0.35      0.72      0.47        18\n",
      "         137       1.00      0.53      0.69        17\n",
      "         138       1.00      0.67      0.80         3\n",
      "         139       0.88      0.78      0.82         9\n",
      "         140       0.71      0.67      0.69        15\n",
      "         141       0.67      0.44      0.53         9\n",
      "         142       0.75      0.43      0.55         7\n",
      "         143       1.00      0.50      0.67         8\n",
      "         144       1.00      0.50      0.67         2\n",
      "         145       1.00      0.56      0.71         9\n",
      "         146       0.83      0.67      0.74        15\n",
      "         147       0.00      0.00      0.00         2\n",
      "         148       1.00      0.83      0.91        12\n",
      "         149       1.00      0.84      0.91        19\n",
      "         150       1.00      0.89      0.94        19\n",
      "         151       0.90      0.47      0.62        19\n",
      "         152       0.80      0.80      0.80        20\n",
      "         153       0.82      0.70      0.76        20\n",
      "         154       1.00      0.67      0.80        21\n",
      "         155       0.71      0.81      0.76        21\n",
      "         156       1.00      0.78      0.88        18\n",
      "         157       1.00      0.40      0.57         5\n",
      "         158       0.89      0.62      0.73        13\n",
      "         159       0.90      0.95      0.92        19\n",
      "         160       0.35      0.65      0.46        17\n",
      "         161       0.85      0.79      0.81        14\n",
      "         162       1.00      0.83      0.91        18\n",
      "         163       0.91      0.91      0.91        11\n",
      "         164       0.92      0.55      0.69        22\n",
      "         165       0.00      0.00      0.00         2\n",
      "         166       1.00      0.82      0.90        17\n",
      "         167       1.00      0.50      0.67        18\n",
      "         168       0.50      0.47      0.48        17\n",
      "         169       1.00      0.39      0.56        18\n",
      "         170       1.00      0.89      0.94        18\n",
      "         171       0.89      0.89      0.89        18\n",
      "         172       1.00      0.50      0.67         4\n",
      "         173       1.00      0.58      0.74        12\n",
      "         174       0.92      0.80      0.86        15\n",
      "         175       1.00      0.17      0.29         6\n",
      "         176       0.91      0.71      0.80        14\n",
      "         177       1.00      0.50      0.67         4\n",
      "         178       1.00      0.86      0.92        21\n",
      "         179       0.93      0.76      0.84        17\n",
      "         180       1.00      0.50      0.67         6\n",
      "         181       0.68      0.72      0.70        18\n",
      "         182       0.83      0.71      0.77         7\n",
      "         183       1.00      0.33      0.50         6\n",
      "         184       1.00      0.75      0.86        20\n",
      "         185       1.00      0.50      0.67         2\n",
      "         186       0.88      0.93      0.90        15\n",
      "         187       0.86      0.75      0.80        16\n",
      "         188       1.00      0.29      0.44         7\n",
      "         189       1.00      0.40      0.57        10\n",
      "         190       0.00      0.00      0.00         1\n",
      "         191       1.00      0.44      0.62        18\n",
      "         192       1.00      0.80      0.89        20\n",
      "         193       1.00      0.60      0.75        15\n",
      "         194       1.00      0.50      0.67         8\n",
      "         195       1.00      0.78      0.88        23\n",
      "         196       0.81      0.91      0.86        23\n",
      "         197       1.00      0.33      0.50         3\n",
      "         198       1.00      0.20      0.33         5\n",
      "         199       0.69      0.65      0.67        17\n",
      "         200       0.95      0.86      0.90        21\n",
      "         201       1.00      0.81      0.90        16\n",
      "         202       0.85      0.58      0.69        19\n",
      "         203       0.00      0.00      0.00         2\n",
      "         204       0.00      0.00      0.00         2\n",
      "         205       1.00      0.64      0.78        11\n",
      "         206       1.00      0.67      0.80         6\n",
      "         207       0.00      0.00      0.00         2\n",
      "         208       0.40      0.33      0.36         6\n",
      "         209       0.55      0.67      0.60        18\n",
      "         210       0.94      0.94      0.94        18\n",
      "         211       1.00      0.75      0.86        16\n",
      "         212       1.00      0.88      0.93         8\n",
      "         213       1.00      0.85      0.92        20\n",
      "         214       0.71      0.56      0.63         9\n",
      "         215       1.00      0.65      0.79        17\n",
      "         216       0.05      0.43      0.08         7\n",
      "         217       1.00      0.75      0.86        12\n",
      "         218       1.00      0.60      0.75        10\n",
      "         219       1.00      0.70      0.82        23\n",
      "         220       1.00      0.80      0.89        20\n",
      "         221       0.09      0.57      0.15        21\n",
      "         222       1.00      0.50      0.67        18\n",
      "         223       1.00      0.71      0.83        14\n",
      "         224       0.93      0.65      0.76        20\n",
      "         225       1.00      0.90      0.95        20\n",
      "         226       0.93      0.68      0.79        19\n",
      "         227       1.00      0.67      0.80         6\n",
      "         228       1.00      0.65      0.79        23\n",
      "         229       0.19      0.67      0.29        18\n",
      "         230       0.70      0.70      0.70        20\n",
      "         231       1.00      0.25      0.40         4\n",
      "         232       0.18      0.46      0.26        13\n",
      "         233       0.00      0.00      0.00         3\n",
      "         234       0.00      0.00      0.00         5\n",
      "         235       1.00      0.50      0.67         6\n",
      "         236       0.86      0.80      0.83        15\n",
      "         237       1.00      0.63      0.77        19\n",
      "         238       1.00      0.58      0.73        19\n",
      "         239       1.00      0.79      0.88        14\n",
      "         240       0.83      0.56      0.67         9\n",
      "         241       1.00      0.58      0.73        19\n",
      "         242       0.86      0.90      0.88        21\n",
      "         243       1.00      0.64      0.78        11\n",
      "         244       0.00      0.00      0.00         1\n",
      "         245       1.00      0.33      0.50         3\n",
      "         246       0.67      0.67      0.67         9\n",
      "         247       0.92      0.63      0.75        19\n",
      "         248       0.88      0.58      0.70        12\n",
      "         249       1.00      0.56      0.71         9\n",
      "         250       0.40      0.50      0.44         4\n",
      "         251       1.00      0.47      0.64        19\n",
      "         252       1.00      0.72      0.84        18\n",
      "         253       1.00      0.65      0.79        23\n",
      "         254       1.00      0.68      0.81        19\n",
      "         255       0.91      0.91      0.91        22\n",
      "         256       0.94      0.75      0.83        20\n",
      "         257       0.00      0.00      0.00         1\n",
      "         258       1.00      0.85      0.92        20\n",
      "         259       0.80      0.73      0.76        11\n",
      "         260       1.00      0.50      0.67        20\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       1.00      0.33      0.50         6\n",
      "         263       0.00      0.00      0.00         2\n",
      "         264       0.75      0.75      0.75         4\n",
      "         265       0.73      0.62      0.67        13\n",
      "         266       0.24      0.47      0.32        15\n",
      "         267       0.80      0.80      0.80        15\n",
      "         268       1.00      0.25      0.40         4\n",
      "         269       0.25      0.33      0.29         6\n",
      "         270       0.75      0.50      0.60        12\n",
      "         271       1.00      0.82      0.90        22\n",
      "         272       1.00      0.62      0.77        16\n",
      "         273       0.92      0.75      0.83        16\n",
      "         274       1.00      0.81      0.90        16\n",
      "         275       0.83      0.45      0.59        11\n",
      "         276       1.00      0.55      0.71        20\n",
      "         277       1.00      0.27      0.43        22\n",
      "         278       1.00      0.45      0.62        22\n",
      "         279       0.00      0.00      0.00         2\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.82      0.88      0.85        16\n",
      "         282       1.00      0.76      0.87        17\n",
      "         283       0.68      0.83      0.75        18\n",
      "         284       1.00      0.33      0.50         3\n",
      "         285       0.92      0.79      0.85        14\n",
      "         286       0.57      0.62      0.59        13\n",
      "         287       0.89      0.71      0.79        24\n",
      "         288       1.00      0.92      0.96        12\n",
      "         289       1.00      0.42      0.59        12\n",
      "         290       0.91      0.71      0.80        14\n",
      "         291       1.00      0.64      0.78        11\n",
      "         292       1.00      0.67      0.80         6\n",
      "         293       1.00      0.83      0.91        12\n",
      "         294       1.00      0.14      0.25         7\n",
      "         295       0.50      0.17      0.25         6\n",
      "         296       1.00      0.59      0.74        17\n",
      "         297       0.10      0.85      0.18        20\n",
      "         298       0.89      0.76      0.82        21\n",
      "         299       1.00      0.86      0.92         7\n",
      "         300       0.50      0.60      0.55         5\n",
      "         301       1.00      0.33      0.50         6\n",
      "         302       0.90      0.50      0.64        18\n",
      "         303       0.28      0.50      0.36        16\n",
      "         304       0.57      0.50      0.53         8\n",
      "         305       0.52      0.83      0.64        18\n",
      "         306       0.67      0.20      0.31        10\n",
      "         307       1.00      0.43      0.60         7\n",
      "         308       1.00      0.67      0.80        15\n",
      "         309       1.00      0.33      0.50         3\n",
      "         310       1.00      0.86      0.92        14\n",
      "         311       1.00      0.59      0.74        17\n",
      "         312       1.00      0.69      0.81        16\n",
      "         313       0.00      0.00      0.00         2\n",
      "         314       0.75      0.65      0.70        23\n",
      "         315       1.00      0.73      0.84        11\n",
      "         316       1.00      0.83      0.91         6\n",
      "         317       0.00      0.00      0.00         1\n",
      "         318       0.38      0.38      0.38         8\n",
      "         319       1.00      0.82      0.90        17\n",
      "         320       1.00      0.43      0.60         7\n",
      "         321       1.00      0.54      0.70        13\n",
      "         322       0.00      0.00      0.00         4\n",
      "         323       0.93      0.70      0.80        20\n",
      "         324       1.00      0.80      0.89         5\n",
      "         325       1.00      0.82      0.90        11\n",
      "         326       0.46      0.69      0.55        16\n",
      "         327       0.00      0.00      0.00         1\n",
      "         328       1.00      0.80      0.89        20\n",
      "         329       1.00      0.89      0.94        19\n",
      "         330       1.00      0.83      0.91        12\n",
      "         331       1.00      0.33      0.50         6\n",
      "         332       1.00      0.50      0.67         4\n",
      "         333       0.62      0.73      0.67        11\n",
      "         334       0.80      0.57      0.67         7\n",
      "         335       0.00      0.00      0.00         1\n",
      "         336       1.00      0.39      0.56        18\n",
      "         337       0.33      0.40      0.36         5\n",
      "         338       1.00      0.88      0.94        17\n",
      "         339       1.00      0.83      0.91         6\n",
      "         340       0.00      0.00      0.00         1\n",
      "         341       1.00      0.71      0.83        14\n",
      "         342       0.71      0.42      0.53        12\n",
      "         343       0.75      0.50      0.60         6\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       1.00      0.48      0.65        21\n",
      "         346       0.75      0.71      0.73        17\n",
      "         347       1.00      0.43      0.60         7\n",
      "         348       1.00      0.82      0.90        22\n",
      "         350       1.00      0.50      0.67         4\n",
      "         351       0.00      0.00      0.00         2\n",
      "         352       0.87      0.57      0.68        23\n",
      "         353       0.64      0.70      0.67        10\n",
      "         354       1.00      0.50      0.67         4\n",
      "         355       0.00      0.00      0.00         1\n",
      "         356       1.00      0.71      0.83         7\n",
      "         357       1.00      0.56      0.71         9\n",
      "         358       0.90      0.90      0.90        20\n",
      "         359       1.00      0.85      0.92        13\n",
      "         360       0.60      0.50      0.55         6\n",
      "         361       0.50      0.89      0.64        18\n",
      "         362       1.00      0.71      0.83         7\n",
      "         363       1.00      0.67      0.80         3\n",
      "         364       0.33      0.50      0.40         4\n",
      "         365       0.30      0.64      0.41        14\n",
      "         366       0.94      0.94      0.94        17\n",
      "         367       0.00      0.00      0.00         4\n",
      "         368       0.00      0.00      0.00         2\n",
      "         369       0.88      0.83      0.86        18\n",
      "         370       1.00      0.77      0.87        13\n",
      "         371       1.00      0.25      0.40         4\n",
      "         372       1.00      0.50      0.67         4\n",
      "         373       0.89      0.77      0.83        22\n",
      "         374       1.00      0.81      0.89        21\n",
      "         375       1.00      0.62      0.77         8\n",
      "         376       0.52      0.75      0.62        16\n",
      "         377       1.00      0.50      0.67        14\n",
      "         378       1.00      0.90      0.95        10\n",
      "         379       1.00      1.00      1.00         5\n",
      "         380       1.00      0.42      0.59        19\n",
      "         381       1.00      0.43      0.60         7\n",
      "         382       1.00      0.83      0.91        18\n",
      "         383       0.60      0.43      0.50         7\n",
      "         384       0.93      0.87      0.90        15\n",
      "         385       0.92      0.67      0.77        18\n",
      "         386       1.00      0.40      0.57        15\n",
      "         387       1.00      0.43      0.60         7\n",
      "         388       1.00      0.20      0.33        15\n",
      "         389       1.00      0.64      0.78        11\n",
      "         390       1.00      0.43      0.60         7\n",
      "         391       0.75      0.60      0.67         5\n",
      "         392       1.00      0.56      0.71        18\n",
      "         393       0.00      0.00      0.00         2\n",
      "         394       1.00      0.78      0.88        18\n",
      "         395       1.00      0.81      0.90        16\n",
      "         396       0.57      0.67      0.62        12\n",
      "         397       0.83      0.59      0.69        17\n",
      "         398       1.00      0.33      0.50         6\n",
      "         399       0.00      0.00      0.00         3\n",
      "         400       0.92      0.57      0.71        21\n",
      "         401       1.00      0.86      0.92        14\n",
      "         402       1.00      0.92      0.96        12\n",
      "         403       0.68      0.83      0.75        18\n",
      "         404       1.00      0.53      0.69        17\n",
      "         405       1.00      0.75      0.86        20\n",
      "         406       0.89      0.67      0.76        12\n",
      "         407       1.00      0.63      0.77        19\n",
      "         408       1.00      0.69      0.81        16\n",
      "         409       1.00      0.53      0.69        19\n",
      "         410       0.57      0.59      0.58        22\n",
      "         411       0.00      0.00      0.00         2\n",
      "         412       1.00      0.70      0.82        20\n",
      "         413       0.95      0.82      0.88        22\n",
      "         414       1.00      0.50      0.67         2\n",
      "         415       0.90      0.75      0.82        12\n",
      "         416       0.93      0.65      0.76        20\n",
      "         417       0.43      0.67      0.52         9\n",
      "         418       0.36      0.62      0.45        24\n",
      "         419       0.45      0.93      0.61        15\n",
      "         420       1.00      0.71      0.83        14\n",
      "         421       0.88      0.70      0.78        20\n",
      "         422       1.00      0.59      0.74        22\n",
      "         423       0.86      0.67      0.75        18\n",
      "         424       1.00      0.71      0.83        17\n",
      "         425       1.00      0.71      0.83        17\n",
      "         426       0.00      0.00      0.00         2\n",
      "         427       1.00      0.72      0.84        18\n",
      "         428       1.00      0.78      0.88         9\n",
      "         429       1.00      0.82      0.90        11\n",
      "         430       1.00      0.22      0.36         9\n",
      "         431       0.93      0.81      0.87        16\n",
      "         432       0.94      0.94      0.94        17\n",
      "         433       1.00      0.77      0.87        22\n",
      "         434       0.80      0.80      0.80         5\n",
      "         435       1.00      0.29      0.44         7\n",
      "         436       1.00      0.82      0.90        17\n",
      "         437       0.71      0.59      0.65        17\n",
      "         438       1.00      0.65      0.79        20\n",
      "         439       0.15      0.22      0.18         9\n",
      "         440       0.00      0.00      0.00         6\n",
      "         441       0.89      0.81      0.85        21\n",
      "         442       1.00      0.69      0.82        13\n",
      "         443       0.85      0.94      0.89        18\n",
      "         444       1.00      0.50      0.67         4\n",
      "         445       0.33      1.00      0.50         3\n",
      "         446       0.80      0.73      0.76        22\n",
      "         447       0.35      0.71      0.47        17\n",
      "         448       0.78      0.70      0.74        10\n",
      "         449       1.00      0.79      0.88        19\n",
      "         450       1.00      0.56      0.72        16\n",
      "         451       1.00      0.87      0.93        23\n",
      "         452       1.00      0.88      0.93        16\n",
      "         453       0.85      0.88      0.87        26\n",
      "         454       0.00      0.00      0.00         3\n",
      "         455       1.00      0.90      0.95        20\n",
      "         456       1.00      0.90      0.95        21\n",
      "         457       1.00      0.81      0.89        21\n",
      "         458       1.00      0.62      0.77         8\n",
      "         459       1.00      0.50      0.67         4\n",
      "         460       0.75      0.65      0.70        23\n",
      "         461       0.00      0.00      0.00         1\n",
      "         462       1.00      0.67      0.80        12\n",
      "         463       1.00      0.62      0.76        21\n",
      "         464       0.34      0.61      0.44        18\n",
      "         465       0.90      0.50      0.64        18\n",
      "         466       0.80      0.67      0.73        18\n",
      "         467       1.00      0.71      0.83        17\n",
      "         468       1.00      0.75      0.86         4\n",
      "         469       1.00      0.50      0.67         6\n",
      "         470       1.00      0.50      0.67        10\n",
      "         471       1.00      0.75      0.86         4\n",
      "         472       0.78      0.64      0.70        11\n",
      "         473       1.00      0.25      0.40         4\n",
      "         474       1.00      1.00      1.00        13\n",
      "         475       0.71      0.71      0.71        17\n",
      "         476       1.00      0.46      0.63        13\n",
      "         477       1.00      0.58      0.73        19\n",
      "         478       0.70      0.50      0.58        14\n",
      "         479       0.00      0.00      0.00         2\n",
      "         480       0.93      0.68      0.79        19\n",
      "         481       0.92      0.75      0.83        16\n",
      "         482       1.00      0.89      0.94        19\n",
      "         483       1.00      0.84      0.91        19\n",
      "         484       1.00      0.59      0.74        17\n",
      "         485       0.00      0.00      0.00         3\n",
      "         486       0.65      0.69      0.67        16\n",
      "         487       1.00      0.55      0.71        22\n",
      "         488       0.95      0.95      0.95        20\n",
      "         489       0.94      0.77      0.85        22\n",
      "         490       0.88      0.88      0.88         8\n",
      "         491       0.00      0.00      0.00         4\n",
      "         492       1.00      1.00      1.00         3\n",
      "         493       0.20      0.33      0.25         3\n",
      "         494       0.92      0.63      0.75        19\n",
      "         495       1.00      0.67      0.80        18\n",
      "         496       0.75      0.60      0.67         5\n",
      "         497       1.00      0.59      0.74        22\n",
      "         498       1.00      0.84      0.91        25\n",
      "         499       1.00      0.73      0.84        11\n",
      "         500       1.00      0.50      0.67        10\n",
      "         501       1.00      0.67      0.80        18\n",
      "         502       0.00      0.00      0.00         2\n",
      "         503       1.00      0.43      0.60        14\n",
      "         504       1.00      0.56      0.72        16\n",
      "         505       1.00      0.77      0.87        13\n",
      "         506       0.91      0.91      0.91        22\n",
      "         507       0.75      0.30      0.43        10\n",
      "         508       1.00      0.82      0.90        11\n",
      "         509       1.00      0.64      0.78        11\n",
      "         510       1.00      1.00      1.00         4\n",
      "         511       1.00      0.50      0.67        10\n",
      "         512       1.00      0.20      0.33         5\n",
      "         513       0.78      0.54      0.64        13\n",
      "         514       1.00      0.38      0.55         8\n",
      "         515       0.29      0.60      0.39        10\n",
      "         516       1.00      0.36      0.53        14\n",
      "         517       1.00      0.90      0.95        20\n",
      "         518       0.95      0.90      0.92        20\n",
      "         519       0.71      0.71      0.71        14\n",
      "         520       0.90      0.56      0.69        16\n",
      "         521       0.95      1.00      0.97        19\n",
      "         522       1.00      0.25      0.40         4\n",
      "         523       0.40      0.50      0.44         4\n",
      "         524       1.00      0.50      0.67        14\n",
      "         525       1.00      0.67      0.80         6\n",
      "         526       1.00      0.78      0.88         9\n",
      "         527       1.00      0.67      0.80        18\n",
      "         528       1.00      0.90      0.95        20\n",
      "         529       0.09      0.63      0.15        19\n",
      "         530       1.00      0.71      0.83        17\n",
      "         531       0.92      0.61      0.73        18\n",
      "         532       1.00      0.65      0.79        20\n",
      "         533       1.00      0.65      0.79        20\n",
      "         534       1.00      0.63      0.77        19\n",
      "         535       1.00      0.33      0.50         6\n",
      "         536       1.00      0.14      0.25         7\n",
      "         537       1.00      0.85      0.92        13\n",
      "         538       1.00      0.63      0.77        19\n",
      "         539       0.63      0.60      0.62        20\n",
      "         540       1.00      0.12      0.22         8\n",
      "         541       1.00      0.86      0.92        14\n",
      "         542       0.80      0.44      0.57         9\n",
      "         543       1.00      1.00      1.00         8\n",
      "         544       0.94      0.80      0.86        20\n",
      "         545       0.00      0.00      0.00         1\n",
      "         546       1.00      0.78      0.88        18\n",
      "         547       0.67      1.00      0.80         4\n",
      "         548       0.00      0.00      0.00         1\n",
      "         549       1.00      0.57      0.73         7\n",
      "         550       0.57      0.44      0.50         9\n",
      "         551       0.81      0.91      0.86        23\n",
      "         552       1.00      1.00      1.00         7\n",
      "         553       0.71      0.45      0.56        11\n",
      "         554       0.67      0.91      0.77        22\n",
      "         555       0.45      0.50      0.48        10\n",
      "         556       1.00      0.62      0.77         8\n",
      "         557       1.00      0.63      0.77        19\n",
      "         558       1.00      0.67      0.80        15\n",
      "         559       1.00      0.21      0.35        19\n",
      "         560       0.92      0.73      0.81        15\n",
      "         561       0.64      0.74      0.68        19\n",
      "         562       1.00      0.90      0.95        20\n",
      "         563       0.94      0.75      0.83        20\n",
      "         564       1.00      0.59      0.74        22\n",
      "         565       0.93      0.93      0.93        15\n",
      "         566       1.00      0.69      0.82        13\n",
      "         567       0.00      0.00      0.00         1\n",
      "         568       1.00      0.45      0.62        11\n",
      "         569       0.84      0.84      0.84        19\n",
      "         570       1.00      0.25      0.40         4\n",
      "         571       1.00      0.58      0.73        19\n",
      "         572       1.00      0.75      0.86         8\n",
      "         573       0.94      0.89      0.91        18\n",
      "         574       0.87      0.76      0.81        17\n",
      "         575       1.00      0.29      0.44         7\n",
      "         576       1.00      0.64      0.78        11\n",
      "         577       0.80      0.57      0.67         7\n",
      "         578       1.00      0.25      0.40         4\n",
      "         579       0.83      0.50      0.62        10\n",
      "         580       0.00      0.00      0.00         4\n",
      "         581       1.00      0.61      0.76        18\n",
      "         582       1.00      0.50      0.67         6\n",
      "         583       1.00      0.48      0.65        23\n",
      "         584       1.00      0.65      0.79        17\n",
      "         585       0.75      0.60      0.67         5\n",
      "         586       1.00      0.29      0.44         7\n",
      "         587       0.93      0.72      0.81        18\n",
      "         588       0.71      0.50      0.59        20\n",
      "         589       0.60      0.55      0.57        11\n",
      "         590       1.00      0.44      0.62         9\n",
      "         591       1.00      0.60      0.75        20\n",
      "         592       1.00      0.67      0.80         3\n",
      "         593       1.00      0.33      0.50         9\n",
      "         594       0.80      0.44      0.57         9\n",
      "         595       0.76      0.73      0.74        22\n",
      "         596       1.00      0.67      0.80        21\n",
      "         597       1.00      0.71      0.83        17\n",
      "         598       1.00      0.25      0.40         4\n",
      "         599       0.75      0.94      0.83        16\n",
      "         600       1.00      0.73      0.84        11\n",
      "         601       1.00      0.79      0.88        19\n",
      "         602       1.00      0.67      0.80         9\n",
      "         603       1.00      0.80      0.89         5\n",
      "         604       0.72      0.87      0.79        15\n",
      "         605       0.91      0.67      0.77        15\n",
      "         606       1.00      0.90      0.95        21\n",
      "         607       1.00      0.25      0.40         4\n",
      "         608       1.00      0.57      0.73         7\n",
      "         609       1.00      0.69      0.81        16\n",
      "         610       1.00      0.75      0.86         8\n",
      "         611       0.65      0.79      0.71        14\n",
      "         612       0.80      0.67      0.73        18\n",
      "         613       0.71      0.50      0.59        10\n",
      "         614       0.90      0.75      0.82        12\n",
      "         615       1.00      1.00      1.00        12\n",
      "         616       1.00      0.50      0.67         4\n",
      "         617       1.00      0.85      0.92        13\n",
      "         618       1.00      0.80      0.89         5\n",
      "         619       0.87      0.72      0.79        18\n",
      "         620       1.00      0.75      0.86        12\n",
      "         621       1.00      0.50      0.67         2\n",
      "         622       0.58      0.88      0.70        16\n",
      "         623       1.00      0.71      0.83        21\n",
      "         624       1.00      0.82      0.90        17\n",
      "         625       1.00      0.89      0.94        18\n",
      "         626       1.00      1.00      1.00         2\n",
      "         627       0.00      0.00      0.00         1\n",
      "         628       0.92      0.71      0.80        17\n",
      "         629       1.00      0.52      0.69        23\n",
      "         630       0.62      0.56      0.59         9\n",
      "         631       1.00      0.40      0.57         5\n",
      "         632       1.00      0.45      0.62        20\n",
      "         633       0.67      1.00      0.80        22\n",
      "         634       0.19      0.30      0.23        10\n",
      "         635       1.00      0.25      0.40         4\n",
      "         636       1.00      0.95      0.98        21\n",
      "         637       1.00      0.78      0.88        18\n",
      "         638       0.00      0.00      0.00         1\n",
      "         639       0.75      0.43      0.55         7\n",
      "         640       0.79      0.55      0.65        20\n",
      "         641       0.64      1.00      0.78         9\n",
      "         642       0.80      0.86      0.83        14\n",
      "         643       1.00      0.50      0.67        14\n",
      "         644       1.00      1.00      1.00        19\n",
      "         645       1.00      0.50      0.67        10\n",
      "         646       0.90      0.47      0.62        19\n",
      "         647       1.00      0.85      0.92        20\n",
      "         648       0.56      0.82      0.67        11\n",
      "         649       0.62      0.57      0.59        14\n",
      "         650       0.50      0.20      0.29         5\n",
      "         651       1.00      0.38      0.55        16\n",
      "         652       1.00      0.75      0.86        20\n",
      "         653       0.00      0.00      0.00         1\n",
      "         654       1.00      0.50      0.67         2\n",
      "         655       0.02      0.80      0.03         5\n",
      "         656       1.00      0.43      0.60         7\n",
      "         657       1.00      0.80      0.89         5\n",
      "         658       1.00      0.60      0.75        15\n",
      "         659       1.00      0.60      0.75         5\n",
      "         660       0.33      0.71      0.45        17\n",
      "         661       0.82      0.78      0.80        18\n",
      "         662       1.00      0.88      0.93        16\n",
      "         663       0.80      0.50      0.62         8\n",
      "         664       1.00      0.62      0.77         8\n",
      "         665       0.94      0.76      0.84        21\n",
      "         666       0.90      0.69      0.78        13\n",
      "         667       1.00      0.50      0.67         8\n",
      "         668       0.93      0.65      0.76        20\n",
      "         669       1.00      0.50      0.67         2\n",
      "         670       1.00      0.29      0.44         7\n",
      "         671       1.00      0.67      0.80         6\n",
      "         672       0.00      0.00      0.00         3\n",
      "         673       1.00      0.42      0.59        12\n",
      "         674       0.62      0.56      0.59        18\n",
      "         675       0.92      0.67      0.77        18\n",
      "         676       1.00      0.86      0.93        22\n",
      "         677       0.80      0.86      0.83        14\n",
      "         678       0.00      0.00      0.00         3\n",
      "         679       1.00      0.67      0.80        18\n",
      "         680       1.00      0.40      0.57        10\n",
      "         681       1.00      0.67      0.80        21\n",
      "         682       0.50      0.50      0.50        10\n",
      "         683       1.00      0.67      0.80        18\n",
      "         684       1.00      0.79      0.88        19\n",
      "         685       0.85      0.85      0.85        13\n",
      "         686       1.00      0.40      0.57         5\n",
      "         687       0.00      0.00      0.00         3\n",
      "         688       0.00      0.00      0.00         1\n",
      "         689       1.00      0.73      0.85        15\n",
      "         690       1.00      0.52      0.68        25\n",
      "         691       1.00      0.62      0.77         8\n",
      "         692       1.00      0.94      0.97        17\n",
      "         693       0.46      0.50      0.48        12\n",
      "         694       1.00      0.68      0.81        22\n",
      "         695       0.70      0.33      0.45        21\n",
      "         696       0.95      0.86      0.90        21\n",
      "         697       1.00      0.74      0.85        23\n",
      "         698       1.00      0.77      0.87        22\n",
      "         699       0.74      0.94      0.83        18\n",
      "         700       1.00      0.71      0.83        21\n",
      "         701       0.67      0.86      0.75         7\n",
      "         702       1.00      0.60      0.75        15\n",
      "         703       0.67      0.50      0.57         4\n",
      "         704       1.00      1.00      1.00         3\n",
      "         705       1.00      0.76      0.86        21\n",
      "         706       0.92      0.50      0.65        24\n",
      "         707       0.47      0.39      0.42        18\n",
      "         708       0.10      0.17      0.12         6\n",
      "         709       1.00      0.41      0.58        17\n",
      "         710       1.00      0.80      0.89        10\n",
      "         711       1.00      0.33      0.50         3\n",
      "         712       1.00      0.33      0.50         6\n",
      "         713       1.00      0.83      0.91         6\n",
      "         714       1.00      0.75      0.86        20\n",
      "         715       1.00      0.91      0.95        22\n",
      "         716       1.00      0.68      0.81        22\n",
      "         717       1.00      0.74      0.85        19\n",
      "         718       1.00      0.67      0.80         9\n",
      "         719       1.00      0.78      0.88         9\n",
      "         720       1.00      0.38      0.55         8\n",
      "         721       0.00      0.00      0.00         2\n",
      "         722       1.00      0.59      0.74        17\n",
      "         723       1.00      0.88      0.93         8\n",
      "         724       1.00      0.86      0.92         7\n",
      "         725       1.00      0.25      0.40         4\n",
      "         726       0.83      0.71      0.77        14\n",
      "         727       0.75      0.33      0.46         9\n",
      "         728       0.00      0.00      0.00         1\n",
      "         729       1.00      0.40      0.57         5\n",
      "         730       1.00      0.33      0.50         3\n",
      "         731       1.00      0.86      0.92         7\n",
      "         732       0.92      0.75      0.83        16\n",
      "         733       0.75      0.43      0.55         7\n",
      "         734       0.75      0.43      0.55        14\n",
      "         735       1.00      0.67      0.80        18\n",
      "         736       1.00      0.71      0.83        17\n",
      "         737       1.00      0.80      0.89         5\n",
      "         738       1.00      0.50      0.67         8\n",
      "         739       1.00      0.40      0.57         5\n",
      "         740       1.00      0.33      0.50         3\n",
      "         741       0.00      0.00      0.00         2\n",
      "         742       1.00      0.42      0.59        12\n",
      "         743       1.00      0.82      0.90        22\n",
      "         744       1.00      0.76      0.87        17\n",
      "         745       1.00      0.44      0.62         9\n",
      "         746       1.00      0.72      0.84        25\n",
      "         747       0.82      0.86      0.84        21\n",
      "         748       0.93      0.88      0.90        16\n",
      "         749       0.73      0.61      0.67        18\n",
      "         750       0.94      0.77      0.85        22\n",
      "         751       0.67      0.67      0.67         3\n",
      "         752       1.00      0.73      0.85        15\n",
      "         753       1.00      0.78      0.88         9\n",
      "         754       1.00      0.80      0.89        15\n",
      "         755       0.86      0.72      0.78        25\n",
      "         756       0.75      0.30      0.43        10\n",
      "         757       0.64      0.93      0.76        15\n",
      "         758       0.93      0.88      0.90        16\n",
      "         759       1.00      0.67      0.80         6\n",
      "         760       0.93      1.00      0.96        13\n",
      "         761       1.00      0.52      0.69        23\n",
      "         762       0.95      0.79      0.86        24\n",
      "         763       1.00      0.30      0.46        10\n",
      "         764       0.43      0.75      0.55         4\n",
      "         765       1.00      0.60      0.75        20\n",
      "         766       0.90      0.38      0.53        24\n",
      "         767       0.84      0.84      0.84        19\n",
      "         768       0.00      0.00      0.00         2\n",
      "         769       1.00      0.47      0.64        15\n",
      "         770       0.71      0.62      0.67         8\n",
      "         771       1.00      0.62      0.77        16\n",
      "         772       1.00      0.56      0.71        18\n",
      "         773       1.00      0.75      0.86         4\n",
      "         774       1.00      1.00      1.00        13\n",
      "         775       1.00      0.50      0.67         6\n",
      "         776       1.00      0.20      0.33         5\n",
      "         777       1.00      0.40      0.57         5\n",
      "         778       0.14      0.50      0.22         2\n",
      "         779       1.00      1.00      1.00        24\n",
      "         780       0.85      0.73      0.79        15\n",
      "         781       1.00      0.76      0.87        17\n",
      "         782       0.68      0.79      0.73        24\n",
      "         783       1.00      0.76      0.87        17\n",
      "         784       1.00      0.36      0.53        14\n",
      "         785       1.00      0.60      0.75        10\n",
      "         786       0.00      0.00      0.00         3\n",
      "         787       0.00      0.00      0.00         5\n",
      "         788       1.00      0.68      0.81        19\n",
      "         789       0.00      0.78      0.00         9\n",
      "         790       1.00      0.78      0.88         9\n",
      "         791       1.00      0.60      0.75        20\n",
      "         792       0.00      0.00      0.00         4\n",
      "         793       1.00      0.60      0.75         5\n",
      "         794       1.00      0.44      0.62         9\n",
      "         795       1.00      0.67      0.80        21\n",
      "         796       0.90      0.90      0.90        10\n",
      "         797       1.00      0.63      0.77        19\n",
      "         798       1.00      0.50      0.67         4\n",
      "         799       1.00      0.55      0.71        20\n",
      "         800       1.00      0.33      0.50         3\n",
      "         801       0.00      0.00      0.00         2\n",
      "         802       1.00      0.40      0.57        20\n",
      "         803       0.88      0.88      0.88        17\n",
      "         804       1.00      0.85      0.92        20\n",
      "         805       1.00      0.58      0.74        12\n",
      "         806       0.01      0.67      0.01        15\n",
      "         807       1.00      0.69      0.82        13\n",
      "         808       1.00      0.67      0.80         9\n",
      "         809       1.00      0.50      0.67        14\n",
      "         810       1.00      0.86      0.92         7\n",
      "         811       0.91      0.62      0.74        16\n",
      "         812       0.39      0.54      0.45        13\n",
      "         813       1.00      0.74      0.85        19\n",
      "         814       0.00      0.00      0.00         1\n",
      "         815       1.00      0.59      0.74        22\n",
      "         816       1.00      0.62      0.76        21\n",
      "         817       0.77      0.56      0.65        18\n",
      "         818       0.00      0.00      0.00         2\n",
      "         819       1.00      0.44      0.62         9\n",
      "         820       1.00      0.57      0.73        14\n",
      "         821       1.00      0.69      0.81        16\n",
      "         822       1.00      0.71      0.83        21\n",
      "         823       1.00      0.59      0.74        17\n",
      "         824       0.67      0.44      0.53         9\n",
      "         825       1.00      0.57      0.73         7\n",
      "         826       0.93      0.72      0.81        18\n",
      "         827       1.00      1.00      1.00         3\n",
      "         828       1.00      0.86      0.92         7\n",
      "         829       0.90      0.64      0.75        14\n",
      "         830       0.64      0.47      0.54        15\n",
      "         831       0.00      0.00      0.00         1\n",
      "         832       0.83      0.59      0.69        17\n",
      "         833       0.92      0.69      0.79        16\n",
      "         834       1.00      0.81      0.89        21\n",
      "         835       1.00      0.60      0.75        10\n",
      "         836       1.00      0.82      0.90        11\n",
      "         837       0.94      0.77      0.85        22\n",
      "         838       1.00      0.90      0.95        10\n",
      "         839       1.00      0.50      0.67        10\n",
      "         840       1.00      0.29      0.44         7\n",
      "         841       0.94      0.75      0.83        20\n",
      "         842       1.00      0.45      0.62        11\n",
      "         843       0.00      0.00      0.00         4\n",
      "         844       1.00      0.11      0.20         9\n",
      "         845       0.00      0.00      0.00         1\n",
      "         846       0.83      0.83      0.83        12\n",
      "         847       0.93      0.74      0.82        19\n",
      "         848       0.00      0.00      0.00         3\n",
      "         849       0.67      0.31      0.42        13\n",
      "         850       0.88      0.75      0.81        20\n",
      "         851       1.00      0.75      0.86         8\n",
      "         852       1.00      0.60      0.75        10\n",
      "         853       0.00      0.00      0.00         3\n",
      "         854       1.00      0.50      0.67        10\n",
      "         855       1.00      0.75      0.86        12\n",
      "         856       1.00      0.91      0.95        23\n",
      "         857       0.94      0.79      0.86        19\n",
      "         858       1.00      0.77      0.87        22\n",
      "         859       1.00      0.87      0.93        23\n",
      "         860       0.62      0.87      0.72        15\n",
      "         861       1.00      0.25      0.40         4\n",
      "         862       1.00      0.68      0.81        19\n",
      "         863       0.00      0.00      0.00         1\n",
      "         864       1.00      0.50      0.67         6\n",
      "         865       1.00      0.91      0.95        11\n",
      "         866       1.00      0.87      0.93        15\n",
      "         867       1.00      0.43      0.60         7\n",
      "         868       0.80      0.44      0.57         9\n",
      "         869       0.79      0.65      0.71        17\n",
      "         870       0.89      0.53      0.67        15\n",
      "         871       1.00      0.82      0.90        11\n",
      "         872       1.00      0.80      0.89        15\n",
      "         873       0.88      0.58      0.70        12\n",
      "         874       1.00      0.83      0.91        18\n",
      "         875       1.00      0.70      0.82        20\n",
      "         876       1.00      0.77      0.87        22\n",
      "         877       1.00      0.65      0.79        17\n",
      "         878       0.00      0.00      0.00         1\n",
      "         879       0.00      0.00      0.00         1\n",
      "         880       0.20      0.33      0.25         9\n",
      "         881       1.00      0.50      0.67        10\n",
      "         882       1.00      0.61      0.76        18\n",
      "         883       1.00      0.50      0.67         6\n",
      "         884       0.85      0.79      0.81        14\n",
      "         885       0.71      0.62      0.67         8\n",
      "         886       1.00      0.22      0.36         9\n",
      "         887       1.00      0.80      0.89        10\n",
      "         888       0.82      0.82      0.82        17\n",
      "         889       1.00      0.64      0.78        14\n",
      "         890       1.00      0.67      0.80         9\n",
      "         891       0.84      0.73      0.78        22\n",
      "         892       0.62      0.56      0.59         9\n",
      "         893       1.00      0.75      0.86        24\n",
      "         894       0.33      0.38      0.35         8\n",
      "         895       0.00      0.00      0.00         5\n",
      "         896       0.00      0.00      0.00         1\n",
      "         897       0.47      0.58      0.52        12\n",
      "         898       0.78      0.88      0.82        16\n",
      "         899       1.00      0.33      0.50         6\n",
      "         900       1.00      0.20      0.33         5\n",
      "         901       1.00      0.81      0.90        16\n",
      "         902       0.67      0.67      0.67        24\n",
      "         903       0.90      0.82      0.86        22\n",
      "         904       1.00      0.61      0.76        23\n",
      "         905       1.00      0.62      0.76        21\n",
      "         906       1.00      0.81      0.90        16\n",
      "         907       1.00      0.65      0.79        17\n",
      "         908       0.88      0.75      0.81        20\n",
      "         909       1.00      0.79      0.88        19\n",
      "         910       1.00      0.33      0.50         6\n",
      "         911       1.00      0.25      0.40         4\n",
      "         912       0.92      0.69      0.79        16\n",
      "         913       0.89      0.80      0.84        10\n",
      "         914       0.56      0.64      0.60        14\n",
      "         915       0.91      0.50      0.65        20\n",
      "         916       1.00      0.57      0.73         7\n",
      "         917       1.00      0.62      0.77        16\n",
      "         918       1.00      0.62      0.77         8\n",
      "         919       0.68      0.87      0.76        15\n",
      "         920       1.00      0.82      0.90        11\n",
      "         921       0.83      0.56      0.67         9\n",
      "         922       1.00      0.45      0.62        11\n",
      "         923       1.00      0.33      0.50         9\n",
      "         924       1.00      0.62      0.77         8\n",
      "         925       1.00      0.56      0.71         9\n",
      "         926       0.76      0.73      0.74        22\n",
      "         927       1.00      0.67      0.80         9\n",
      "         928       1.00      0.50      0.67         8\n",
      "         929       0.54      0.58      0.56        12\n",
      "         930       0.45      0.67      0.54        15\n",
      "         931       1.00      0.43      0.60         7\n",
      "         932       0.93      0.72      0.81        18\n",
      "         933       0.76      0.76      0.76        17\n",
      "         934       1.00      0.42      0.59        19\n",
      "         935       1.00      0.78      0.88         9\n",
      "         936       1.00      0.30      0.46        10\n",
      "         937       0.89      0.77      0.83        22\n",
      "         938       1.00      0.50      0.67         2\n",
      "         939       1.00      0.70      0.82        23\n",
      "         940       1.00      0.50      0.67         6\n",
      "         941       1.00      0.50      0.67        14\n",
      "         942       1.00      0.57      0.73        14\n",
      "         943       1.00      0.50      0.67         2\n",
      "         944       0.94      0.71      0.81        21\n",
      "         945       1.00      0.87      0.93        15\n",
      "         946       1.00      0.43      0.60         7\n",
      "         947       0.93      0.70      0.80        20\n",
      "         948       0.89      0.57      0.70        14\n",
      "         949       1.00      0.33      0.50         6\n",
      "         950       0.00      0.00      0.00         1\n",
      "         951       0.00      0.00      0.00         2\n",
      "         952       0.00      0.00      0.00         2\n",
      "         953       1.00      0.70      0.82        10\n",
      "         954       0.00      0.00      0.00         2\n",
      "         955       1.00      0.50      0.67         4\n",
      "         956       1.00      0.48      0.65        21\n",
      "         957       0.62      1.00      0.77         5\n",
      "         958       0.40      0.50      0.44         8\n",
      "         959       1.00      0.73      0.84        11\n",
      "         960       1.00      0.38      0.55        24\n",
      "         961       0.00      0.00      0.00         2\n",
      "         962       0.14      0.27      0.19        11\n",
      "         963       1.00      0.67      0.80        12\n",
      "         964       1.00      0.29      0.44         7\n",
      "         965       1.00      0.64      0.78        11\n",
      "         966       1.00      0.71      0.83        21\n",
      "         967       1.00      0.50      0.67        10\n",
      "         968       1.00      0.63      0.77        19\n",
      "         969       1.00      0.67      0.80         9\n",
      "         970       0.00      0.00      0.00         1\n",
      "         971       1.00      0.85      0.92        20\n",
      "         972       0.85      0.81      0.83        21\n",
      "         973       1.00      0.80      0.89        25\n",
      "         974       0.29      0.56      0.38         9\n",
      "         975       1.00      0.67      0.80         6\n",
      "         976       1.00      0.71      0.83        14\n",
      "         977       0.89      0.84      0.86        19\n",
      "         978       0.65      0.76      0.70        17\n",
      "         979       1.00      0.33      0.50         6\n",
      "         980       1.00      0.56      0.71         9\n",
      "         981       1.00      0.78      0.88        23\n",
      "         982       0.49      0.71      0.58        24\n",
      "         983       1.00      0.17      0.29         6\n",
      "         984       1.00      0.80      0.89        20\n",
      "         985       1.00      0.60      0.75        25\n",
      "         986       0.94      0.77      0.85        22\n",
      "         987       1.00      0.56      0.71         9\n",
      "         988       0.75      0.75      0.75        16\n",
      "         989       1.00      0.42      0.59        12\n",
      "         990       0.67      0.33      0.44         6\n",
      "         991       1.00      0.88      0.94        17\n",
      "         992       0.12      0.67      0.20         6\n",
      "         993       0.33      0.29      0.31         7\n",
      "         994       1.00      0.73      0.85        15\n",
      "         995       0.67      0.71      0.69        14\n",
      "         996       1.00      0.30      0.46        10\n",
      "         997       0.75      0.60      0.67        10\n",
      "         998       1.00      0.50      0.67         2\n",
      "         999       0.78      0.64      0.70        22\n",
      "        1000       1.00      0.50      0.67        20\n",
      "        1001       1.00      0.65      0.79        23\n",
      "        1002       0.50      0.17      0.25         6\n",
      "        1003       1.00      0.64      0.78        11\n",
      "        1004       1.00      0.79      0.88        19\n",
      "        1005       0.00      0.00      0.00         4\n",
      "        1006       1.00      0.50      0.67         6\n",
      "        1007       1.00      0.75      0.86        16\n",
      "        1008       1.00      0.60      0.75        10\n",
      "        1009       0.00      0.00      0.00         2\n",
      "        1010       0.92      0.55      0.69        22\n",
      "        1011       1.00      0.25      0.40         4\n",
      "        1012       1.00      0.30      0.46        10\n",
      "        1013       1.00      0.62      0.77         8\n",
      "        1014       1.00      1.00      1.00         8\n",
      "        1015       0.00      0.00      0.00         2\n",
      "        1016       1.00      0.88      0.93         8\n",
      "        1017       0.00      0.00      0.00         6\n",
      "        1018       1.00      0.80      0.89        20\n",
      "        1019       1.00      0.58      0.74        12\n",
      "        1020       1.00      0.60      0.75         5\n",
      "        1021       0.95      0.86      0.90        21\n",
      "        1022       1.00      0.68      0.81        19\n",
      "        1023       0.83      0.71      0.77         7\n",
      "        1024       1.00      0.64      0.78        11\n",
      "        1025       0.50      0.50      0.50         4\n",
      "        1026       0.00      0.00      0.00         4\n",
      "        1027       0.91      0.91      0.91        22\n",
      "        1028       0.48      0.81      0.60        16\n",
      "        1029       0.67      0.29      0.40         7\n",
      "        1030       1.00      0.25      0.40         8\n",
      "        1031       1.00      0.68      0.81        22\n",
      "        1032       0.77      0.71      0.74        14\n",
      "        1033       1.00      0.67      0.80        12\n",
      "        1034       0.00      0.00      0.00         3\n",
      "        1035       1.00      0.71      0.83        14\n",
      "        1036       1.00      0.50      0.67         2\n",
      "        1037       1.00      0.62      0.76        13\n",
      "        1038       1.00      0.25      0.40         8\n",
      "        1039       1.00      0.57      0.73         7\n",
      "        1040       1.00      0.47      0.64        19\n",
      "        1041       1.00      0.69      0.81        16\n",
      "        1042       0.79      0.61      0.69        18\n",
      "        1043       0.88      0.61      0.72        23\n",
      "        1044       0.78      0.78      0.78         9\n",
      "        1045       0.83      0.90      0.86        21\n",
      "        1046       0.23      0.43      0.30         7\n",
      "        1047       1.00      0.78      0.88        18\n",
      "        1048       0.07      0.56      0.13        18\n",
      "        1049       1.00      0.40      0.57        20\n",
      "        1050       0.00      0.00      0.00         3\n",
      "        1051       0.35      0.75      0.48         8\n",
      "        1052       0.65      0.55      0.59        20\n",
      "        1053       1.00      0.75      0.86        24\n",
      "        1054       1.00      0.53      0.69        17\n",
      "        1055       1.00      0.33      0.50        12\n",
      "        1056       1.00      0.25      0.40         4\n",
      "        1057       0.00      0.00      0.00         4\n",
      "        1058       1.00      0.92      0.96        12\n",
      "        1059       0.94      0.88      0.91        17\n",
      "        1060       0.78      0.86      0.82        21\n",
      "        1061       1.00      0.93      0.96        14\n",
      "        1062       0.94      0.79      0.86        19\n",
      "        1063       1.00      0.39      0.56        23\n",
      "        1064       1.00      0.77      0.87        13\n",
      "        1065       1.00      0.69      0.82        13\n",
      "        1066       1.00      0.53      0.69        19\n",
      "        1067       1.00      0.88      0.94        17\n",
      "        1068       0.50      0.50      0.50         6\n",
      "        1069       0.50      0.60      0.55        20\n",
      "        1070       0.75      0.60      0.67        15\n",
      "        1071       0.80      0.92      0.86        13\n",
      "        1072       1.00      0.38      0.55         8\n",
      "        1073       0.89      0.57      0.70        14\n",
      "        1074       1.00      0.88      0.93        24\n",
      "        1075       1.00      0.64      0.78        22\n",
      "        1076       0.50      0.65      0.56        17\n",
      "        1077       1.00      0.14      0.25         7\n",
      "        1078       1.00      0.33      0.50         3\n",
      "        1079       1.00      0.94      0.97        16\n",
      "        1080       0.00      0.00      0.00         2\n",
      "        1081       0.70      0.41      0.52        17\n",
      "        1082       1.00      0.65      0.79        17\n",
      "        1083       0.00      0.00      0.00         2\n",
      "        1084       0.80      0.50      0.62        16\n",
      "        1085       1.00      0.60      0.75        10\n",
      "        1086       1.00      0.93      0.97        15\n",
      "        1087       1.00      0.89      0.94        18\n",
      "        1088       1.00      0.75      0.86        16\n",
      "        1089       0.89      0.47      0.62        17\n",
      "        1090       0.88      1.00      0.94        23\n",
      "        1091       1.00      0.75      0.86         8\n",
      "        1092       0.89      0.67      0.76        12\n",
      "        1093       0.88      0.88      0.88        16\n",
      "        1094       1.00      0.47      0.64        19\n",
      "        1095       0.00      0.00      0.00         1\n",
      "        1096       1.00      0.62      0.76        13\n",
      "        1097       1.00      0.76      0.87        17\n",
      "        1098       1.00      0.55      0.71        20\n",
      "        1099       0.00      0.00      0.00         1\n",
      "        1100       1.00      0.76      0.87        17\n",
      "        1101       1.00      0.21      0.35        14\n",
      "        1102       0.73      0.57      0.64        14\n",
      "        1103       1.00      0.73      0.85        15\n",
      "        1104       1.00      0.75      0.86         4\n",
      "        1105       1.00      0.67      0.80        12\n",
      "        1106       0.40      1.00      0.57         4\n",
      "        1107       0.80      0.44      0.57         9\n",
      "        1108       1.00      0.60      0.75        20\n",
      "        1109       0.17      0.17      0.17         6\n",
      "        1110       1.00      0.86      0.92        21\n",
      "        1111       1.00      0.33      0.50         3\n",
      "        1112       1.00      0.62      0.77        16\n",
      "        1113       1.00      0.59      0.74        17\n",
      "        1114       0.94      0.89      0.91        18\n",
      "        1115       1.00      0.25      0.40         4\n",
      "        1116       1.00      0.60      0.75         5\n",
      "        1117       1.00      0.54      0.70        13\n",
      "        1118       1.00      0.47      0.64        19\n",
      "        1119       0.00      0.00      0.00         3\n",
      "        1120       0.12      0.20      0.15         5\n",
      "        1121       1.00      0.50      0.67         8\n",
      "        1122       0.86      0.63      0.73        19\n",
      "        1123       1.00      0.43      0.60         7\n",
      "        1124       0.93      0.88      0.90        16\n",
      "        1125       1.00      0.70      0.82        10\n",
      "        1126       0.94      0.89      0.92        19\n",
      "        1127       0.92      0.75      0.83        16\n",
      "        1128       0.17      0.45      0.24        22\n",
      "        1129       0.61      0.86      0.72        22\n",
      "        1130       0.25      0.86      0.39         7\n",
      "        1131       0.67      0.40      0.50         5\n",
      "        1132       1.00      0.60      0.75         5\n",
      "        1133       1.00      0.58      0.73        19\n",
      "        1134       0.00      0.00      0.00         1\n",
      "        1135       1.00      0.67      0.80        18\n",
      "        1136       0.70      0.54      0.61        13\n",
      "        1137       0.67      0.55      0.60        11\n",
      "        1138       0.86      0.86      0.86         7\n",
      "        1139       0.88      0.50      0.64        14\n",
      "        1140       0.92      0.55      0.69        20\n",
      "        1141       1.00      0.93      0.96        14\n",
      "        1142       1.00      0.88      0.94        17\n",
      "        1143       0.80      0.67      0.73         6\n",
      "        1144       0.94      0.89      0.91        18\n",
      "        1145       1.00      0.50      0.67         2\n",
      "        1146       1.00      0.58      0.74        12\n",
      "        1147       1.00      0.78      0.88        18\n",
      "        1148       0.00      0.00      0.00         2\n",
      "        1149       0.92      0.75      0.83        16\n",
      "        1150       0.00      0.00      0.00         2\n",
      "        1151       0.00      0.00      0.00         1\n",
      "        1152       1.00      0.29      0.44        14\n",
      "        1153       1.00      0.33      0.50         3\n",
      "        1154       1.00      0.43      0.60         7\n",
      "        1155       0.00      0.00      0.00         4\n",
      "        1156       1.00      0.78      0.88        18\n",
      "        1157       1.00      0.50      0.67         8\n",
      "        1158       0.87      0.65      0.74        20\n",
      "        1159       0.76      0.81      0.79        16\n",
      "        1160       0.93      0.88      0.90        16\n",
      "        1161       0.50      0.25      0.33         4\n",
      "        1162       0.92      0.92      0.92        12\n",
      "        1163       1.00      0.67      0.80        12\n",
      "        1164       0.88      0.44      0.58        16\n",
      "        1165       0.00      0.00      0.00         2\n",
      "        1166       1.00      0.33      0.50         6\n",
      "        1167       0.50      0.33      0.40         3\n",
      "        1168       1.00      0.64      0.78        14\n",
      "        1169       0.00      0.00      0.00         4\n",
      "        1170       1.00      0.60      0.75         5\n",
      "        1171       1.00      0.79      0.88        14\n",
      "        1172       1.00      0.83      0.91        24\n",
      "        1173       0.71      0.62      0.67         8\n",
      "        1174       1.00      0.44      0.62         9\n",
      "        1175       1.00      0.60      0.75        10\n",
      "        1176       1.00      0.17      0.29         6\n",
      "        1177       0.00      0.00      0.00         1\n",
      "        1178       0.71      0.50      0.59        10\n",
      "        1179       0.33      0.20      0.25         5\n",
      "        1180       0.57      0.44      0.50         9\n",
      "        1181       0.86      0.38      0.52        16\n",
      "        1182       0.00      0.00      0.00         2\n",
      "        1183       1.00      0.96      0.98        27\n",
      "        1184       1.00      0.81      0.89        21\n",
      "        1185       0.92      0.52      0.67        23\n",
      "        1186       0.67      0.40      0.50        15\n",
      "        1187       1.00      0.80      0.89        20\n",
      "        1188       1.00      0.40      0.57        10\n",
      "        1189       0.94      0.67      0.78        24\n",
      "        1190       0.81      0.57      0.67        23\n",
      "        1191       0.82      0.82      0.82        17\n",
      "        1192       0.62      0.28      0.38        18\n",
      "        1193       0.82      0.56      0.67        16\n",
      "        1194       1.00      0.50      0.67        10\n",
      "        1195       1.00      0.40      0.57         5\n",
      "        1196       1.00      0.78      0.88         9\n",
      "        1197       1.00      0.82      0.90        17\n",
      "        1198       0.93      0.76      0.84        17\n",
      "        1199       0.95      1.00      0.98        21\n",
      "        1200       0.75      0.55      0.63        11\n",
      "        1201       0.95      0.83      0.89        24\n",
      "        1202       0.78      1.00      0.88         7\n",
      "        1203       0.94      0.76      0.84        21\n",
      "        1204       1.00      0.62      0.77        16\n",
      "        1205       1.00      0.69      0.82        13\n",
      "        1206       1.00      0.29      0.44         7\n",
      "        1207       1.00      0.50      0.67         2\n",
      "        1208       1.00      0.75      0.86        16\n",
      "        1209       0.34      0.56      0.43        18\n",
      "        1210       1.00      0.92      0.96        12\n",
      "        1211       0.86      0.67      0.75         9\n",
      "        1212       0.78      0.82      0.80        17\n",
      "        1213       1.00      0.75      0.86         8\n",
      "        1214       1.00      0.75      0.86        16\n",
      "        1215       1.00      0.50      0.67         2\n",
      "        1216       1.00      0.58      0.74        12\n",
      "        1217       1.00      0.50      0.67         2\n",
      "        1218       0.43      0.38      0.40         8\n",
      "        1219       1.00      0.50      0.67         6\n",
      "        1220       1.00      0.30      0.46        10\n",
      "        1221       1.00      0.56      0.72        16\n",
      "        1222       1.00      0.48      0.65        23\n",
      "        1223       1.00      0.60      0.75         5\n",
      "        1224       0.50      0.69      0.58        16\n",
      "        1225       1.00      0.77      0.87        22\n",
      "        1226       1.00      0.69      0.82        26\n",
      "        1227       1.00      0.50      0.67         8\n",
      "        1228       0.92      0.69      0.79        16\n",
      "        1229       1.00      0.89      0.94        18\n",
      "        1230       0.80      0.67      0.73        18\n",
      "        1231       1.00      0.44      0.62         9\n",
      "        1232       1.00      0.91      0.95        11\n",
      "        1233       0.67      0.43      0.52        14\n",
      "        1234       0.90      0.82      0.86        22\n",
      "        1235       0.86      0.38      0.52        16\n",
      "        1236       1.00      0.57      0.73         7\n",
      "        1237       0.00      0.00      0.00         3\n",
      "        1238       0.27      0.60      0.37         5\n",
      "        1239       1.00      0.67      0.80         3\n",
      "        1240       1.00      0.27      0.42        15\n",
      "        1241       1.00      0.36      0.53        11\n",
      "        1242       0.88      0.62      0.73        24\n",
      "        1243       0.00      0.00      0.00         2\n",
      "        1244       1.00      0.89      0.94        19\n",
      "        1245       1.00      0.50      0.67         2\n",
      "        1246       1.00      0.75      0.86        20\n",
      "        1247       1.00      0.48      0.65        21\n",
      "        1248       0.71      0.50      0.59        10\n",
      "        1249       1.00      0.50      0.67         4\n",
      "        1250       1.00      0.33      0.50         3\n",
      "        1251       1.00      0.62      0.76        21\n",
      "        1252       0.80      0.84      0.82        19\n",
      "        1253       0.00      0.00      0.00         4\n",
      "        1254       0.56      0.36      0.43        14\n",
      "        1255       0.62      0.62      0.62         8\n",
      "        1256       1.00      0.83      0.91        24\n",
      "        1257       0.80      0.67      0.73        12\n",
      "        1258       0.88      0.68      0.77        22\n",
      "        1259       0.00      0.00      0.00         3\n",
      "        1260       1.00      0.88      0.93         8\n",
      "        1261       1.00      0.45      0.62        11\n",
      "        1262       1.00      0.79      0.88        14\n",
      "        1263       0.90      0.64      0.75        14\n",
      "        1264       1.00      0.40      0.57        15\n",
      "        1265       0.50      0.88      0.64         8\n",
      "        1266       1.00      0.82      0.90        22\n",
      "        1267       0.92      0.96      0.94        23\n",
      "        1268       1.00      0.73      0.85        15\n",
      "        1269       0.40      0.67      0.50         3\n",
      "        1270       0.80      0.67      0.73         6\n",
      "        1271       1.00      0.88      0.93         8\n",
      "        1272       0.80      0.67      0.73        12\n",
      "        1273       1.00      0.60      0.75        20\n",
      "        1274       1.00      0.46      0.63        13\n",
      "        1275       0.67      0.80      0.73        15\n",
      "        1276       1.00      0.67      0.80         9\n",
      "        1277       1.00      0.75      0.86        16\n",
      "        1278       0.20      0.52      0.29        23\n",
      "        1279       1.00      0.33      0.50         9\n",
      "        1280       0.83      0.50      0.62        10\n",
      "        1281       0.00      0.00      0.00         5\n",
      "        1282       1.00      0.56      0.71         9\n",
      "        1283       1.00      0.57      0.73        14\n",
      "        1284       1.00      0.67      0.80         3\n",
      "        1285       1.00      0.70      0.82        23\n",
      "        1286       1.00      0.69      0.81        16\n",
      "        1287       1.00      0.68      0.81        22\n",
      "        1288       1.00      0.89      0.94        18\n",
      "        1289       1.00      0.85      0.92        26\n",
      "        1290       0.92      0.69      0.79        16\n",
      "        1291       0.62      0.31      0.42        16\n",
      "        1292       0.00      0.00      0.00         3\n",
      "        1293       1.00      0.87      0.93        23\n",
      "        1294       0.00      0.00      0.00         3\n",
      "        1295       1.00      0.43      0.60        14\n",
      "        1296       1.00      0.81      0.89        21\n",
      "        1297       1.00      0.53      0.69        19\n",
      "        1298       0.91      0.71      0.80        14\n",
      "        1299       0.57      0.67      0.62         6\n",
      "        1300       1.00      0.31      0.47        13\n",
      "        1301       0.00      0.00      0.00         4\n",
      "        1302       0.93      0.70      0.80        20\n",
      "        1303       1.00      0.56      0.71         9\n",
      "        1304       0.80      0.36      0.50        11\n",
      "        1305       0.81      0.74      0.77        23\n",
      "        1306       0.72      0.65      0.68        20\n",
      "        1307       0.00      0.00      0.00         4\n",
      "        1308       0.57      0.63      0.60        19\n",
      "        1309       1.00      0.75      0.86        20\n",
      "        1310       1.00      0.43      0.60         7\n",
      "        1311       1.00      0.92      0.96        24\n",
      "        1312       1.00      0.71      0.83        17\n",
      "        1313       1.00      0.56      0.71         9\n",
      "        1314       1.00      0.14      0.25         7\n",
      "        1315       1.00      0.89      0.94        18\n",
      "        1316       1.00      0.60      0.75        10\n",
      "        1317       1.00      0.90      0.95        21\n",
      "        1318       1.00      0.25      0.40        24\n",
      "        1319       0.78      0.88      0.82         8\n",
      "        1320       1.00      1.00      1.00         4\n",
      "        1321       1.00      0.73      0.84        11\n",
      "        1322       1.00      1.00      1.00         7\n",
      "        1323       1.00      0.95      0.98        22\n",
      "        1324       1.00      0.62      0.77        16\n",
      "        1325       0.44      0.65      0.53        23\n",
      "        1326       1.00      0.79      0.88        14\n",
      "        1327       1.00      0.62      0.77        16\n",
      "        1328       1.00      0.83      0.90        23\n",
      "        1329       0.78      0.78      0.78         9\n",
      "        1330       0.00      0.00      0.00         2\n",
      "        1331       1.00      0.83      0.91        24\n",
      "        1332       1.00      0.86      0.92        21\n",
      "        1333       1.00      0.42      0.59        12\n",
      "        1334       1.00      0.60      0.75        15\n",
      "        1335       1.00      0.71      0.83        17\n",
      "        1336       1.00      0.73      0.84        22\n",
      "        1337       1.00      0.56      0.71         9\n",
      "        1338       1.00      0.52      0.69        23\n",
      "        1339       1.00      0.75      0.86         8\n",
      "        1340       1.00      0.82      0.90        11\n",
      "        1341       1.00      0.90      0.95        21\n",
      "        1342       1.00      0.90      0.95        10\n",
      "        1343       1.00      1.00      1.00         6\n",
      "        1344       0.84      0.67      0.74        24\n",
      "        1345       0.94      0.79      0.86        19\n",
      "        1346       1.00      0.84      0.91        19\n",
      "        1347       1.00      0.36      0.53        14\n",
      "        1348       0.00      0.00      0.00         1\n",
      "        1349       1.00      0.50      0.67         8\n",
      "        1350       1.00      0.40      0.57         5\n",
      "        1351       0.57      0.72      0.63        18\n",
      "        1352       1.00      0.82      0.90        17\n",
      "        1353       0.53      0.67      0.59        12\n",
      "        1354       1.00      0.62      0.77        16\n",
      "        1355       0.91      0.48      0.62        21\n",
      "        1356       0.95      0.90      0.92        20\n",
      "        1357       1.00      0.75      0.86         4\n",
      "        1358       0.83      0.62      0.71         8\n",
      "        1359       1.00      0.46      0.63        13\n",
      "        1360       1.00      0.65      0.79        20\n",
      "        1361       1.00      0.50      0.67         6\n",
      "        1362       1.00      0.17      0.29         6\n",
      "        1363       1.00      0.80      0.89        15\n",
      "        1364       0.67      0.40      0.50         5\n",
      "        1365       0.00      0.00      0.00         1\n",
      "        1366       0.92      0.67      0.77        18\n",
      "        1367       0.79      0.65      0.71        17\n",
      "        1368       0.54      0.47      0.50        15\n",
      "        1369       1.00      0.86      0.93        22\n",
      "        1370       1.00      0.62      0.77        16\n",
      "        1371       1.00      0.78      0.88         9\n",
      "        1372       1.00      0.50      0.67        20\n",
      "        1373       1.00      0.44      0.62         9\n",
      "        1374       1.00      0.70      0.82        20\n",
      "        1375       1.00      0.60      0.75         5\n",
      "        1376       1.00      0.73      0.85        15\n",
      "        1377       0.25      0.50      0.33        10\n",
      "        1378       0.44      0.55      0.49        20\n",
      "        1379       1.00      0.20      0.33         5\n",
      "        1380       1.00      0.89      0.94        18\n",
      "        1381       1.00      0.36      0.53        14\n",
      "        1382       1.00      0.59      0.74        17\n",
      "        1383       1.00      0.47      0.64        19\n",
      "        1384       0.95      0.86      0.90        22\n",
      "        1385       1.00      0.40      0.57         5\n",
      "        1386       0.78      0.78      0.78        18\n",
      "        1387       1.00      0.62      0.76        21\n",
      "        1388       0.00      0.00      0.00         3\n",
      "        1389       1.00      0.43      0.60         7\n",
      "        1390       0.50      0.75      0.60         4\n",
      "        1391       0.88      0.79      0.83        19\n",
      "        1392       1.00      0.87      0.93        23\n",
      "        1393       1.00      0.43      0.60         7\n",
      "        1394       1.00      0.41      0.58        17\n",
      "        1395       0.00      0.00      0.00         2\n",
      "        1396       1.00      0.45      0.62        11\n",
      "        1397       0.00      0.00      0.00         3\n",
      "        1398       0.00      0.00      0.00         5\n",
      "        1399       0.63      0.63      0.63        19\n",
      "        1400       1.00      0.62      0.77        16\n",
      "        1401       1.00      0.75      0.86         8\n",
      "        1402       0.79      0.52      0.63        21\n",
      "        1403       1.00      0.40      0.57         5\n",
      "        1404       0.00      0.00      0.00         2\n",
      "        1405       1.00      0.67      0.80         6\n",
      "        1406       1.00      0.59      0.74        17\n",
      "        1407       0.35      0.71      0.47        17\n",
      "        1408       0.88      0.79      0.83        19\n",
      "        1409       1.00      0.80      0.89        10\n",
      "        1410       1.00      0.50      0.67        14\n",
      "        1411       0.94      0.81      0.87        21\n",
      "        1412       0.91      0.77      0.83        13\n",
      "        1414       1.00      0.61      0.76        23\n",
      "        1415       0.84      0.64      0.73        25\n",
      "        1416       0.27      0.21      0.24        14\n",
      "        1417       1.00      0.90      0.95        21\n",
      "        1418       1.00      0.12      0.22         8\n",
      "        1419       0.85      0.96      0.90        23\n",
      "        1420       1.00      0.95      0.98        21\n",
      "        1421       1.00      0.33      0.50         9\n",
      "        1422       0.50      0.52      0.51        21\n",
      "        1423       1.00      0.82      0.90        17\n",
      "        1424       1.00      0.96      0.98        25\n",
      "        1425       1.00      0.90      0.95        20\n",
      "        1426       0.00      0.00      0.00         1\n",
      "        1427       0.46      0.60      0.52        10\n",
      "        1428       1.00      0.40      0.57        10\n",
      "        1429       1.00      0.58      0.74        12\n",
      "        1430       1.00      0.29      0.44         7\n",
      "        1431       0.38      0.30      0.33        10\n",
      "        1432       0.00      0.00      0.00         4\n",
      "        1433       0.47      0.44      0.45        16\n",
      "        1434       1.00      1.00      1.00         3\n",
      "        1435       0.50      0.67      0.57         9\n",
      "        1436       0.80      0.47      0.59        17\n",
      "        1437       1.00      0.78      0.88        18\n",
      "        1438       1.00      0.25      0.40         4\n",
      "        1439       0.00      0.00      0.00         2\n",
      "        1440       1.00      0.40      0.57        10\n",
      "        1441       0.67      1.00      0.80         2\n",
      "        1442       0.57      0.67      0.62         6\n",
      "        1443       1.00      0.61      0.76        18\n",
      "        1444       1.00      0.65      0.79        20\n",
      "        1445       1.00      0.50      0.67        12\n",
      "        1446       1.00      0.76      0.87        17\n",
      "        1447       1.00      0.50      0.67         8\n",
      "        1448       1.00      0.90      0.95        10\n",
      "        1449       1.00      0.62      0.77        16\n",
      "        1450       0.71      0.68      0.70        22\n",
      "        1451       1.00      0.50      0.67         4\n",
      "        1452       1.00      0.74      0.85        23\n",
      "        1453       0.91      0.71      0.80        14\n",
      "        1454       1.00      0.57      0.73         7\n",
      "        1455       0.83      0.56      0.67        18\n",
      "        1456       1.00      0.62      0.77         8\n",
      "        1457       0.75      0.43      0.55        14\n",
      "        1458       1.00      0.50      0.67         8\n",
      "        1459       0.91      0.71      0.80        14\n",
      "        1460       1.00      0.75      0.86         4\n",
      "        1461       1.00      0.78      0.88        23\n",
      "        1462       1.00      0.78      0.88         9\n",
      "        1463       1.00      0.45      0.62        11\n",
      "        1464       0.75      0.43      0.55         7\n",
      "        1465       0.00      0.00      0.00         2\n",
      "        1466       1.00      0.20      0.33         5\n",
      "        1467       1.00      0.64      0.78        11\n",
      "        1468       0.00      0.00      0.00         2\n",
      "        1469       0.00      0.00      0.00         2\n",
      "        1470       0.67      0.50      0.57         4\n",
      "        1471       0.71      0.83      0.77         6\n",
      "        1472       0.93      0.82      0.87        17\n",
      "        1473       0.57      0.73      0.64        11\n",
      "        1474       1.00      0.28      0.43        18\n",
      "        1475       1.00      0.50      0.67         2\n",
      "        1476       0.00      0.00      0.00         4\n",
      "        1477       1.00      0.44      0.62         9\n",
      "        1478       1.00      0.82      0.90        17\n",
      "        1479       1.00      0.52      0.69        21\n",
      "        1480       1.00      0.67      0.80         6\n",
      "        1481       0.82      0.82      0.82        11\n",
      "        1482       1.00      0.82      0.90        17\n",
      "        1483       0.00      0.00      0.00         2\n",
      "        1484       0.00      0.00      0.00         2\n",
      "        1485       0.00      0.00      0.00         3\n",
      "        1486       1.00      0.67      0.80        12\n",
      "        1487       0.56      0.83      0.67         6\n",
      "        1488       0.69      0.64      0.67        14\n",
      "        1489       1.00      0.78      0.88         9\n",
      "        1490       1.00      0.68      0.81        19\n",
      "        1491       1.00      0.69      0.82        13\n",
      "        1492       1.00      0.89      0.94        18\n",
      "        1493       1.00      0.77      0.87        22\n",
      "        1494       0.65      0.61      0.63        18\n",
      "        1495       0.92      0.67      0.77        18\n",
      "        1496       1.00      0.36      0.53        11\n",
      "        1497       0.00      0.00      0.00         3\n",
      "        1498       0.33      0.50      0.40        14\n",
      "        1499       0.73      0.67      0.70        12\n",
      "        1500       1.00      0.29      0.44         7\n",
      "        1501       1.00      0.91      0.95        22\n",
      "        1502       0.00      0.00      0.00         1\n",
      "        1503       1.00      0.86      0.93        22\n",
      "        1504       1.00      0.33      0.50         3\n",
      "        1505       1.00      0.40      0.57         5\n",
      "        1506       1.00      0.90      0.95        20\n",
      "        1507       1.00      0.88      0.93         8\n",
      "        1508       0.29      0.72      0.41        18\n",
      "        1509       0.64      0.69      0.67        13\n",
      "        1510       1.00      0.71      0.83        21\n",
      "        1511       0.83      0.92      0.87        26\n",
      "        1512       0.94      0.79      0.86        19\n",
      "        1513       0.00      0.00      0.00         4\n",
      "        1514       0.89      0.57      0.70        14\n",
      "        1515       1.00      0.63      0.77        19\n",
      "        1516       1.00      0.27      0.43        22\n",
      "        1517       1.00      0.79      0.88        19\n",
      "        1518       0.56      0.83      0.67         6\n",
      "        1519       1.00      0.83      0.91        12\n",
      "        1520       1.00      0.71      0.83        21\n",
      "        1521       1.00      0.50      0.67         4\n",
      "        1522       0.90      0.69      0.78        13\n",
      "        1523       1.00      0.33      0.50         3\n",
      "        1524       0.75      0.50      0.60         6\n",
      "        1525       0.00      0.00      0.00         2\n",
      "        1526       1.00      1.00      1.00        20\n",
      "        1527       1.00      0.75      0.86        20\n",
      "        1528       1.00      0.36      0.53        14\n",
      "        1529       0.78      0.86      0.82        21\n",
      "        1530       0.94      0.80      0.86        20\n",
      "        1531       0.54      0.64      0.58        11\n",
      "        1532       0.80      0.67      0.73         6\n",
      "        1533       0.00      0.00      0.00         8\n",
      "        1534       1.00      0.52      0.69        21\n",
      "        1535       1.00      0.68      0.81        19\n",
      "        1536       0.77      0.59      0.67        17\n",
      "        1537       1.00      0.83      0.90        23\n",
      "        1538       1.00      0.67      0.80        21\n",
      "        1539       1.00      0.55      0.71        11\n",
      "        1540       1.00      0.76      0.86        25\n",
      "        1541       1.00      0.95      0.98        22\n",
      "        1542       1.00      0.38      0.56        13\n",
      "        1543       0.00      0.00      0.00         1\n",
      "        1544       0.67      1.00      0.80         2\n",
      "        1545       0.88      0.68      0.77        22\n",
      "        1546       1.00      0.73      0.84        11\n",
      "        1547       1.00      0.53      0.69        19\n",
      "        1548       0.00      0.00      0.00         4\n",
      "        1549       1.00      0.60      0.75        10\n",
      "        1550       0.00      0.00      0.00         2\n",
      "        1551       1.00      0.54      0.70        13\n",
      "        1552       0.92      0.86      0.89        14\n",
      "        1553       1.00      0.79      0.88        14\n",
      "        1554       0.74      0.74      0.74        19\n",
      "        1555       1.00      0.25      0.40         4\n",
      "        1556       0.73      0.84      0.78        19\n",
      "        1557       1.00      0.88      0.94        17\n",
      "        1558       1.00      0.84      0.91        19\n",
      "        1559       1.00      0.79      0.88        14\n",
      "        1560       0.25      0.67      0.36         9\n",
      "        1561       0.43      0.59      0.50        17\n",
      "        1562       1.00      0.42      0.59        19\n",
      "        1563       0.81      0.85      0.83        20\n",
      "        1564       1.00      0.33      0.50         3\n",
      "        1565       1.00      0.80      0.89         5\n",
      "        1566       0.81      0.59      0.68        22\n",
      "        1567       1.00      0.53      0.69        17\n",
      "        1568       1.00      0.55      0.71        11\n",
      "        1569       1.00      0.71      0.83         7\n",
      "        1570       0.61      0.71      0.65        24\n",
      "        1571       1.00      0.92      0.96        13\n",
      "        1572       1.00      0.14      0.25         7\n",
      "        1573       0.58      0.65      0.61        17\n",
      "        1574       0.87      0.59      0.70        22\n",
      "        1575       0.23      0.46      0.31        13\n",
      "        1576       0.00      0.00      0.00         8\n",
      "        1577       1.00      0.70      0.82        10\n",
      "        1578       1.00      0.72      0.84        18\n",
      "        1579       1.00      0.70      0.82        23\n",
      "        1580       1.00      0.53      0.69        19\n",
      "        1581       0.82      0.69      0.75        13\n",
      "        1582       0.85      1.00      0.92        11\n",
      "        1583       1.00      0.43      0.60         7\n",
      "        1584       0.60      0.60      0.60        10\n",
      "        1585       0.94      0.85      0.89        20\n",
      "        1586       1.00      0.33      0.50         9\n",
      "        1587       0.72      1.00      0.84        13\n",
      "        1588       1.00      0.77      0.87        13\n",
      "        1589       0.87      0.59      0.70        22\n",
      "        1590       1.00      0.48      0.65        21\n",
      "        1591       1.00      0.70      0.82        23\n",
      "        1592       1.00      0.75      0.86        12\n",
      "        1593       1.00      0.65      0.79        23\n",
      "        1594       1.00      0.14      0.25         7\n",
      "        1595       1.00      0.38      0.55         8\n",
      "        1596       0.50      0.33      0.40         9\n",
      "        1597       1.00      0.64      0.78        11\n",
      "        1598       0.50      0.60      0.55         5\n",
      "        1599       0.76      0.81      0.79        16\n",
      "        1600       1.00      0.71      0.83        17\n",
      "        1601       1.00      0.62      0.77         8\n",
      "        1602       0.80      0.67      0.73        18\n",
      "        1603       1.00      0.82      0.90        17\n",
      "        1604       1.00      0.53      0.69        17\n",
      "        1605       0.00      0.00      0.00         2\n",
      "        1606       0.83      0.56      0.67         9\n",
      "        1607       0.67      0.17      0.27        12\n",
      "        1608       0.75      0.71      0.73        17\n",
      "        1609       0.94      0.94      0.94        18\n",
      "        1610       1.00      0.53      0.69        17\n",
      "        1611       1.00      0.50      0.67         2\n",
      "        1612       1.00      0.38      0.55         8\n",
      "        1613       0.80      0.89      0.84         9\n",
      "        1614       0.75      1.00      0.86         6\n",
      "        1615       1.00      0.70      0.82        10\n",
      "        1616       0.00      0.00      0.00         2\n",
      "        1617       0.64      0.67      0.65        21\n",
      "        1618       0.86      0.55      0.67        11\n",
      "        1619       1.00      0.50      0.67        10\n",
      "        1620       0.00      0.00      0.00         3\n",
      "        1621       1.00      0.69      0.81        16\n",
      "        1622       0.47      0.62      0.53        13\n",
      "        1623       1.00      0.50      0.67         6\n",
      "        1624       1.00      0.77      0.87        22\n",
      "        1625       1.00      0.50      0.67        14\n",
      "        1626       1.00      0.59      0.74        17\n",
      "        1627       0.86      0.75      0.80         8\n",
      "        1628       1.00      0.80      0.89         5\n",
      "        1629       0.83      0.62      0.71         8\n",
      "        1630       0.93      0.87      0.90        15\n",
      "        1631       0.82      0.82      0.82        11\n",
      "        1632       0.87      0.65      0.74        20\n",
      "        1633       0.48      0.77      0.59        13\n",
      "        1634       1.00      0.50      0.67         2\n",
      "        1635       1.00      0.57      0.73         7\n",
      "        1636       1.00      0.73      0.85        15\n",
      "        1637       1.00      0.38      0.55        16\n",
      "        1638       0.67      0.57      0.62        14\n",
      "        1639       1.00      0.44      0.62         9\n",
      "        1640       1.00      0.67      0.80        12\n",
      "        1641       1.00      0.91      0.95        22\n",
      "        1642       1.00      0.71      0.83        21\n",
      "        1643       0.88      0.78      0.82         9\n",
      "        1644       0.00      0.00      0.00         4\n",
      "        1645       1.00      0.71      0.83        14\n",
      "        1646       1.00      0.62      0.77        16\n",
      "        1647       0.95      0.75      0.84        24\n",
      "        1648       1.00      0.57      0.73        14\n",
      "        1649       1.00      0.50      0.67         8\n",
      "        1650       0.69      0.75      0.72        12\n",
      "        1651       0.50      0.68      0.58        22\n",
      "        1652       1.00      0.48      0.65        21\n",
      "        1653       0.75      0.50      0.60         6\n",
      "        1654       1.00      0.57      0.73        14\n",
      "        1655       1.00      0.67      0.80        24\n",
      "        1656       1.00      0.50      0.67        16\n",
      "        1657       0.94      0.84      0.89        19\n",
      "        1658       1.00      0.67      0.80         9\n",
      "        1659       1.00      0.81      0.90        16\n",
      "        1660       1.00      0.57      0.73        14\n",
      "        1661       0.94      0.67      0.78        24\n",
      "        1662       0.00      0.00      0.00         3\n",
      "        1663       1.00      0.50      0.67         8\n",
      "        1664       0.64      0.64      0.64        11\n",
      "        1665       1.00      0.67      0.80         6\n",
      "        1666       1.00      0.50      0.67         2\n",
      "        1667       1.00      0.33      0.50         3\n",
      "        1668       0.82      0.56      0.67        16\n",
      "        1669       1.00      0.40      0.57        10\n",
      "        1670       0.00      0.00      0.00         2\n",
      "        1671       0.71      0.79      0.75        19\n",
      "        1672       0.00      0.00      0.00         3\n",
      "        1673       0.57      0.67      0.62         6\n",
      "        1674       1.00      0.75      0.86        16\n",
      "        1675       1.00      0.50      0.67         8\n",
      "        1676       1.00      0.71      0.83        14\n",
      "        1677       0.75      0.67      0.71         9\n",
      "        1678       1.00      0.33      0.50         3\n",
      "        1679       1.00      0.90      0.95        21\n",
      "        1680       1.00      0.67      0.80        15\n",
      "        1681       0.53      0.50      0.52        16\n",
      "        1682       1.00      0.45      0.62        11\n",
      "        1683       1.00      0.27      0.43        11\n",
      "        1684       0.91      0.77      0.83        13\n",
      "        1685       0.84      0.70      0.76        23\n",
      "        1686       0.87      0.57      0.68        23\n",
      "        1687       1.00      0.85      0.92        13\n",
      "        1688       0.00      0.00      0.00         4\n",
      "        1689       0.83      0.53      0.65        19\n",
      "        1690       1.00      0.71      0.83        14\n",
      "        1691       0.92      0.67      0.77        18\n",
      "        1692       0.60      0.38      0.46         8\n",
      "        1693       1.00      0.56      0.71        18\n",
      "        1694       0.35      0.73      0.47        11\n",
      "        1695       0.53      0.59      0.56        17\n",
      "        1696       1.00      0.93      0.97        15\n",
      "        1697       0.00      0.00      0.00         2\n",
      "        1698       0.90      0.90      0.90        10\n",
      "        1699       0.00      0.00      0.00         4\n",
      "        1700       1.00      0.47      0.64        15\n",
      "        1701       0.00      0.00      0.00         3\n",
      "        1702       0.67      0.75      0.71         8\n",
      "        1703       0.50      0.20      0.29         5\n",
      "        1704       1.00      0.71      0.83        17\n",
      "        1705       0.00      0.00      0.00         2\n",
      "        1706       1.00      0.33      0.50         3\n",
      "        1707       0.22      0.40      0.29        10\n",
      "        1708       0.62      0.67      0.65        15\n",
      "        1709       0.38      0.55      0.44        11\n",
      "        1710       0.00      0.00      0.00         2\n",
      "        1711       1.00      0.70      0.82        23\n",
      "        1712       1.00      0.38      0.56        13\n",
      "        1713       0.80      0.70      0.74        23\n",
      "        1714       0.95      0.91      0.93        22\n",
      "        1715       1.00      0.80      0.89        15\n",
      "        1716       1.00      0.68      0.81        22\n",
      "        1717       1.00      0.25      0.40         4\n",
      "        1718       1.00      0.64      0.78        14\n",
      "        1719       0.00      0.00      0.00         1\n",
      "        1720       1.00      0.20      0.33         5\n",
      "        1721       1.00      0.79      0.88        19\n",
      "        1722       1.00      0.47      0.64        19\n",
      "        1723       1.00      0.77      0.87        13\n",
      "        1724       0.90      0.82      0.86        11\n",
      "        1725       1.00      0.65      0.79        23\n",
      "        1726       0.95      0.95      0.95        22\n",
      "        1727       1.00      0.67      0.80         3\n",
      "        1728       1.00      0.50      0.67        16\n",
      "        1729       1.00      0.50      0.67         8\n",
      "        1730       0.75      0.43      0.55         7\n",
      "        1731       1.00      0.36      0.53        14\n",
      "        1732       0.85      0.73      0.79        15\n",
      "        1733       1.00      0.79      0.88        19\n",
      "        1734       1.00      0.78      0.88        23\n",
      "        1735       1.00      0.67      0.80         9\n",
      "        1736       1.00      0.38      0.55         8\n",
      "        1737       0.92      0.75      0.83        16\n",
      "        1738       0.00      0.00      0.00         1\n",
      "        1739       1.00      0.60      0.75        15\n",
      "        1740       1.00      1.00      1.00         1\n",
      "        1741       1.00      0.42      0.59        12\n",
      "        1742       1.00      0.75      0.86        20\n",
      "        1743       0.40      0.61      0.48        23\n",
      "        1744       1.00      0.76      0.86        21\n",
      "        1745       1.00      0.67      0.80         6\n",
      "        1746       0.59      0.71      0.65        14\n",
      "        1747       1.00      0.55      0.71        20\n",
      "        1748       0.60      0.27      0.37        11\n",
      "        1749       1.00      0.17      0.29         6\n",
      "        1750       1.00      0.50      0.67         2\n",
      "        1751       1.00      0.68      0.81        22\n",
      "        1752       1.00      0.70      0.82        10\n",
      "        1753       1.00      0.25      0.40         4\n",
      "        1754       1.00      0.80      0.89        20\n",
      "        1755       0.95      0.78      0.86        23\n",
      "        1756       1.00      0.90      0.95        21\n",
      "        1757       0.94      0.89      0.92        19\n",
      "        1758       1.00      0.62      0.77         8\n",
      "        1759       0.91      0.83      0.87        12\n",
      "        1760       1.00      0.76      0.87        17\n",
      "        1761       0.57      0.67      0.62         6\n",
      "        1762       1.00      0.44      0.61        16\n",
      "        1763       1.00      0.40      0.57        10\n",
      "        1764       0.67      0.80      0.73        20\n",
      "        1765       0.00      0.00      0.00         4\n",
      "        1766       1.00      0.78      0.88         9\n",
      "        1767       0.83      0.31      0.45        16\n",
      "        1768       1.00      1.00      1.00         7\n",
      "        1769       0.94      0.81      0.87        21\n",
      "        1770       1.00      0.88      0.94        17\n",
      "        1771       1.00      0.81      0.90        16\n",
      "        1772       1.00      0.80      0.89        20\n",
      "        1773       0.94      0.75      0.83        20\n",
      "        1774       0.91      0.77      0.83        13\n",
      "        1775       0.32      0.50      0.39        12\n",
      "        1776       1.00      0.81      0.89        21\n",
      "        1777       0.64      0.78      0.70         9\n",
      "        1778       0.94      0.81      0.87        21\n",
      "        1779       0.73      0.73      0.73        22\n",
      "        1780       0.87      0.59      0.70        22\n",
      "        1781       1.00      0.84      0.91        19\n",
      "        1782       0.82      0.61      0.70        23\n",
      "        1783       0.76      0.67      0.71        24\n",
      "        1784       1.00      1.00      1.00         6\n",
      "        1785       0.60      0.60      0.60         5\n",
      "        1786       0.89      0.85      0.87        20\n",
      "        1787       1.00      0.50      0.67        16\n",
      "        1788       0.52      0.65      0.58        23\n",
      "        1789       0.00      0.00      0.00         2\n",
      "        1790       0.71      0.42      0.53        12\n",
      "        1791       0.33      0.27      0.30        11\n",
      "        1792       0.94      0.75      0.83        20\n",
      "        1793       0.00      0.00      0.00         2\n",
      "        1794       1.00      0.61      0.76        23\n",
      "        1795       0.40      0.29      0.33        14\n",
      "        1796       1.00      0.59      0.74        17\n",
      "        1797       1.00      0.50      0.67         2\n",
      "        1798       0.87      0.59      0.70        22\n",
      "        1799       1.00      0.62      0.77         8\n",
      "        1800       0.92      0.50      0.65        22\n",
      "        1801       0.94      0.77      0.85        22\n",
      "        1802       0.00      0.00      0.00         1\n",
      "        1803       0.83      0.71      0.77        14\n",
      "        1804       1.00      0.78      0.88         9\n",
      "        1805       0.91      0.77      0.83        13\n",
      "        1806       1.00      0.80      0.89        20\n",
      "        1807       0.89      0.62      0.73        13\n",
      "        1808       1.00      0.33      0.50         6\n",
      "        1809       0.27      0.70      0.39        20\n",
      "        1810       0.80      0.47      0.59        17\n",
      "        1811       0.91      0.62      0.74        16\n",
      "        1812       0.80      0.89      0.84         9\n",
      "        1813       1.00      0.60      0.75        10\n",
      "        1814       0.85      0.85      0.85        20\n",
      "        1815       0.20      0.17      0.18         6\n",
      "        1816       0.94      0.75      0.83        20\n",
      "        1817       0.89      0.64      0.74        25\n",
      "        1818       1.00      0.69      0.82        13\n",
      "        1819       1.00      0.71      0.83         7\n",
      "        1820       1.00      0.86      0.92         7\n",
      "        1821       1.00      0.50      0.67         8\n",
      "        1822       1.00      1.00      1.00        13\n",
      "        1823       1.00      0.64      0.78        14\n",
      "        1824       1.00      0.89      0.94         9\n",
      "        1825       1.00      0.75      0.86        20\n",
      "        1826       0.93      0.78      0.85        18\n",
      "        1827       1.00      0.29      0.44         7\n",
      "        1828       1.00      0.43      0.60         7\n",
      "        1829       1.00      0.75      0.86        16\n",
      "        1830       0.00      0.00      0.00         6\n",
      "        1831       1.00      0.59      0.74        22\n",
      "        1832       1.00      0.67      0.80         6\n",
      "        1833       0.93      0.68      0.79        19\n",
      "        1834       0.00      0.00      0.00         6\n",
      "        1835       1.00      0.75      0.86         4\n",
      "        1836       0.65      0.93      0.76        14\n",
      "        1837       0.75      0.64      0.69        14\n",
      "        1838       1.00      0.83      0.90        23\n",
      "        1839       0.09      0.17      0.12         6\n",
      "        1840       0.00      0.00      0.00         3\n",
      "        1841       0.00      0.00      0.00         1\n",
      "        1842       1.00      0.67      0.80        21\n",
      "        1843       1.00      1.00      1.00         5\n",
      "        1844       1.00      0.91      0.95        11\n",
      "        1845       1.00      0.57      0.73        14\n",
      "        1846       0.59      0.48      0.53        21\n",
      "        1847       1.00      0.78      0.88         9\n",
      "        1848       1.00      0.79      0.88        19\n",
      "        1849       1.00      0.75      0.86        16\n",
      "        1850       1.00      0.33      0.50        18\n",
      "        1851       1.00      0.76      0.87        17\n",
      "        1852       0.00      0.00      0.00         3\n",
      "        1853       1.00      0.64      0.78        14\n",
      "        1854       1.00      0.33      0.50         3\n",
      "        1855       0.70      0.88      0.78        16\n",
      "        1856       1.00      0.82      0.90        17\n",
      "        1857       1.00      0.33      0.50         6\n",
      "        1858       1.00      0.46      0.63        13\n",
      "        1859       1.00      0.30      0.46        20\n",
      "        1860       1.00      0.67      0.80        21\n",
      "        1861       1.00      0.40      0.57         5\n",
      "        1862       0.77      0.62      0.69        16\n",
      "        1863       0.94      0.65      0.77        23\n",
      "        1864       1.00      0.80      0.89         5\n",
      "        1865       0.92      0.52      0.67        23\n",
      "        1866       0.56      0.71      0.63        21\n",
      "        1867       0.75      0.63      0.69        19\n",
      "        1868       0.86      0.86      0.86         7\n",
      "        1869       1.00      0.58      0.74        12\n",
      "        1870       1.00      0.89      0.94        18\n",
      "        1871       0.50      0.50      0.50         4\n",
      "        1872       1.00      0.33      0.50         6\n",
      "        1873       0.33      0.20      0.25         5\n",
      "        1874       1.00      0.89      0.94        18\n",
      "        1875       0.29      0.50      0.36         4\n",
      "        1876       1.00      0.60      0.75         5\n",
      "        1877       1.00      0.69      0.81        16\n",
      "        1878       0.91      0.53      0.67        19\n",
      "        1879       1.00      0.76      0.86        21\n",
      "        1880       1.00      0.25      0.40         4\n",
      "        1881       0.79      0.85      0.81        13\n",
      "        1882       1.00      0.73      0.84        11\n",
      "        1883       0.86      0.90      0.88        21\n",
      "        1884       1.00      0.50      0.67         8\n",
      "        1885       1.00      0.86      0.92        14\n",
      "        1886       1.00      0.62      0.77        16\n",
      "        1887       1.00      0.35      0.52        17\n",
      "        1888       0.67      0.55      0.60        11\n",
      "        1889       1.00      0.46      0.63        13\n",
      "        1890       0.89      0.62      0.73        13\n",
      "        1891       1.00      0.71      0.83         7\n",
      "        1892       0.90      1.00      0.95         9\n",
      "        1893       1.00      0.50      0.67        10\n",
      "        1894       1.00      0.83      0.91        18\n",
      "        1895       1.00      0.83      0.91        24\n",
      "        1896       1.00      0.58      0.74        12\n",
      "        1897       1.00      0.41      0.58        37\n",
      "        1898       1.00      0.46      0.63        13\n",
      "        1899       0.94      0.71      0.81        21\n",
      "        1900       1.00      0.86      0.92         7\n",
      "        1901       1.00      0.60      0.75        25\n",
      "        1902       0.90      0.82      0.86        11\n",
      "        1903       1.00      0.79      0.88        14\n",
      "        1904       1.00      0.83      0.91         6\n",
      "        1905       0.00      0.00      0.00         4\n",
      "        1906       1.00      0.50      0.67         6\n",
      "        1907       0.00      0.00      0.00         4\n",
      "        1908       0.54      0.75      0.63        20\n",
      "        1909       1.00      0.75      0.86         4\n",
      "        1910       1.00      0.89      0.94         9\n",
      "        1911       1.00      0.72      0.84        18\n",
      "        1912       1.00      0.38      0.55         8\n",
      "        1913       0.75      0.50      0.60        12\n",
      "        1914       1.00      0.57      0.73        14\n",
      "        1915       1.00      0.46      0.63        13\n",
      "        1916       0.90      0.83      0.86        23\n",
      "        1917       0.92      0.69      0.79        16\n",
      "        1918       1.00      0.50      0.67         2\n",
      "        1919       0.43      0.40      0.41        15\n",
      "        1920       1.00      0.59      0.74        17\n",
      "        1921       0.00      0.00      0.00         3\n",
      "        1922       0.00      0.00      0.00         3\n",
      "        1923       1.00      0.33      0.50        12\n",
      "        1924       1.00      0.36      0.53        11\n",
      "        1925       1.00      0.73      0.84        11\n",
      "        1926       0.69      0.69      0.69        16\n",
      "        1927       1.00      0.33      0.50         3\n",
      "        1928       0.57      1.00      0.73         4\n",
      "        1929       0.00      0.00      0.00         2\n",
      "        1930       0.45      1.00      0.62        10\n",
      "        1931       1.00      1.00      1.00         4\n",
      "        1932       1.00      0.44      0.62         9\n",
      "        1933       1.00      0.67      0.80         6\n",
      "        1934       1.00      0.67      0.80         6\n",
      "        1935       0.95      0.86      0.90        21\n",
      "        1936       0.88      0.78      0.82        18\n",
      "        1937       1.00      0.80      0.89        15\n",
      "        1938       0.00      0.00      0.00         1\n",
      "        1939       1.00      0.83      0.91        12\n",
      "        1940       1.00      0.75      0.86        16\n",
      "        1941       1.00      0.42      0.59        12\n",
      "        1942       0.90      0.86      0.88        21\n",
      "        1943       0.00      0.00      0.00         2\n",
      "        1944       0.93      0.78      0.85        18\n",
      "        1945       0.80      0.57      0.67         7\n",
      "        1946       1.00      0.40      0.57        10\n",
      "        1947       1.00      0.44      0.62        18\n",
      "        1948       1.00      0.33      0.50         3\n",
      "        1949       0.90      0.78      0.84        23\n",
      "        1950       0.00      0.00      0.00         1\n",
      "        1951       0.95      0.69      0.80        26\n",
      "        1952       1.00      0.58      0.73        19\n",
      "        1953       0.03      0.39      0.05        18\n",
      "        1954       1.00      0.56      0.71         9\n",
      "        1955       1.00      0.89      0.94        18\n",
      "        1956       1.00      0.78      0.88        18\n",
      "        1957       1.00      0.45      0.62        20\n",
      "        1958       1.00      0.71      0.83        17\n",
      "        1959       0.93      0.58      0.72        24\n",
      "        1960       0.71      0.62      0.67         8\n",
      "        1961       0.92      0.65      0.76        17\n",
      "        1962       1.00      0.83      0.91        12\n",
      "        1963       1.00      0.68      0.81        19\n",
      "        1964       1.00      0.62      0.77         8\n",
      "        1965       0.68      0.85      0.76        20\n",
      "        1966       0.00      0.00      0.00         4\n",
      "        1967       0.82      0.74      0.78        19\n",
      "        1968       1.00      0.92      0.96        12\n",
      "        1969       1.00      0.69      0.82        13\n",
      "        1970       0.00      0.00      0.00         3\n",
      "        1971       1.00      0.60      0.75        10\n",
      "        1972       1.00      0.40      0.57        10\n",
      "        1973       1.00      0.52      0.69        21\n",
      "        1974       0.00      0.00      0.00         1\n",
      "        1975       0.91      0.91      0.91        22\n",
      "        1976       0.67      0.80      0.73         5\n",
      "        1977       0.91      0.91      0.91        23\n",
      "        1978       0.71      0.62      0.67         8\n",
      "        1979       1.00      0.88      0.93        16\n",
      "        1980       0.00      0.00      0.00         5\n",
      "        1981       0.00      0.00      0.00         1\n",
      "        1982       1.00      0.67      0.80        21\n",
      "        1983       1.00      0.83      0.91         6\n",
      "        1984       0.75      0.60      0.67         5\n",
      "        1985       1.00      0.76      0.87        17\n",
      "        1986       1.00      0.60      0.75        10\n",
      "        1987       1.00      0.62      0.76        13\n",
      "        1988       1.00      0.25      0.40         4\n",
      "        1989       1.00      0.78      0.88        18\n",
      "        1990       1.00      0.82      0.90        11\n",
      "        1991       0.00      0.00      0.00         2\n",
      "        1992       1.00      0.58      0.74        12\n",
      "        1993       0.27      0.44      0.33         9\n",
      "        1994       0.00      0.00      0.00         6\n",
      "        1995       1.00      0.80      0.89        15\n",
      "        1996       1.00      0.63      0.77        19\n",
      "        1997       1.00      0.88      0.93         8\n",
      "        1998       1.00      0.33      0.50         6\n",
      "        1999       1.00      0.65      0.79        20\n",
      "        2000       1.00      0.64      0.78        11\n",
      "        2001       0.00      0.00      0.00         3\n",
      "        2002       0.94      0.65      0.77        26\n",
      "        2003       1.00      0.50      0.67        18\n",
      "        2004       1.00      0.29      0.44        14\n",
      "        2005       1.00      0.86      0.92        21\n",
      "        2006       1.00      0.63      0.77        19\n",
      "        2007       0.89      0.89      0.89        18\n",
      "        2008       1.00      0.63      0.77        19\n",
      "        2009       1.00      0.91      0.95        11\n",
      "        2010       0.92      0.67      0.77        18\n",
      "        2011       0.75      0.67      0.71         9\n",
      "        2012       0.78      0.82      0.80        17\n",
      "        2013       1.00      0.79      0.88        19\n",
      "        2014       1.00      0.67      0.80         3\n",
      "        2015       0.78      0.47      0.58        15\n",
      "        2016       1.00      0.50      0.67         6\n",
      "        2017       1.00      0.67      0.80         6\n",
      "        2018       1.00      0.80      0.89         5\n",
      "        2019       0.60      0.67      0.63        18\n",
      "        2020       1.00      0.25      0.40         4\n",
      "        2021       0.00      0.00      0.00         2\n",
      "        2022       1.00      0.25      0.40         4\n",
      "        2023       1.00      0.33      0.50        12\n",
      "        2024       0.81      0.87      0.84        15\n",
      "        2025       1.00      0.50      0.67        12\n",
      "        2026       1.00      0.73      0.85        15\n",
      "        2027       1.00      0.70      0.82        20\n",
      "        2028       1.00      0.88      0.93        16\n",
      "        2029       0.75      0.60      0.67        15\n",
      "        2030       0.73      0.73      0.73        11\n",
      "        2031       0.52      0.69      0.59        16\n",
      "        2032       1.00      0.62      0.76        21\n",
      "        2033       1.00      0.29      0.44         7\n",
      "        2034       0.50      0.33      0.40         3\n",
      "        2035       1.00      0.67      0.80         6\n",
      "        2036       0.78      0.78      0.78         9\n",
      "        2037       1.00      0.92      0.96        13\n",
      "        2038       1.00      0.75      0.86         8\n",
      "        2039       1.00      0.60      0.75        15\n",
      "        2040       1.00      0.67      0.80         9\n",
      "        2041       1.00      0.79      0.88        14\n",
      "        2042       1.00      0.12      0.22         8\n",
      "        2043       1.00      0.25      0.40         4\n",
      "        2044       0.50      0.33      0.40         3\n",
      "        2045       0.93      0.70      0.80        20\n",
      "        2046       0.44      0.67      0.53        18\n",
      "        2047       0.83      0.56      0.67         9\n",
      "        2048       0.00      0.00      0.00         4\n",
      "        2049       0.80      0.62      0.70        13\n",
      "        2050       0.33      0.71      0.45        17\n",
      "        2051       1.00      0.56      0.71         9\n",
      "        2052       1.00      0.40      0.57         5\n",
      "        2053       1.00      0.42      0.59        12\n",
      "        2054       0.00      0.00      0.00         3\n",
      "        2055       1.00      0.50      0.67        20\n",
      "        2056       1.00      0.50      0.67         2\n",
      "        2057       1.00      0.50      0.67         2\n",
      "        2058       1.00      0.83      0.91        18\n",
      "        2059       0.25      0.33      0.29         6\n",
      "        2060       0.25      0.65      0.36        23\n",
      "        2061       0.93      0.68      0.79        19\n",
      "        2062       1.00      0.75      0.86        16\n",
      "        2063       1.00      0.20      0.33         5\n",
      "        2064       0.78      0.37      0.50        19\n",
      "        2065       0.00      0.00      0.00         2\n",
      "        2066       0.86      0.90      0.88        21\n",
      "        2067       1.00      0.39      0.56        18\n",
      "        2068       1.00      0.31      0.47        13\n",
      "        2069       1.00      0.62      0.77         8\n",
      "        2070       1.00      0.85      0.92        13\n",
      "        2071       1.00      0.64      0.78        11\n",
      "        2072       1.00      0.75      0.86         8\n",
      "        2073       1.00      0.80      0.89         5\n",
      "        2074       1.00      0.50      0.67         2\n",
      "        2075       1.00      0.38      0.56        13\n",
      "        2076       0.90      0.86      0.88        21\n",
      "        2077       0.33      0.50      0.40         6\n",
      "        2078       1.00      0.73      0.84        11\n",
      "        2079       1.00      0.70      0.82        10\n",
      "        2080       0.00      0.00      0.00         3\n",
      "        2081       0.75      0.33      0.46         9\n",
      "        2082       1.00      0.55      0.71        11\n",
      "        2083       1.00      0.20      0.33         5\n",
      "        2084       1.00      0.75      0.86         4\n",
      "        2085       0.53      0.69      0.60        13\n",
      "        2086       0.33      0.33      0.33         6\n",
      "        2087       0.88      0.68      0.77        22\n",
      "        2088       0.50      0.62      0.56         8\n",
      "        2089       0.83      0.71      0.77         7\n",
      "        2090       1.00      0.67      0.80         9\n",
      "        2091       1.00      0.82      0.90        11\n",
      "        2092       1.00      0.65      0.79        20\n",
      "        2093       1.00      0.79      0.88        19\n",
      "        2094       1.00      0.88      0.93        24\n",
      "        2095       0.46      0.60      0.52        10\n",
      "        2096       1.00      0.80      0.89        20\n",
      "        2097       0.38      0.30      0.33        10\n",
      "        2098       1.00      0.81      0.90        16\n",
      "        2099       1.00      0.14      0.25         7\n",
      "        2100       0.00      0.00      0.00         2\n",
      "        2101       0.94      0.84      0.89        19\n",
      "        2102       1.00      0.38      0.55         8\n",
      "        2103       1.00      0.80      0.89         5\n",
      "        2104       0.91      0.67      0.77        15\n",
      "        2105       1.00      0.79      0.88        14\n",
      "        2106       1.00      0.83      0.91        12\n",
      "        2107       1.00      0.20      0.33         5\n",
      "        2108       0.80      0.50      0.62         8\n",
      "        2109       1.00      0.62      0.77        16\n",
      "        2110       1.00      0.44      0.62         9\n",
      "        2111       1.00      0.67      0.80         3\n",
      "        2112       1.00      0.67      0.80        21\n",
      "        2113       1.00      0.33      0.50         6\n",
      "        2114       1.00      0.68      0.81        19\n",
      "        2115       1.00      0.45      0.62        11\n",
      "        2116       1.00      0.40      0.57         5\n",
      "        2117       0.73      0.67      0.70        12\n",
      "        2118       0.67      0.50      0.57         8\n",
      "        2119       0.00      0.00      0.00         1\n",
      "        2120       1.00      0.83      0.91        18\n",
      "        2121       0.75      0.60      0.67         5\n",
      "        2122       1.00      0.43      0.60        14\n",
      "        2123       0.00      0.00      0.00         2\n",
      "        2124       1.00      0.60      0.75        15\n",
      "        2125       1.00      0.61      0.76        18\n",
      "        2126       1.00      0.45      0.62        11\n",
      "        2127       0.94      0.67      0.78        24\n",
      "        2128       0.83      0.79      0.81        24\n",
      "        2129       0.93      0.81      0.87        16\n",
      "        2130       0.45      0.74      0.56        19\n",
      "        2131       0.67      0.80      0.73         5\n",
      "        2132       0.85      0.61      0.71        18\n",
      "        2133       1.00      0.69      0.81        16\n",
      "        2134       0.92      0.75      0.83        16\n",
      "        2135       1.00      0.62      0.76        21\n",
      "        2136       1.00      0.67      0.80        21\n",
      "        2137       1.00      0.67      0.80        12\n",
      "        2138       0.00      0.00      0.00         3\n",
      "        2139       1.00      0.86      0.92        14\n",
      "        2140       0.92      0.85      0.88        13\n",
      "        2141       0.00      0.00      0.00         3\n",
      "        2142       1.00      0.57      0.73        14\n",
      "        2143       1.00      0.81      0.89        21\n",
      "        2144       0.86      0.63      0.73        19\n",
      "        2146       0.67      0.33      0.44         6\n",
      "        2147       0.80      0.75      0.77        16\n",
      "        2148       1.00      0.72      0.84        18\n",
      "        2149       1.00      0.46      0.63        13\n",
      "        2150       1.00      0.47      0.64        19\n",
      "        2151       1.00      0.75      0.86         8\n",
      "        2152       0.57      0.40      0.47        10\n",
      "        2153       0.83      0.62      0.71         8\n",
      "        2154       1.00      0.64      0.78        11\n",
      "        2155       0.00      0.00      0.00         2\n",
      "        2156       0.33      0.33      0.33         6\n",
      "        2157       1.00      1.00      1.00        15\n",
      "        2158       0.95      0.86      0.90        21\n",
      "        2159       0.00      0.00      0.00         2\n",
      "        2160       1.00      0.57      0.73         7\n",
      "        2161       1.00      0.62      0.77        16\n",
      "        2162       1.00      1.00      1.00         9\n",
      "        2163       0.78      0.70      0.74        20\n",
      "        2164       1.00      0.69      0.82        13\n",
      "        2165       1.00      0.50      0.67         6\n",
      "        2166       1.00      0.61      0.76        18\n",
      "        2167       1.00      0.40      0.57         5\n",
      "        2168       1.00      0.43      0.60         7\n",
      "        2169       1.00      0.67      0.80         6\n",
      "        2170       1.00      1.00      1.00        15\n",
      "        2171       1.00      0.77      0.87        13\n",
      "        2172       1.00      0.62      0.76        13\n",
      "        2173       1.00      0.80      0.89         5\n",
      "        2174       0.12      0.14      0.13         7\n",
      "        2175       0.74      0.88      0.80        16\n",
      "        2176       0.83      0.62      0.71         8\n",
      "        2177       1.00      0.72      0.84        18\n",
      "        2178       0.00      0.00      0.00         2\n",
      "        2179       1.00      0.86      0.92        21\n",
      "        2180       1.00      0.74      0.85        23\n",
      "        2181       1.00      0.84      0.91        19\n",
      "        2182       1.00      0.50      0.67         4\n",
      "        2183       0.83      0.71      0.77         7\n",
      "        2184       1.00      0.67      0.80        12\n",
      "        2185       1.00      0.63      0.77        19\n",
      "        2186       1.00      0.69      0.81        16\n",
      "        2187       1.00      0.61      0.76        18\n",
      "        2188       0.00      0.00      0.00         6\n",
      "        2189       1.00      0.80      0.89         5\n",
      "        2190       0.00      0.00      0.00         2\n",
      "        2191       0.68      0.72      0.70        18\n",
      "        2192       0.80      0.50      0.62         8\n",
      "        2193       1.00      0.75      0.86        24\n",
      "        2194       0.60      0.80      0.69        15\n",
      "        2195       1.00      0.46      0.63        13\n",
      "        2196       0.62      0.67      0.64        12\n",
      "        2197       0.70      0.58      0.64        12\n",
      "        2198       0.57      0.76      0.65        17\n",
      "        2199       0.41      0.48      0.44        23\n",
      "        2200       1.00      0.54      0.70        26\n",
      "        2201       1.00      0.50      0.67         4\n",
      "        2202       0.81      0.59      0.68        22\n",
      "        2203       1.00      0.33      0.50         3\n",
      "        2204       1.00      0.80      0.89        10\n",
      "        2205       0.88      0.50      0.64        14\n",
      "        2206       1.00      0.58      0.74        12\n",
      "        2207       1.00      0.75      0.86         8\n",
      "        2208       0.87      0.72      0.79        18\n",
      "        2209       1.00      0.50      0.67         8\n",
      "        2210       1.00      1.00      1.00         6\n",
      "        2211       1.00      0.67      0.80        21\n",
      "        2212       0.89      0.57      0.70        14\n",
      "        2213       1.00      0.77      0.87        22\n",
      "        2214       1.00      0.73      0.84        22\n",
      "        2215       1.00      0.42      0.59        12\n",
      "        2216       1.00      0.40      0.57        10\n",
      "        2217       0.91      0.83      0.87        12\n",
      "        2218       1.00      0.65      0.79        17\n",
      "        2219       1.00      0.84      0.91        19\n",
      "        2220       0.92      0.69      0.79        16\n",
      "        2221       1.00      0.75      0.86        20\n",
      "        2222       1.00      0.33      0.50         3\n",
      "        2223       1.00      0.50      0.67         6\n",
      "        2224       1.00      0.77      0.87        22\n",
      "        2225       0.39      0.64      0.48        11\n",
      "        2226       1.00      0.50      0.67        10\n",
      "        2227       0.00      0.00      0.00         6\n",
      "        2228       0.67      0.80      0.73         5\n",
      "        2229       1.00      0.65      0.79        20\n",
      "        2230       1.00      0.76      0.86        21\n",
      "        2231       0.71      0.88      0.79        17\n",
      "        2232       1.00      0.33      0.50         3\n",
      "        2233       1.00      0.64      0.78        14\n",
      "        2234       1.00      0.22      0.36         9\n",
      "        2235       1.00      0.80      0.89         5\n",
      "        2236       1.00      0.60      0.75        15\n",
      "        2237       0.80      0.75      0.77        16\n",
      "        2238       0.71      0.71      0.71         7\n",
      "        2239       1.00      0.62      0.76        13\n",
      "        2240       1.00      0.53      0.69        17\n",
      "        2241       0.00      0.00      0.00         3\n",
      "        2242       1.00      0.50      0.67         4\n",
      "        2243       0.88      0.78      0.82         9\n",
      "        2244       0.93      0.67      0.78        21\n",
      "        2245       0.71      0.55      0.62        22\n",
      "        2246       1.00      0.62      0.77         8\n",
      "        2247       0.57      0.40      0.47        10\n",
      "        2248       0.91      0.77      0.83        13\n",
      "        2249       1.00      0.64      0.78        11\n",
      "        2250       1.00      0.68      0.81        19\n",
      "        2251       0.71      0.67      0.69        18\n",
      "        2252       0.92      0.63      0.75        19\n",
      "        2253       0.33      0.67      0.44         3\n",
      "        2254       1.00      0.27      0.43        11\n",
      "        2255       1.00      1.00      1.00         1\n",
      "        2256       0.89      0.89      0.89         9\n",
      "        2257       0.00      0.00      0.00         3\n",
      "        2258       0.50      0.50      0.50        10\n",
      "        2259       1.00      0.65      0.79        20\n",
      "        2260       1.00      0.62      0.77         8\n",
      "        2261       1.00      0.32      0.48        19\n",
      "        2262       1.00      0.25      0.40         8\n",
      "        2263       0.00      0.00      0.00         3\n",
      "        2264       1.00      0.64      0.78        14\n",
      "        2265       0.95      0.86      0.90        21\n",
      "        2266       1.00      0.82      0.90        11\n",
      "        2267       1.00      0.55      0.71        11\n",
      "        2268       0.80      0.80      0.80         5\n",
      "        2269       1.00      0.78      0.88         9\n",
      "        2270       0.60      0.92      0.73        13\n",
      "        2271       1.00      0.86      0.92         7\n",
      "        2272       1.00      0.37      0.54        19\n",
      "        2273       0.67      0.50      0.57         4\n",
      "        2274       0.60      0.43      0.50        21\n",
      "        2275       0.00      0.00      0.00         2\n",
      "        2276       1.00      0.70      0.82        23\n",
      "        2277       1.00      0.88      0.93        16\n",
      "        2278       1.00      0.79      0.88        19\n",
      "        2279       1.00      0.53      0.69        19\n",
      "        2280       1.00      0.75      0.86         4\n",
      "        2281       0.00      0.00      0.00         4\n",
      "        2282       1.00      0.88      0.93         8\n",
      "        2283       1.00      0.57      0.73         7\n",
      "        2285       1.00      0.67      0.80        21\n",
      "        2286       0.33      0.47      0.39        15\n",
      "        2287       1.00      0.89      0.94        19\n",
      "        2288       1.00      0.33      0.50         6\n",
      "        2289       0.77      0.77      0.77        13\n",
      "        2290       1.00      0.80      0.89         5\n",
      "        2291       1.00      0.76      0.87        17\n",
      "        2292       1.00      0.83      0.91        12\n",
      "        2293       1.00      0.56      0.71         9\n",
      "        2294       1.00      0.43      0.60        14\n",
      "        2295       0.00      0.00      0.00         5\n",
      "        2296       1.00      0.71      0.83         7\n",
      "        2297       1.00      0.60      0.75        10\n",
      "        2298       0.86      0.75      0.80        16\n",
      "        2299       1.00      0.75      0.86         4\n",
      "        2300       1.00      0.33      0.50        12\n",
      "        2301       1.00      0.73      0.85        15\n",
      "        2302       0.92      0.65      0.76        17\n",
      "        2303       0.94      0.70      0.80        23\n",
      "        2304       1.00      0.56      0.72        16\n",
      "        2305       1.00      0.73      0.85        15\n",
      "        2306       1.00      0.42      0.59        12\n",
      "        2307       1.00      0.75      0.86         4\n",
      "        2308       1.00      0.71      0.83        14\n",
      "        2309       0.00      0.00      0.00         2\n",
      "        2310       1.00      0.67      0.80        21\n",
      "        2311       1.00      0.46      0.63        13\n",
      "        2312       1.00      0.75      0.86        20\n",
      "        2313       0.25      0.43      0.32         7\n",
      "        2314       1.00      0.56      0.71         9\n",
      "        2315       1.00      0.53      0.70        15\n",
      "        2316       0.43      0.67      0.52         9\n",
      "        2317       1.00      0.70      0.82        20\n",
      "        2318       0.88      0.78      0.82         9\n",
      "        2319       0.00      0.00      0.00         1\n",
      "        2320       0.82      0.75      0.78        12\n",
      "        2321       0.14      0.27      0.18        11\n",
      "        2322       0.50      0.33      0.40         3\n",
      "        2323       1.00      0.20      0.33         5\n",
      "        2324       0.00      0.00      0.00         2\n",
      "        2325       1.00      0.64      0.78        14\n",
      "        2326       0.26      0.71      0.38         7\n",
      "        2327       1.00      0.43      0.60        14\n",
      "        2328       1.00      0.82      0.90        22\n",
      "        2329       0.79      0.69      0.73        16\n",
      "        2330       1.00      0.21      0.35        14\n",
      "        2331       1.00      0.69      0.81        16\n",
      "        2332       0.85      0.65      0.73        17\n",
      "        2333       0.00      0.00      0.00         1\n",
      "        2334       1.00      0.62      0.76        21\n",
      "        2335       1.00      0.14      0.25         7\n",
      "        2336       0.50      0.25      0.33         4\n",
      "        2337       1.00      0.67      0.80         6\n",
      "        2338       1.00      0.88      0.93         8\n",
      "        2339       0.92      0.63      0.75        19\n",
      "        2340       1.00      0.67      0.80        15\n",
      "        2341       1.00      0.50      0.67        12\n",
      "        2342       0.00      0.00      0.00         6\n",
      "        2343       0.91      0.53      0.67        19\n",
      "        2344       0.67      0.45      0.54        22\n",
      "        2345       0.53      0.62      0.57        13\n",
      "        2346       0.92      0.65      0.76        17\n",
      "        2347       1.00      0.41      0.58        17\n",
      "        2348       0.00      0.00      0.00         7\n",
      "        2349       1.00      0.50      0.67        10\n",
      "        2350       0.89      0.47      0.62        17\n",
      "        2351       1.00      0.25      0.40         4\n",
      "        2352       1.00      0.56      0.72        16\n",
      "        2353       1.00      0.52      0.69        21\n",
      "        2354       1.00      0.44      0.62         9\n",
      "        2355       0.62      0.89      0.73         9\n",
      "        2356       1.00      0.53      0.70        15\n",
      "        2357       0.43      0.67      0.53        15\n",
      "        2358       1.00      0.82      0.90        11\n",
      "        2359       0.67      0.29      0.40         7\n",
      "        2360       1.00      0.67      0.80        21\n",
      "        2361       1.00      0.60      0.75        10\n",
      "        2362       0.60      0.46      0.52        13\n",
      "        2363       0.92      0.85      0.88        13\n",
      "        2364       1.00      0.83      0.91        18\n",
      "        2365       1.00      0.72      0.84        25\n",
      "        2366       0.90      0.56      0.69        16\n",
      "        2367       0.50      0.50      0.50         6\n",
      "        2368       1.00      0.72      0.84        18\n",
      "        2369       1.00      0.73      0.84        22\n",
      "        2370       0.92      0.67      0.77        18\n",
      "        2371       0.89      0.36      0.52        22\n",
      "        2372       0.88      0.64      0.74        11\n",
      "        2373       0.00      0.00      0.00         1\n",
      "        2374       1.00      0.50      0.67         6\n",
      "        2375       0.56      0.82      0.67        17\n",
      "        2376       1.00      0.62      0.77         8\n",
      "        2377       0.60      0.75      0.67        20\n",
      "        2378       1.00      0.60      0.75        10\n",
      "        2379       1.00      0.76      0.87        17\n",
      "        2380       0.94      1.00      0.97        15\n",
      "        2381       1.00      0.33      0.50        12\n",
      "        2382       1.00      0.88      0.93        24\n",
      "        2383       1.00      0.50      0.67        22\n",
      "        2384       1.00      0.62      0.77        16\n",
      "        2385       0.80      0.80      0.80         5\n",
      "        2386       0.00      0.00      0.00         2\n",
      "        2387       0.95      0.78      0.86        23\n",
      "        2388       1.00      0.65      0.79        20\n",
      "        2389       0.83      0.56      0.67         9\n",
      "        2390       0.94      0.75      0.83        20\n",
      "        2391       1.00      0.72      0.84        18\n",
      "        2392       1.00      0.80      0.89         5\n",
      "        2393       1.00      0.69      0.82        13\n",
      "        2394       0.75      0.90      0.82        10\n",
      "        2395       1.00      0.33      0.50         6\n",
      "        2396       1.00      0.75      0.86        24\n",
      "        2397       0.82      0.78      0.80        18\n",
      "        2398       1.00      0.60      0.75        15\n",
      "        2399       1.00      0.86      0.92         7\n",
      "        2400       1.00      0.18      0.31        22\n",
      "        2401       1.00      0.75      0.86         8\n",
      "        2402       0.00      0.00      0.00         2\n",
      "        2403       0.00      0.00      0.00         1\n",
      "        2404       1.00      0.50      0.67         8\n",
      "        2405       1.00      0.40      0.57        10\n",
      "        2406       0.92      0.85      0.88        13\n",
      "        2407       0.00      0.00      0.00         2\n",
      "        2408       0.59      0.67      0.62        15\n",
      "        2409       1.00      0.71      0.83        17\n",
      "        2410       1.00      0.67      0.80         9\n",
      "        2411       0.90      0.75      0.82        12\n",
      "        2412       1.00      0.67      0.80         6\n",
      "        2413       1.00      0.62      0.76        21\n",
      "        2414       1.00      0.38      0.55         8\n",
      "        2415       0.71      0.55      0.62        22\n",
      "        2416       1.00      0.64      0.78        14\n",
      "        2417       1.00      0.75      0.86        16\n",
      "        2418       0.95      0.82      0.88        22\n",
      "        2419       0.71      0.75      0.73        16\n",
      "        2420       1.00      0.75      0.86        24\n",
      "        2421       0.78      0.88      0.82         8\n",
      "        2422       1.00      0.74      0.85        19\n",
      "        2423       1.00      0.74      0.85        19\n",
      "        2424       1.00      0.56      0.71         9\n",
      "        2425       0.89      0.67      0.76        12\n",
      "        2426       1.00      0.57      0.73        21\n",
      "        2427       0.85      0.85      0.85        20\n",
      "        2428       0.00      0.00      0.00         1\n",
      "        2429       1.00      0.78      0.88        18\n",
      "        2430       0.90      0.86      0.88        21\n",
      "        2431       0.00      0.00      0.00         6\n",
      "        2432       1.00      0.62      0.77         8\n",
      "        2433       1.00      0.59      0.74        22\n",
      "        2434       1.00      0.76      0.86        21\n",
      "        2435       1.00      0.44      0.62         9\n",
      "        2436       0.71      0.71      0.71         7\n",
      "        2437       0.93      0.76      0.84        17\n",
      "        2438       0.76      0.80      0.78        20\n",
      "        2439       1.00      0.43      0.60         7\n",
      "        2440       1.00      0.60      0.75        10\n",
      "        2441       0.00      0.00      0.00         3\n",
      "        2442       1.00      0.78      0.88        18\n",
      "        2443       1.00      0.20      0.33        10\n",
      "        2444       1.00      0.53      0.70        15\n",
      "        2445       0.83      0.62      0.71         8\n",
      "        2446       1.00      0.78      0.88         9\n",
      "        2447       1.00      0.86      0.92        21\n",
      "        2448       0.50      0.40      0.44         5\n",
      "        2449       0.57      0.67      0.62        12\n",
      "        2450       0.88      0.67      0.76        21\n",
      "        2451       1.00      0.33      0.50         3\n",
      "        2452       1.00      0.91      0.95        11\n",
      "        2453       1.00      0.67      0.80         9\n",
      "        2454       0.94      0.75      0.83        20\n",
      "        2455       1.00      0.80      0.89        20\n",
      "        2456       1.00      0.60      0.75         5\n",
      "        2457       1.00      0.80      0.89        20\n",
      "        2458       1.00      0.86      0.92         7\n",
      "        2459       1.00      0.69      0.82        13\n",
      "        2460       1.00      0.53      0.70        15\n",
      "        2461       1.00      0.50      0.67         8\n",
      "        2462       0.58      0.78      0.67        18\n",
      "        2463       0.94      0.83      0.88        18\n",
      "        2464       0.70      1.00      0.82        21\n",
      "        2465       1.00      0.43      0.60        21\n",
      "        2466       0.86      0.80      0.83        15\n",
      "        2467       0.82      0.70      0.76        20\n",
      "        2468       0.00      0.00      0.00         6\n",
      "        2469       1.00      0.14      0.25         7\n",
      "        2470       1.00      0.72      0.84        18\n",
      "        2471       1.00      0.67      0.80        18\n",
      "        2472       0.79      0.61      0.69        18\n",
      "        2473       0.92      0.50      0.65        22\n",
      "        2474       0.00      0.00      0.00         5\n",
      "        2475       1.00      0.38      0.56        13\n",
      "        2476       0.25      0.20      0.22         5\n",
      "        2477       0.00      0.00      0.00         1\n",
      "        2478       0.75      0.50      0.60         6\n",
      "        2479       1.00      0.57      0.73         7\n",
      "        2480       0.67      0.50      0.57         4\n",
      "        2481       1.00      0.90      0.95        20\n",
      "        2482       0.92      0.71      0.80        17\n",
      "        2483       1.00      0.64      0.78        11\n",
      "        2484       1.00      0.63      0.77        19\n",
      "        2485       1.00      0.65      0.79        23\n",
      "        2486       1.00      0.20      0.33         5\n",
      "        2487       0.95      0.95      0.95        20\n",
      "        2488       1.00      0.83      0.91         6\n",
      "        2489       1.00      0.68      0.81        22\n",
      "        2490       1.00      0.92      0.96        13\n",
      "        2491       0.91      0.67      0.77        15\n",
      "        2492       1.00      0.42      0.59        12\n",
      "        2493       1.00      0.33      0.50         3\n",
      "        2494       0.00      0.00      0.00         3\n",
      "        2495       0.60      0.33      0.43         9\n",
      "        2496       1.00      0.43      0.60         7\n",
      "        2497       1.00      0.50      0.67        16\n",
      "        2498       0.87      0.59      0.70        22\n",
      "        2499       1.00      0.67      0.80         9\n",
      "        2500       0.86      0.46      0.60        13\n",
      "        2501       1.00      0.80      0.89         5\n",
      "        2502       1.00      0.80      0.89        20\n",
      "        2503       0.75      0.69      0.72        13\n",
      "        2504       1.00      0.78      0.88        18\n",
      "        2505       1.00      0.87      0.93        15\n",
      "        2506       0.83      0.38      0.53        13\n",
      "        2507       1.00      0.33      0.50         6\n",
      "        2508       0.00      0.00      0.00         1\n",
      "        2509       0.92      0.73      0.81        15\n",
      "        2510       0.62      0.62      0.62         8\n",
      "        2511       1.00      0.70      0.82        20\n",
      "        2512       1.00      0.57      0.73         7\n",
      "        2513       1.00      0.71      0.83         7\n",
      "        2514       0.94      0.71      0.81        21\n",
      "        2515       0.92      0.75      0.83        16\n",
      "        2516       1.00      0.83      0.91         6\n",
      "        2517       1.00      0.80      0.89         5\n",
      "        2518       1.00      0.75      0.86         8\n",
      "        2519       0.92      0.71      0.80        17\n",
      "        2520       1.00      0.33      0.50         3\n",
      "        2521       1.00      0.69      0.81        16\n",
      "        2522       1.00      0.78      0.88         9\n",
      "        2523       0.88      0.78      0.82         9\n",
      "        2524       0.85      0.69      0.76        16\n",
      "        2525       1.00      0.30      0.46        10\n",
      "        2526       0.81      0.72      0.76        18\n",
      "        2527       0.00      0.00      0.00         3\n",
      "        2528       1.00      0.48      0.65        23\n",
      "        2529       1.00      0.71      0.83         7\n",
      "        2530       1.00      0.70      0.82        20\n",
      "        2531       1.00      0.84      0.91        19\n",
      "        2532       1.00      0.33      0.50         3\n",
      "        2533       0.83      0.91      0.87        22\n",
      "        2534       1.00      0.96      0.98        23\n",
      "        2535       1.00      0.67      0.80         6\n",
      "        2536       1.00      0.44      0.62         9\n",
      "        2537       0.62      0.47      0.53        17\n",
      "        2538       1.00      0.50      0.67         8\n",
      "        2539       0.92      0.67      0.77        18\n",
      "        2540       1.00      0.62      0.77         8\n",
      "        2541       0.69      0.60      0.64        15\n",
      "        2542       1.00      0.57      0.73         7\n",
      "        2543       1.00      0.40      0.57        15\n",
      "        2544       1.00      0.85      0.92        13\n",
      "        2545       1.00      0.37      0.54        19\n",
      "        2546       1.00      0.85      0.92        20\n",
      "        2547       1.00      0.67      0.80        18\n",
      "        2548       0.92      0.73      0.81        15\n",
      "        2549       1.00      0.89      0.94        18\n",
      "        2550       1.00      1.00      1.00        14\n",
      "        2551       1.00      0.75      0.86         4\n",
      "        2552       1.00      0.50      0.67        10\n",
      "        2553       1.00      0.85      0.92        13\n",
      "        2554       1.00      0.89      0.94        19\n",
      "        2555       1.00      0.50      0.67         4\n",
      "        2556       1.00      0.69      0.82        13\n",
      "        2557       1.00      0.39      0.56        18\n",
      "        2558       1.00      0.85      0.92        13\n",
      "        2559       0.62      0.50      0.56        10\n",
      "        2560       1.00      0.44      0.62         9\n",
      "        2561       1.00      0.85      0.92        20\n",
      "        2562       1.00      0.62      0.77        16\n",
      "        2563       1.00      0.57      0.72        23\n",
      "        2564       0.86      0.75      0.80        16\n",
      "        2565       1.00      0.64      0.78        11\n",
      "        2566       1.00      0.63      0.77        19\n",
      "        2567       0.00      0.00      0.00         3\n",
      "        2568       0.50      0.50      0.50         4\n",
      "        2569       0.94      0.89      0.91        18\n",
      "        2570       0.00      0.00      0.00         1\n",
      "        2571       1.00      0.50      0.67         4\n",
      "        2572       1.00      0.71      0.83         7\n",
      "        2573       1.00      0.56      0.72        16\n",
      "        2574       0.87      0.62      0.72        21\n",
      "        2575       1.00      0.70      0.82        20\n",
      "        2576       1.00      0.59      0.74        17\n",
      "        2577       1.00      0.43      0.60         7\n",
      "        2578       0.31      0.57      0.40         7\n",
      "        2579       1.00      0.41      0.58        17\n",
      "        2580       1.00      0.50      0.67         8\n",
      "        2581       1.00      0.58      0.73        19\n",
      "        2582       0.76      0.68      0.72        19\n",
      "        2583       0.00      0.00      0.00         3\n",
      "        2584       1.00      1.00      1.00         4\n",
      "        2585       1.00      0.25      0.40         4\n",
      "        2586       1.00      0.77      0.87        22\n",
      "        2587       1.00      0.17      0.29         6\n",
      "        2588       0.00      0.00      0.00         1\n",
      "        2589       1.00      0.28      0.43        18\n",
      "        2590       1.00      0.57      0.73         7\n",
      "        2591       0.76      0.95      0.84        20\n",
      "        2592       1.00      0.75      0.86        20\n",
      "        2593       0.92      0.71      0.80        17\n",
      "        2594       0.45      0.75      0.56        12\n",
      "        2595       1.00      0.75      0.86        20\n",
      "        2596       0.00      0.00      0.00         3\n",
      "        2597       0.82      0.64      0.72        14\n",
      "        2598       1.00      0.33      0.50         6\n",
      "        2599       1.00      0.58      0.73        19\n",
      "        2600       1.00      0.43      0.60         7\n",
      "        2601       0.00      0.00      0.00         3\n",
      "        2602       1.00      0.83      0.91        18\n",
      "        2603       1.00      0.55      0.71        20\n",
      "        2604       1.00      0.71      0.83        24\n",
      "        2605       1.00      0.94      0.97        17\n",
      "        2606       1.00      0.78      0.88         9\n",
      "        2607       1.00      0.55      0.71        20\n",
      "        2608       1.00      0.71      0.83        17\n",
      "        2609       1.00      1.00      1.00         3\n",
      "        2610       1.00      0.82      0.90        11\n",
      "        2611       0.00      0.00      0.00         3\n",
      "        2612       1.00      0.62      0.76        13\n",
      "        2613       1.00      0.33      0.50         3\n",
      "        2614       0.29      0.40      0.33         5\n",
      "        2615       0.67      0.80      0.73        10\n",
      "        2616       1.00      0.65      0.79        20\n",
      "        2617       0.33      0.27      0.30        11\n",
      "        2618       1.00      0.90      0.95        20\n",
      "        2619       1.00      0.70      0.82        23\n",
      "        2620       0.88      0.47      0.61        15\n",
      "        2621       1.00      0.50      0.67        10\n",
      "        2622       1.00      0.60      0.75        20\n",
      "        2623       0.00      0.00      0.00         1\n",
      "        2624       0.75      0.75      0.75        20\n",
      "        2625       1.00      0.79      0.88        19\n",
      "        2626       0.64      0.43      0.51        21\n",
      "        2627       1.00      0.59      0.74        17\n",
      "        2628       1.00      0.62      0.77         8\n",
      "        2629       0.57      0.65      0.60        20\n",
      "        2630       0.86      0.57      0.69        21\n",
      "        2631       1.00      0.75      0.86        16\n",
      "        2632       0.00      0.00      0.00         2\n",
      "        2633       0.00      0.00      0.00         1\n",
      "        2634       0.67      0.40      0.50         5\n",
      "        2635       0.00      0.00      0.00         3\n",
      "        2636       0.67      0.50      0.57         4\n",
      "        2637       0.57      0.85      0.68        20\n",
      "        2638       1.00      0.75      0.86        24\n",
      "        2639       0.00      0.00      0.00         9\n",
      "        2640       1.00      0.59      0.74        22\n",
      "        2641       0.67      0.80      0.73        15\n",
      "        2642       1.00      0.83      0.91        24\n",
      "        2643       1.00      0.77      0.87        13\n",
      "        2644       0.88      0.54      0.67        13\n",
      "        2645       1.00      1.00      1.00         8\n",
      "        2646       0.62      0.77      0.69        13\n",
      "        2647       1.00      0.60      0.75         5\n",
      "        2648       1.00      0.89      0.94        19\n",
      "        2649       1.00      0.50      0.67         2\n",
      "        2650       1.00      0.67      0.80         6\n",
      "        2651       1.00      0.71      0.83         7\n",
      "        2652       1.00      0.50      0.67        12\n",
      "        2653       0.88      0.78      0.82         9\n",
      "        2654       1.00      0.70      0.82        10\n",
      "        2655       0.78      0.54      0.64        13\n",
      "        2656       1.00      1.00      1.00        12\n",
      "        2657       1.00      0.17      0.29         6\n",
      "        2658       1.00      0.78      0.88         9\n",
      "        2659       1.00      0.50      0.67         6\n",
      "        2660       0.64      0.70      0.67        10\n",
      "        2661       1.00      0.65      0.79        20\n",
      "        2662       1.00      0.65      0.79        23\n",
      "        2663       1.00      0.57      0.73        14\n",
      "        2664       1.00      0.67      0.80         6\n",
      "        2665       1.00      0.71      0.83        14\n",
      "        2666       0.91      0.77      0.83        13\n",
      "        2667       0.86      0.80      0.83        15\n",
      "        2668       0.32      0.40      0.36        20\n",
      "        2669       1.00      0.75      0.86         8\n",
      "        2670       0.79      0.79      0.79        14\n",
      "        2671       1.00      0.43      0.60         7\n",
      "        2672       0.00      0.00      0.00         3\n",
      "        2673       1.00      0.50      0.67        16\n",
      "        2674       0.00      0.00      0.00         2\n",
      "        2675       1.00      0.69      0.81        16\n",
      "        2676       1.00      0.67      0.80        18\n",
      "        2677       0.40      0.67      0.50         6\n",
      "        2678       1.00      0.29      0.44        21\n",
      "        2679       1.00      0.50      0.67         6\n",
      "        2680       1.00      0.62      0.77         8\n",
      "        2681       0.00      0.00      0.00         3\n",
      "        2682       1.00      0.71      0.83        14\n",
      "        2683       0.83      0.45      0.59        11\n",
      "        2684       0.62      0.62      0.62        16\n",
      "        2685       0.85      0.65      0.73        17\n",
      "        2686       1.00      0.52      0.69        21\n",
      "        2687       1.00      0.60      0.75        10\n",
      "        2688       0.00      0.00      0.00         1\n",
      "        2689       0.81      0.85      0.83        20\n",
      "        2690       1.00      0.71      0.83        14\n",
      "        2691       1.00      0.80      0.89        15\n",
      "        2692       0.89      0.89      0.89        19\n",
      "        2693       1.00      0.85      0.92        13\n",
      "        2694       0.75      0.30      0.43        10\n",
      "        2695       0.67      0.40      0.50         5\n",
      "        2696       0.80      0.67      0.73         6\n",
      "        2697       1.00      0.20      0.33        10\n",
      "        2698       1.00      0.76      0.87        17\n",
      "        2699       0.88      0.58      0.70        12\n",
      "        2700       0.91      0.71      0.80        14\n",
      "        2701       1.00      0.43      0.60        14\n",
      "        2702       0.00      0.00      0.00         2\n",
      "        2703       0.70      0.50      0.58        14\n",
      "        2704       1.00      0.85      0.92        20\n",
      "        2705       0.00      0.00      0.00         2\n",
      "        2706       1.00      0.55      0.71        20\n",
      "        2707       1.00      0.38      0.55         8\n",
      "        2708       0.00      0.00      0.00         2\n",
      "        2709       0.75      0.60      0.67         5\n",
      "        2710       1.00      0.39      0.56        18\n",
      "        2711       1.00      0.85      0.92        20\n",
      "        2712       0.06      0.65      0.11        20\n",
      "        2713       1.00      0.85      0.92        13\n",
      "        2714       0.82      0.45      0.58        20\n",
      "        2715       0.91      0.48      0.62        21\n",
      "        2716       0.88      0.64      0.74        11\n",
      "        2717       1.00      0.60      0.75        25\n",
      "        2718       0.74      0.88      0.80        16\n",
      "        2719       1.00      0.67      0.80         9\n",
      "        2720       0.50      0.17      0.25         6\n",
      "        2721       0.57      0.31      0.40        13\n",
      "        2722       0.00      0.00      0.00         1\n",
      "        2723       1.00      0.57      0.73         7\n",
      "        2724       0.33      0.33      0.33         3\n",
      "        2725       1.00      0.57      0.73        14\n",
      "        2726       1.00      0.94      0.97        16\n",
      "        2727       0.93      0.78      0.85        18\n",
      "        2728       0.89      0.67      0.76        12\n",
      "        2729       1.00      0.50      0.67        16\n",
      "        2730       1.00      0.29      0.44         7\n",
      "        2731       1.00      0.33      0.50         6\n",
      "        2732       1.00      0.83      0.91        18\n",
      "        2733       1.00      0.43      0.60         7\n",
      "        2734       1.00      0.33      0.50         6\n",
      "        2735       0.86      0.90      0.88        20\n",
      "        2736       0.00      0.00      0.00         2\n",
      "        2737       1.00      0.11      0.20         9\n",
      "        2738       0.56      0.40      0.47        25\n",
      "        2739       1.00      0.64      0.78        14\n",
      "        2740       0.55      0.75      0.63         8\n",
      "        2741       1.00      0.93      0.96        14\n",
      "        2742       0.88      0.75      0.81        20\n",
      "        2743       0.83      0.60      0.70        25\n",
      "        2744       1.00      0.73      0.84        22\n",
      "        2745       0.60      0.50      0.55         6\n",
      "        2746       0.00      0.00      0.00         2\n",
      "        2747       0.57      0.33      0.42        12\n",
      "        2748       1.00      0.50      0.67         8\n",
      "        2749       1.00      0.67      0.80         3\n",
      "        2750       1.00      0.41      0.58        17\n",
      "        2751       1.00      0.84      0.91        19\n",
      "        2752       0.67      0.50      0.57         8\n",
      "        2753       1.00      0.92      0.96        13\n",
      "        2754       1.00      0.79      0.88        14\n",
      "        2755       1.00      0.25      0.40         8\n",
      "        2756       1.00      0.77      0.87        13\n",
      "        2757       1.00      0.69      0.81        16\n",
      "        2758       0.00      0.00      0.00         3\n",
      "        2759       1.00      0.20      0.33         5\n",
      "        2760       0.00      0.00      0.00         3\n",
      "        2761       1.00      0.89      0.94         9\n",
      "        2762       0.00      0.00      0.00         1\n",
      "        2763       1.00      0.64      0.78        11\n",
      "        2764       0.88      0.70      0.78        10\n",
      "        2765       1.00      0.71      0.83         7\n",
      "        2766       0.00      0.00      0.00         3\n",
      "        2767       1.00      0.46      0.63        13\n",
      "        2768       0.79      0.65      0.71        17\n",
      "        2769       0.52      0.62      0.57        21\n",
      "        2770       1.00      0.71      0.83        14\n",
      "        2771       1.00      0.53      0.69        17\n",
      "        2772       1.00      0.86      0.92         7\n",
      "        2773       1.00      0.78      0.88        23\n",
      "        2774       0.00      0.00      0.00         1\n",
      "        2775       1.00      0.25      0.40         4\n",
      "        2776       1.00      0.63      0.77        19\n",
      "        2777       0.86      0.86      0.86         7\n",
      "        2778       0.70      0.88      0.78         8\n",
      "        2779       1.00      0.85      0.92        20\n",
      "        2780       1.00      1.00      1.00        19\n",
      "        2781       1.00      0.67      0.80        21\n",
      "        2782       0.62      0.62      0.62         8\n",
      "        2783       1.00      0.43      0.60        14\n",
      "        2784       0.92      0.61      0.73        18\n",
      "        2785       1.00      0.89      0.94        18\n",
      "        2786       0.00      0.00      0.00         2\n",
      "        2787       0.76      0.73      0.74        22\n",
      "        2788       1.00      0.62      0.77         8\n",
      "        2789       0.46      0.65      0.54        20\n",
      "        2790       0.87      0.62      0.72        21\n",
      "        2791       1.00      0.53      0.69        19\n",
      "        2792       0.00      0.00      0.00         4\n",
      "        2793       1.00      0.60      0.75         5\n",
      "        2794       0.88      0.78      0.82         9\n",
      "        2795       1.00      0.53      0.69        19\n",
      "        2796       0.91      0.71      0.80        14\n",
      "        2797       1.00      0.33      0.50         3\n",
      "        2798       1.00      0.84      0.91        19\n",
      "        2799       1.00      0.71      0.83         7\n",
      "        2800       0.51      0.92      0.66        24\n",
      "        2801       0.80      0.62      0.70        13\n",
      "        2802       1.00      0.73      0.84        11\n",
      "        2803       1.00      0.40      0.57         5\n",
      "        2804       1.00      0.25      0.40         4\n",
      "        2805       0.00      0.00      0.00         2\n",
      "        2806       1.00      0.47      0.64        17\n",
      "        2807       1.00      0.75      0.86         8\n",
      "        2808       1.00      0.50      0.67        14\n",
      "        2809       1.00      0.95      0.97        20\n",
      "        2810       0.00      0.00      0.00         3\n",
      "        2811       1.00      0.65      0.79        17\n",
      "        2812       1.00      0.25      0.40         4\n",
      "        2813       1.00      0.40      0.57         5\n",
      "        2814       0.67      0.57      0.62         7\n",
      "        2815       1.00      0.44      0.62         9\n",
      "        2816       1.00      0.40      0.57         5\n",
      "        2817       0.50      0.71      0.59         7\n",
      "        2818       1.00      0.83      0.91        24\n",
      "        2819       1.00      0.50      0.67         8\n",
      "        2820       0.88      0.62      0.73        24\n",
      "        2821       1.00      0.83      0.91        24\n",
      "        2822       0.00      0.00      0.00         2\n",
      "        2823       0.92      0.52      0.67        21\n",
      "        2824       0.89      1.00      0.94        17\n",
      "        2825       0.75      0.50      0.60         6\n",
      "        2826       0.80      0.57      0.67         7\n",
      "        2827       0.33      0.20      0.25         5\n",
      "        2828       0.00      0.00      0.00         1\n",
      "        2829       1.00      0.73      0.85        15\n",
      "        2830       0.95      0.86      0.90        22\n",
      "        2831       1.00      0.56      0.71        18\n",
      "        2832       1.00      0.77      0.87        13\n",
      "        2833       0.62      0.50      0.56        10\n",
      "        2834       0.80      0.55      0.65        22\n",
      "        2835       0.91      0.71      0.80        14\n",
      "        2836       0.86      0.46      0.60        13\n",
      "        2837       0.92      0.86      0.89        14\n",
      "        2838       1.00      0.60      0.75         5\n",
      "        2839       0.95      0.72      0.82        25\n",
      "        2840       1.00      0.60      0.75         5\n",
      "        2841       1.00      0.53      0.69        17\n",
      "        2842       0.73      0.69      0.71        16\n",
      "        2843       1.00      0.67      0.80         6\n",
      "        2844       0.43      0.69      0.53        13\n",
      "        2845       1.00      0.55      0.71        11\n",
      "        2846       0.00      0.00      0.00         3\n",
      "        2847       0.00      0.00      0.00         3\n",
      "        2848       0.86      0.67      0.75         9\n",
      "        2849       0.00      0.00      0.00         2\n",
      "        2850       0.88      0.79      0.83        19\n",
      "        2851       1.00      0.81      0.89        21\n",
      "        2852       1.00      0.75      0.86         4\n",
      "        2853       0.92      0.67      0.77        18\n",
      "        2854       1.00      0.80      0.89        10\n",
      "        2855       0.00      0.00      0.00         4\n",
      "        2856       0.00      0.00      0.00         2\n",
      "        2857       1.00      0.50      0.67        14\n",
      "        2858       0.93      0.67      0.78        21\n",
      "        2859       1.00      0.14      0.25         7\n",
      "        2860       1.00      0.72      0.84        18\n",
      "        2861       0.33      0.67      0.44         3\n",
      "        2862       1.00      0.88      0.93         8\n",
      "        2863       1.00      0.36      0.53        11\n",
      "        2864       1.00      0.67      0.80        15\n",
      "        2865       0.34      0.68      0.46        19\n",
      "        2866       0.95      0.82      0.88        22\n",
      "        2867       1.00      0.74      0.85        19\n",
      "        2868       0.00      0.00      0.00         2\n",
      "        2869       1.00      0.33      0.50         3\n",
      "        2870       0.52      0.61      0.56        18\n",
      "        2871       1.00      0.80      0.89        25\n",
      "        2872       0.95      0.76      0.84        25\n",
      "        2873       1.00      1.00      1.00         1\n",
      "        2874       1.00      0.89      0.94        19\n",
      "        2875       1.00      0.57      0.73        14\n",
      "        2876       0.75      0.80      0.77        15\n",
      "        2877       0.92      1.00      0.96        23\n",
      "        2878       0.93      0.67      0.78        21\n",
      "        2879       1.00      0.71      0.83        17\n",
      "        2880       1.00      0.75      0.86        20\n",
      "        2881       0.71      0.77      0.74        13\n",
      "        2882       1.00      0.50      0.67         8\n",
      "        2883       0.00      0.00      0.00         1\n",
      "        2884       1.00      0.40      0.57         5\n",
      "        2885       1.00      0.61      0.76        23\n",
      "        2886       1.00      0.81      0.89        21\n",
      "        2887       0.00      0.00      0.00         3\n",
      "        2888       1.00      0.40      0.57        20\n",
      "        2889       0.68      0.62      0.65        24\n",
      "        2890       0.00      0.00      0.00         2\n",
      "        2891       1.00      0.75      0.86        24\n",
      "        2892       0.80      0.50      0.62         8\n",
      "        2893       1.00      0.62      0.77         8\n",
      "        2894       0.74      0.78      0.76        18\n",
      "        2895       1.00      0.57      0.73         7\n",
      "        2896       1.00      0.17      0.29         6\n",
      "        2897       0.67      0.29      0.40         7\n",
      "        2898       0.83      0.67      0.74        15\n",
      "        2899       0.71      0.71      0.71        14\n",
      "        2900       1.00      0.73      0.85        15\n",
      "        2901       1.00      0.78      0.88        18\n",
      "        2902       0.82      0.64      0.72        22\n",
      "        2903       1.00      0.86      0.92         7\n",
      "        2904       1.00      0.72      0.84        18\n",
      "        2905       0.85      0.85      0.85        13\n",
      "        2906       0.86      0.75      0.80        16\n",
      "        2907       0.00      0.00      0.00         2\n",
      "        2908       1.00      0.47      0.64        19\n",
      "        2909       0.00      0.00      0.00         2\n",
      "        2910       0.00      0.00      0.00         2\n",
      "        2911       1.00      0.87      0.93        23\n",
      "        2912       0.46      0.75      0.57         8\n",
      "        2913       0.00      0.00      0.00         5\n",
      "        2914       1.00      0.75      0.86        12\n",
      "        2915       1.00      0.67      0.80         9\n",
      "        2916       1.00      0.71      0.83        14\n",
      "        2917       1.00      0.58      0.73        19\n",
      "        2918       1.00      0.57      0.73         7\n",
      "        2919       0.71      0.45      0.56        22\n",
      "        2920       0.00      0.00      0.00         3\n",
      "        2921       0.67      0.29      0.40         7\n",
      "        2922       0.00      0.00      0.00         4\n",
      "        2923       1.00      0.75      0.86         4\n",
      "        2924       0.94      0.84      0.89        19\n",
      "        2925       0.90      0.90      0.90        10\n",
      "        2926       0.92      0.86      0.89        14\n",
      "        2927       1.00      0.68      0.81        19\n",
      "        2928       0.95      0.95      0.95        21\n",
      "        2929       0.72      0.59      0.65        22\n",
      "        2930       1.00      0.79      0.88        19\n",
      "        2931       0.80      0.53      0.64        15\n",
      "        2932       1.00      0.23      0.38        13\n",
      "        2933       0.53      0.45      0.49        22\n",
      "        2934       1.00      0.61      0.76        18\n",
      "        2935       1.00      0.80      0.89        20\n",
      "        2936       0.00      0.00      0.00         2\n",
      "        2937       1.00      0.70      0.82        10\n",
      "        2938       0.67      0.40      0.50         5\n",
      "        2939       0.94      0.89      0.92        19\n",
      "        2940       0.62      0.50      0.56        10\n",
      "        2941       0.73      0.73      0.73        15\n",
      "        2942       1.00      0.80      0.89        20\n",
      "        2943       1.00      0.40      0.57         5\n",
      "        2944       0.00      0.00      0.00         2\n",
      "        2945       0.73      0.67      0.70        12\n",
      "        2946       1.00      0.40      0.57         5\n",
      "        2947       0.78      0.88      0.82         8\n",
      "        2948       0.89      0.89      0.89         9\n",
      "        2949       1.00      0.25      0.40         4\n",
      "        2950       1.00      0.20      0.33         5\n",
      "        2951       1.00      0.43      0.60         7\n",
      "        2952       0.20      0.25      0.22         8\n",
      "        2953       1.00      0.73      0.84        11\n",
      "        2954       0.93      0.72      0.81        18\n",
      "        2955       0.75      0.67      0.71         9\n",
      "        2956       1.00      0.25      0.40         4\n",
      "        2957       0.70      0.70      0.70        20\n",
      "        2958       0.44      0.78      0.56         9\n",
      "        2959       0.56      0.71      0.63         7\n",
      "        2960       0.36      0.50      0.42         8\n",
      "        2961       1.00      0.54      0.70        13\n",
      "        2962       1.00      0.57      0.73        21\n",
      "        2963       0.00      0.00      0.00         6\n",
      "        2964       1.00      0.45      0.62        22\n",
      "        2965       0.50      0.40      0.44        15\n",
      "        2966       1.00      0.33      0.50         9\n",
      "        2967       1.00      0.69      0.82        13\n",
      "        2968       0.40      0.40      0.40         5\n",
      "        2969       1.00      0.62      0.77        16\n",
      "        2970       1.00      0.64      0.78        14\n",
      "        2971       0.85      0.68      0.76        25\n",
      "        2972       1.00      0.91      0.95        22\n",
      "        2973       1.00      0.88      0.94        17\n",
      "        2974       0.92      0.69      0.79        16\n",
      "        2975       1.00      0.73      0.84        22\n",
      "        2976       1.00      0.83      0.91        18\n",
      "        2977       1.00      0.12      0.22         8\n",
      "        2978       0.92      0.92      0.92        12\n",
      "        2979       0.00      0.00      0.00         9\n",
      "        2980       0.77      0.85      0.81        20\n",
      "        2981       0.89      0.73      0.80        11\n",
      "        2982       1.00      0.25      0.40         4\n",
      "        2983       0.57      0.57      0.57        14\n",
      "        2984       1.00      0.45      0.62        22\n",
      "        2985       1.00      0.72      0.84        18\n",
      "        2986       1.00      0.90      0.95        10\n",
      "        2987       0.89      0.89      0.89        18\n",
      "        2988       1.00      0.67      0.80        12\n",
      "        2989       1.00      0.83      0.91         6\n",
      "        2990       0.00      0.00      0.00         2\n",
      "        2991       0.68      0.85      0.76        20\n",
      "        2992       1.00      0.62      0.77         8\n",
      "        2993       1.00      0.67      0.80         3\n",
      "        2994       1.00      0.55      0.71        22\n",
      "        2995       1.00      0.25      0.40         4\n",
      "        2996       0.91      0.62      0.74        16\n",
      "        2997       0.88      0.88      0.88        17\n",
      "        2998       1.00      0.50      0.67         4\n",
      "        2999       1.00      0.71      0.83        14\n",
      "        3000       0.94      0.84      0.89        19\n",
      "        3001       1.00      0.36      0.53        14\n",
      "        3002       1.00      0.54      0.70        24\n",
      "        3003       1.00      0.57      0.73        14\n",
      "        3004       0.62      1.00      0.77         5\n",
      "        3005       1.00      0.75      0.86         4\n",
      "        3006       0.83      0.45      0.59        11\n",
      "        3007       0.89      0.57      0.70        14\n",
      "        3008       1.00      0.56      0.72        16\n",
      "        3009       1.00      0.60      0.75        20\n",
      "        3010       0.83      0.91      0.87        11\n",
      "        3011       1.00      0.71      0.83        14\n",
      "        3012       1.00      0.82      0.90        11\n",
      "        3013       1.00      0.94      0.97        16\n",
      "        3014       1.00      0.59      0.74        17\n",
      "        3015       1.00      0.29      0.44        14\n",
      "        3016       1.00      0.47      0.64        19\n",
      "        3017       1.00      0.72      0.84        18\n",
      "        3018       1.00      0.83      0.91         6\n",
      "        3019       0.83      0.62      0.71         8\n",
      "        3020       1.00      0.17      0.29         6\n",
      "        3021       0.62      0.71      0.67         7\n",
      "        3022       1.00      0.85      0.92        13\n",
      "        3023       1.00      0.75      0.86        12\n",
      "        3024       1.00      0.43      0.60         7\n",
      "        3025       0.00      0.00      0.00         3\n",
      "        3026       0.00      0.00      0.00         1\n",
      "        3027       0.50      0.60      0.55         5\n",
      "        3028       1.00      0.67      0.80        15\n",
      "        3029       1.00      0.86      0.92         7\n",
      "        3030       0.93      0.67      0.78        21\n",
      "        3031       1.00      0.69      0.81        16\n",
      "        3032       1.00      0.77      0.87        13\n",
      "        3033       1.00      0.69      0.81        16\n",
      "        3034       1.00      0.33      0.50         6\n",
      "        3035       1.00      0.74      0.85        19\n",
      "        3036       1.00      0.73      0.85        15\n",
      "        3037       0.00      0.00      0.00         1\n",
      "        3038       1.00      0.85      0.92        13\n",
      "        3039       1.00      0.43      0.60         7\n",
      "        3040       1.00      0.47      0.64        19\n",
      "        3041       0.50      0.25      0.33         4\n",
      "        3042       0.75      0.60      0.67         5\n",
      "        3043       1.00      0.50      0.67         6\n",
      "        3044       0.73      0.62      0.67        13\n",
      "        3045       0.43      0.30      0.35        10\n",
      "        3046       0.82      0.70      0.76        20\n",
      "        3047       0.89      0.80      0.84        10\n",
      "        3048       1.00      1.00      1.00         1\n",
      "        3049       1.00      1.00      1.00         2\n",
      "        3050       1.00      0.83      0.91        12\n",
      "        3051       0.58      0.58      0.58        12\n",
      "\n",
      "    accuracy                           0.65     38527\n",
      "   macro avg       0.81      0.58      0.65     38527\n",
      "weighted avg       0.88      0.65      0.73     38527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8e026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ae61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f3ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8dd485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedder, './masked_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcface",
   "language": "python",
   "name": "arcface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
